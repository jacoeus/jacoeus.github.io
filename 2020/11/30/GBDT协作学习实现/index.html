<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><meta name="google-site-verification" content="1he7C7Wt3l_3CNQSu4T8-ZTZacFv5BWFyxQZs38IW7I"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"jacoeus.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="GBDT梯度提升树因其本身是基于梯度来进行决策树构建的，使得他能够无损的支持分布式决策树构建 通过GBDT结合同态加密技术可以应用于横向联邦学习的情况（数据集的特征保证统一） GBDT的主要工程实现包括XGBoost以及LightGBM，两者在几大主流大数据平台有分布式的实现"><meta property="og:type" content="article"><meta property="og:title" content="GBDT协作学习实现"><meta property="og:url" content="https://jacoeus.github.io/2020/11/30/GBDT%E5%8D%8F%E4%BD%9C%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0/index.html"><meta property="og:site_name" content="JacoeusのBlog"><meta property="og:description" content="GBDT梯度提升树因其本身是基于梯度来进行决策树构建的，使得他能够无损的支持分布式决策树构建 通过GBDT结合同态加密技术可以应用于横向联邦学习的情况（数据集的特征保证统一） GBDT的主要工程实现包括XGBoost以及LightGBM，两者在几大主流大数据平台有分布式的实现"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://jacoeus.github.io/2020/11/30/GBDT%E5%8D%8F%E4%BD%9C%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0/f1_xgboost.png"><meta property="og:image" content="https://jacoeus.github.io/2020/11/30/GBDT%E5%8D%8F%E4%BD%9C%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0/f2_xgboost.png"><meta property="og:image" content="https://jacoeus.github.io/2020/11/30/GBDT%E5%8D%8F%E4%BD%9C%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0/f3_histogram.png"><meta property="og:image" content="https://jacoeus.github.io/2020/11/30/GBDT%E5%8D%8F%E4%BD%9C%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0/f4_SGBDT.png"><meta property="article:published_time" content="2020-11-30T11:43:05.000Z"><meta property="article:modified_time" content="2021-11-26T09:10:48.158Z"><meta property="article:author" content="tang ziyin"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="联邦学习"><meta property="article:tag" content="协作学习"><meta property="article:tag" content="决策树算法"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://jacoeus.github.io/2020/11/30/GBDT%E5%8D%8F%E4%BD%9C%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0/f1_xgboost.png"><link rel="canonical" href="https://jacoeus.github.io/2020/11/30/GBDT%E5%8D%8F%E4%BD%9C%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>GBDT协作学习实现 | JacoeusのBlog</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">JacoeusのBlog</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Hello World</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://jacoeus.github.io/2020/11/30/GBDT%E5%8D%8F%E4%BD%9C%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="tang ziyin"><meta itemprop="description" content="Awake, arise or be for ever fall&#39;n"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="JacoeusのBlog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">GBDT协作学习实现</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-11-30 19:43:05" itemprop="dateCreated datePublished" datetime="2020-11-30T19:43:05+08:00">2020-11-30</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-11-26 17:10:48" itemprop="dateModified" datetime="2021-11-26T17:10:48+08:00">2021-11-26</time> </span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>8.7k</span></span></div></header><div class="post-body" itemprop="articleBody"><p>GBDT梯度提升树因其本身是基于梯度来进行决策树构建的，使得他能够无损的支持分布式决策树构建</p><p>通过GBDT结合同态加密技术可以应用于横向联邦学习的情况（数据集的特征保证统一）</p><p>GBDT的主要工程实现包括XGBoost以及LightGBM，两者在几大主流大数据平台有分布式的实现</p><a id="more"></a><h1 id="GBDT-amp-XGBoost-amp-LightGBM"><a href="#GBDT-amp-XGBoost-amp-LightGBM" class="headerlink" title="GBDT &amp; XGBoost &amp; LightGBM"></a>GBDT &amp; XGBoost &amp; LightGBM</h1><p>GBDT，梯度提升树，使用集成学习的boosting方法，根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中。<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1603.02754.pdf">XGBoost</a> 和 <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf">LightGBM</a>是GBDT的两个工程实现</p><h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><p>不同于ID3系列决策树算法使用的信息熵以及信息增益，GBDT引入了<strong>损失函数</strong>以及<strong>目标函数</strong>等概念来实现树的构建</p><h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><ol><li><p>GBDT 一般是由 $k$ 个基模型（基决策树）组成的一个加法运算式 $\hat{y_i} = \sum_{t=1}^kf_t(x_i)$ ，其中 $f_k$ 为第 $k$ 个基模型， $\hat{y_i}$ 为第 $i$ 个样本的预测值</p><p><img src="f1_xgboost.png" alt="f1_xgboost"></p><blockquote><p>Tree Ensemble Model. The final prediction for a given example is the sum of predictions from each tree.</p></blockquote></li><li><p><strong>损失函数</strong>可由预测值 $\hat{y_i}$ 与真实值 $y_i$ 进行表示，$L = \sum_{i=1}^nl(y_i,\hat{y_i})$ ，其中 n 为样本数量，最简单的，我们可以把 $| \hat{y_i} - y_i | $ 理解成损失函数</p></li><li><p>由于模型的预测精度由模型的<strong>偏差</strong>和<strong>方差</strong>共同决定，损失函数代表了模型的偏差，想要方差小则需要简单的模型，所以目标函数由模型的损失函数 $L$ 与抑制模型复杂度的正则项 $\Omega$ 组成，于是在损失函数的基础上基于模型复杂度的考量建立<strong>目标函数</strong>：<br>$$<br>Obj = \sum_{i=1}^nl(\hat{y_i},y_i) + \sum_{t=1}^k\Omega(f_t)<br>$$</p></li><li><p>boosting 模型是前向加法，以第 $t$ 步的模型为例，模型对第 $i$ 个样本 $x_i$ 的预测为 $\hat{y_i}^t = \hat{y_i}^{t-1} + f_t(x_i)$</p><p>其中 $\hat{y_i}^{t-1}$ 由第 $t - 1$ 步的模型给出的预测值，是已知常数， $f_t(x_i)$ 是我们这次需要加入的新模型的预测值，此时，目标函数就可以写成：<br>$$<br>Obj^{(t)} = \sum_{i=1}^nl(y_i,\hat{y_i}^t) + \sum_{t=1}^t\Omega(f_i) = \sum_{i=1}^nl(y_i,\hat{y_i}^{t-1}+f_t(x_i))+\sum_{i=1}^t\Omega(f_t)<br>$$<br>求此时最优化目标函数，就相当于求解 $f_t(x_i)$ 。以 $| \hat{y_i} - y_i | $ 作为损失函数为例的话，我们当然希望其值越接近于零越好，于是我们需要求目标函数取极小值时 $f_t(x_i)$ 的解</p></li><li><p>根据泰勒公式我们把函数 $f(x+\Delta x)$ 在点 $x$ 处进行泰勒的二阶展开，可得到等式：<br>$$<br>f(x+\Delta x) \approx f(x) + f^{‘}(x) + \frac{1}{2}f^{‘’}(x)\Delta x^2f(x+\Delta x) \approx f(x) + f^{‘}(x) + \frac{1}{2}f^{‘’}(x)\Delta x^2<br>$$<br>将把 $\hat{y_i}^{t-1}$ 视为 $x$ ， $f_t(x_i)$ 视为 $\Delta x$ ，故可以将目标函数写为 ：<br>$$<br>Obj^{(t)} = \sum_{t=1}^n l(y_i,\hat{y_i}^{t-1} + f_t(x)) + \sum_{t=1}^t\Omega(f_i) = \sum_{t=1}^n\left[l(y_i,\hat{y_i}^{t-1}) + g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)\right] + \sum_{t=1}^t\Omega(f_i)<br>$$<br>其中 $g_i$ 为损失函数对 $\hat{y_i}^{t-1}$ 的一阶导， $h_i$ 为损失函数对 $\hat{y_i}^{t-1}$ 的二阶导</p></li><li><p>由于在第 $t$ 步时 $\hat{y_i}^{t-1}$ 其实是一个已知的值，所以 $l(y_i,\hat{y_i}^{t-1})$ 是一个常数，其对函数的优化不会产生影响，因此目标函数可以写成：<br>$$<br>Obj^{(t)} \approx \sum_{t=1}^n\left[g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)\right] + \sum_{t=1}^t\Omega(f_i)<br>$$<br>于是，我们只需根据前一步的 $\hat{y_i}^{t-1}$ 求出每一步损失函数的一阶导和二阶导的值，然后最优化目标函数，就可以得到一步的 $f(x)$，最后根据加法模型得到一个整体模型</p></li></ol><h3 id="基于决策树的目标函数（XGBoost）"><a href="#基于决策树的目标函数（XGBoost）" class="headerlink" title="基于决策树的目标函数（XGBoost）"></a>基于决策树的目标函数（XGBoost）</h3><ol><li><p>将决策树定义为 $f_t(x) = w_{q(x)}$ ， $x$ 为某一样本，这里的 $q(x)$ 代表了该样本所处叶子结点上，而 $w_q$ 则代表了叶子结点取值 ，所以 $w_{q(x)}$ 就代表了某个样本$x$（属于 $q(x)$ 叶子节点）的取值 $w$（即预测值）</p></li><li><p>决策树的复杂度可由叶子数 $T$ 体现，叶子节点越少模型越简单，此外叶子节点也不应该含有过高的权重 $w$ ，因此目标函数的正则项被定义为：<br>$$<br>\Omega(f_t) = \gamma T + \frac{1}{2}\lambda\sum_{j=1}^Tw_j^2<br>$$<br>即决策树模型的复杂度由生成决策树的叶子节点数量 $T$ 和所有节点权重所组成的向量的 $L_2$ 范式共同决定</p></li><li><p>设 $I_j = {i|q(x_i) = j}$ 为第 $j$ 个叶子节点的样本集合，故我们的目标函数可以写成：<br>$$<br>\begin{aligned}<br>Obj^{(t)} &amp;\approx \sum_{i=1}^n\left[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)\right] + \Omega(f_t)<br>\\&amp;= \sum_{i=1}^n\left[g_iw_{q(x_i)}^2\right] + \gamma T + \frac{1}{2}\lambda\sum_{j=1}^Tw_j^2<br>\\&amp;= \sum_{j=1}^T\left[(\sum_{i\in I_j}g_i)w_j + \frac{1}{2}(\sum_{i\in I_j}h_i + \lambda)w_j^2\right] + \gamma T<br>\end{aligned}<br>$$<br>其中，第二步是遍历所有的样本后求每个样本的损失函数，但样本最终会落在叶子节点上，所以我们也可以遍历叶子节点，然后获取叶子节点上的样本集合，最后在求损失函数。即我们之前样本的集合，现在都改写成叶子结点的集合，由于一个叶子结点有多个样本存在，因此才有了 $\sum_{i\in I_j}g_i$ 和 $\sum_{i\in I_j}h_i$ 这两项， $w_j$ 为第 $j$ 个叶子节点取值</p></li><li><p>为简化表达式，定义 $G_j = \sum_{i\in I_j}g_i$ ， $H_j = \sum_{i\in I_j}h_i$ ，则目标函数为：<br>$$<br>Obj^{(t)} = \sum_{j=1}^T\left[G_jw_j + \frac{1}{2}(H_j + \lambda)w_j^2\right] + \gamma T<br>$$<br>这里我们要注意 $G_j$ 和 $H_j$ 是前 $t-1$ 步得到的结果，其值已知可视为常数，只有最后一棵树的叶子节点 $w_j$ 不确定，那么将目标函数对 $w_j$ 求一阶导，并令其等于 $0$ ，则可以求得目标函数取极值时叶子结点 $j$ 对应的权值：<br>$$<br>w_j^* = -\frac{G_j}{H_j + \lambda}<br>$$<br>于是目标函数可以化简为：<br>$$<br>Obj = -\frac{1}{2}\sum_{j=1}^T\frac{G_j^2}{H_j+\lambda} + \gamma T<br>$$<br><img src="f2_xgboost.png" alt="f2_xgboost"></p><blockquote><p>Structure Score Calculation. We only need to sum up the gradient and second order gradient statistics on each leaf, then apply the scoring formula to get the quality score.</p></blockquote><p>上图给出目标函数计算的例子，求每个节点每个样本的一阶导数 $g_i$ 和二阶导数 $h_i$ ，然后针对每个节点对所含样本求和得到的 $G_j$ 和 $H_j$ ，最后遍历决策树的节点即可得到目标函数</p><p>在实现过程中，我们不需要对目标函数值进行计算，只需根据损失函数求得的一阶导 g 和二阶导 h 对数据集进行划分，寻找最佳分裂点并建树，然后求得叶子节点对应的权值 $w$ 转换得到下一轮boost的 $\hat{y_i}^{t-1}$ ，同时也作为之后进行预测的依据</p></li><li><p>其中，我们通过计算最大的分裂收益来寻找最佳分裂点，根据上一步简化的目标函数，某一节点分裂前的目标函数可以写为：<br>$$<br>Obj_1 = -\frac{1}{2}\left[\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] + \gamma T<br>$$<br>其中 $G_L$ 表示按当前分裂点分裂得到的左节点的 $G_j$ 值，而分裂后的目标函数为：</p><p>$$<br>Obj_2 = -\frac{1}{2}\left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}\right] + \gamma (T+1)<br>$$<br>所以，对于目标函数来说，分裂后的收益为：<br>$$<br>Gain = \frac{1}{2}\left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma<br>$$<br>为了构建能够最小化损失函数的决策树，在每次进行分裂的时候，需要在候选分裂点中选择分裂收益最大的进行分裂，层序进行，直至完成建树的过程</p></li></ol><h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><p>XGBoost在GDBT的基础上进行了一些算法和工程方面的优化</p><ol><li><p><strong>引入二阶导：</strong>GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数；</p></li><li><p><strong>正则项：</strong>XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合；</p></li><li><p><strong>Shrinkage（缩减）：</strong>类似于学习率，XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间；</p></li><li><p>列抽样：XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算；</p></li><li><p>缺失值处理：XGBoost 采用的<strong>稀疏感知算法</strong>极大的加快了节点分裂的速度；</p></li><li><p>并行化：块结构可以很好的支持并行计算；</p></li></ol><p>但仍存在如下的缺点</p><ol><li>虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；</li><li>预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存；</li></ol><h2 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h2><p>LightGBM 由微软提出，主要用于解决 GDBT 在<strong>海量数据</strong>中遇到的问题，以便其可以更好更快地用于工业实践中</p><p>LightGBM 为了解决这些问题提出了以下几点解决方案：</p><ol><li>单边梯度抽样算法：对样本进行抽样，减少了大量梯度小的样本；</li><li><strong>直方图算法（直方图做差加速）</strong>：将连续的特征离散化为 k 个离散特征，同时构造一个宽度为 k 的直方图用于统计信息（包括 k 个 bin）；</li><li>互斥特征捆绑算法：将一些特征进行融合绑定，可以降低特征数量（高维特征往往是稀疏的，特征间可能是互斥的）；</li><li>基于最大深度的 Leaf-wise 的垂直生长算法：减少了计算量，配合最大深度的限制防止过拟合，由于每次都需要计算增益最大的节点，所以无法并行分裂；</li><li>类别特征最优分割；</li><li>特征并行/数据并行/投票并行；</li><li>缓存优化；</li></ol><h2 id="DimBoost"><a href="#DimBoost" class="headerlink" title="DimBoost"></a>DimBoost</h2><p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3183713.3196892">DimBoost: Boosting Gradient Boosting Decision Tree to Higher Dimensions</a></p><p>另外补充一个针对高维情况的GDBT实现，在特征数量达到百万级别时，其性能要远优于XGBoost</p><h2 id="FATE"><a href="#FATE" class="headerlink" title="FATE"></a>FATE</h2><blockquote><p>FATE (Federated AI Technology Enabler) 是微众银行AI部门发起的开源项目，为联邦学习生态系统提供了可靠的安全计算框架。FATE项目使用多方安全计算 (MPC) 以及同态加密 (HE) 技术构建底层安全计算协议，以此支持不同种类的机器学习的安全计算，包括逻辑回归、基于树的算法、深度学习和迁移学习等</p></blockquote><p>FATE官方网站：<a target="_blank" rel="noopener" href="https://fate.fedai.org/">https://fate.fedai.org/</a></p><p>在进行协作学习实现时，我们主要参考了FATE的homo-secureboost部分代码，并根据需要进行了一些调整</p><ul><li>采用全局的近似算法作为最优切分点划分算法，即学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割；</li><li>使用softmax交叉熵损失函数，并取其一阶导和二阶导建立直方图；</li><li>实现LightGBM的直方图算法，包括直方图作差加速；</li><li>采用类似于XGBoost的Level-wise增长策略，方便并行计算每一层的分裂节点，提高了训练速度，但同时也因为节点增益过小增加了很多不必要的分裂；</li><li>稀疏感知算法 TODO</li><li>特征选择 TODO</li></ul><h1 id="协作学习实现"><a href="#协作学习实现" class="headerlink" title="协作学习实现"></a>协作学习实现</h1><p>先以二分类问题为例说明协作学习实现的过程和原理，再对多分类问题进行简单说明</p><h2 id="大致流程"><a href="#大致流程" class="headerlink" title="大致流程"></a>大致流程</h2><ul><li><p>Parameter Server</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">获取全局 Quantile Sketch (merge) &lt;- Worker</span><br><span class="line">计算全局候选分裂点 -&gt; Worker</span><br><span class="line"><span class="keyword">for</span> epoch_idx <span class="keyword">in</span> range(self.boosting_round)：</span><br><span class="line">	获取所有局部 g h &lt;- Worker</span><br><span class="line">    计算全局 g h -&gt; Worker</span><br><span class="line">    <span class="keyword">for</span> dep <span class="keyword">in</span> range(self.max_depth):</span><br><span class="line">        获取全局的梯度直方图 (+) &lt;- Worker</span><br><span class="line">        计算最佳分裂点 -&gt; Worker</span><br></pre></td></tr></table></figure></li><li><p>Worker</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">计算局部 Quantile Sketch -&gt; PS</span><br><span class="line">初始化(候选分裂点，计算y_hat等) &lt;- PS</span><br><span class="line"><span class="keyword">for</span> epoch_idx <span class="keyword">in</span> range(self.boosting_round)：</span><br><span class="line">	根据 y_hat 计算局部 g h -&gt; PS</span><br><span class="line">	初始化决策树（根据全局 g h 、数据集等建立根节点，初始当前层节点为根节点等） &lt;- PS</span><br><span class="line">    <span class="keyword">for</span> dep <span class="keyword">in</span> range(self.max_depth):</span><br><span class="line">        <span class="keyword">if</span> 树达到最大深度：</span><br><span class="line">        	停止树的构建</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            计算当前层各个节点各个特征的梯度直方图 -&gt; PS</span><br><span class="line">            获取当前层各个节点最佳分裂点 &lt;- PS</span><br><span class="line">            根据最佳分裂点建立下一层节点</span><br><span class="line">    根据叶子节点 weight 预测新的 y_hat</span><br></pre></td></tr></table></figure></li></ul><h2 id="Quantile-Sketch获取全局候选节点"><a href="#Quantile-Sketch获取全局候选节点" class="headerlink" title="Quantile Sketch获取全局候选节点"></a>Quantile Sketch获取全局候选节点</h2><ul><li>每个worker分别收集对应数据集中每个feature 的所有数值并以Quantile Sketch（GK Sketch）进行存储</li><li>参数服务器 PS 汇集所有worker的sketch并merge，得到全局的quantile sketch</li><li>根据给定的全局参数 bin_num 确定分位点，查询sketch得到对应的数值</li></ul><h2 id="梯度直方图-amp-分裂点计算"><a href="#梯度直方图-amp-分裂点计算" class="headerlink" title="梯度直方图 &amp; 分裂点计算"></a>梯度直方图 &amp; 分裂点计算</h2><blockquote><p>梯度直方图：对于一个树结点，扫描树结点上的所有训练样本，根据其特征值，将样本梯度累加到对应的直方图桶中（候选节点将特征的值域分割成n个桶）</p></blockquote><p><img src="f3_histogram.png" alt="f3_histogram"></p><blockquote><p>Histogram-based split finding for one feature</p></blockquote><ol><li><p><strong>计算梯度直方图：</strong> 从待处理树节点的队列中取出待处理的树节点，在Worker上，根据本节点的训练数据计算局部一阶和二阶梯度直方图，对于每个节点的所有特征都有 g 和 h 对应的两个直方图，直方图以bin_id（根据候选分裂点划分）为横坐标，g h 值作为纵坐标</p></li><li><p><strong>同步&amp;合并直方图：</strong> Worker将局部梯度直方图推送到参数服务器，PS节点在接收到Worker发送的局部梯度直方图后，根据梯度直方图对应的树节点，将其按照特征值和 bin_id 加到全局梯度直方图上</p><p><img src="f4_SGBDT.png" alt="f4_LSGBDT"></p><blockquote><p>Workers construct local histograms for all features and aggregate into global ones.</p></blockquote></li><li><p><strong>寻找最佳分裂点：</strong> 参数服务器根据每个特征的梯度直方图计算对于该特征的最佳分裂点及其增益，选取其中目标函数增益最大的特征以及对应的候选分裂点作为全局的最佳分裂点。如果没有合适的分裂点，则将当前节点标记为叶子节点，不再进行后续分裂</p></li><li><p><strong>分裂树节点：</strong> Worker根据计算得到的最佳分裂点，创建节点，将本节点的训练数据切分到两个节点上，并根据分裂得到的 $G_L$ 和 $H_L$ 或 $G_R$ 和 $H_R$ 计算节点的 weight 值。如果树的高度没有达到最大限制，则将两个叶子节点加入到待处理树节点的队列</p></li></ol><h2 id="多分类实现-amp-predict"><a href="#多分类实现-amp-predict" class="headerlink" title="多分类实现 &amp; predict"></a>多分类实现 &amp; predict</h2><ul><li><p>在每一轮boost中对每个class分别训练决策树，将当前类别的 y 值视为 1，其他类别视为 0 进行1 vs rest二分类进行树的构建以及 $w$ 的计算</p></li><li><p>遵循 1 vs rest 分解策略，训练好 $|\mathcal Y|$ 个二分类器后，将每个分类器的实值输出 $H(\boldsymbol{x}) = \sum_{t=1}^T \alpha _th_t(\boldsymbol{x})$ 用于识别最有可能的多分类类别，即<br>$$<br>H(\boldsymbol{x}) = \underset{y \in \mathcal Y}{\arg\max} H_y(\boldsymbol{x})<br>$$</p></li><li><p>在具体实现时，我们通过将测试样本投入每轮 boost 的每个class对应的决策树中（共 boosting_round * class_num 棵），将多轮boost中对应同一class的决策树的预测值结合学习率进行累加计算，于是每个测试样本得到对应 class_num 个类的 class_num 个值，取最大值对应的 class 作为该样本的预测值</p></li></ul><hr><h1 id="PS"><a href="#PS" class="headerlink" title="PS"></a>PS</h1><h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><table><thead><tr><th>变量</th><th>shape</th><th>说明</th></tr></thead><tbody><tr><td>X</td><td>(sample_num, feature_num)</td><td></td></tr><tr><td>y</td><td>(sample_num)</td><td></td></tr><tr><td>y_hat ( $\hat{y}$ )</td><td>(sample_num, [class_num])</td><td>y 的预测值，对于多分类情况第二个维度大小为类的数量</td></tr><tr><td>g_h</td><td>(sample_num, 2)</td><td>记录每个样本对应的 g 值和 h 值</td></tr><tr><td>inst2node_idx</td><td>(sample_num, 2)</td><td>将样本记录与树节点关联</td></tr><tr><td>bin_split_points</td><td>(feature_num, &lt;bin_num)</td><td>记录候选分裂点，每个特征的候选分裂点最大为 bin_num，不同特征分裂点数量不尽相同</td></tr><tr><td>histograms</td><td>(node_num, feature_num, &lt;bin_num, 2)</td><td>记录梯度直方图</td></tr><tr><td>sample_weights</td><td>(sample_num)</td><td>记录样本对应的weights</td></tr></tbody></table><p>其中，sample__num表示样本数量，feature_num表示特征数量，class_num表示类数量，bin_num表示根据候选分裂点划分得到的特征取值区间数量，node_num表示当前层节点数量</p><h2 id="Softmax交叉熵损失函数"><a href="#Softmax交叉熵损失函数" class="headerlink" title="Softmax交叉熵损失函数"></a>Softmax交叉熵损失函数</h2><p>$$<br>\begin{aligned}<br>&amp;l (y_i, \hat{y_i}^{(t-1)}) = y_i log(1 + e^{−\hat{y_i}^{(t-1)}} ) + (1 − y_i )log(1 + e^{\hat{y_i}^{t-1}} )<br>\\&amp;g_i = \frac{1}{1+e^{-\hat{y_i}^{(t-1)}}} - y_i<br>\\&amp;h_i = \frac{1}{1+e^{-\hat{y_i}^{(t-1)}}} * (1-\frac{1}{1+e^{-\hat{y_i}^{(t-1)}}})<br>\end{aligned}<br>$$</p><h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h2><p>secureBoost对softmax的实现似乎与传统的Softmax交叉熵损失函数不同，下面是它的代码实现，感兴趣的读者可以尝试分析证明</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span>(<span class="params">x, axis=<span class="number">-1</span></span>):</span></span><br><span class="line">    y = np.exp(x - np.max(x, axis, keepdims=<span class="literal">True</span>))</span><br><span class="line">    <span class="keyword">return</span> y / np.sum(y, axis, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>$$<br>\begin{aligned}<br>&amp;Softmax(\boldsymbol{x}) = \frac{e^{\boldsymbol{x}-\Vert\boldsymbol{x}\Vert_{\infty}}}{\Vert e^{\boldsymbol{x}-\Vert\boldsymbol{x}\Vert_{\infty}}\Vert_1}<br>\\&amp;g_i = Softmax(\hat{y_i}^{(t-1)}) - y_i<br>\\&amp;h_i = Softmax(\hat{y_i}^{(t-1)}) * (1-Softmax(\hat{y_i}^{(t-1)}))<br>\\&amp;\hat{y_i}^{(t)} = Softmax(w_j^*) = Softmax(-\frac{G_j}{H_j+\lambda}) = Softmax(-\frac{\sum_{i \in I_j}g_i}{\sum_{i \in I_j}h_i + \lambda})<br>\end{aligned}<br>$$</p></div><div class="reward-container"><div></div><button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'>打赏</button><div id="qr" style="display:none"><div style="display:inline-block"><img src="/images/alipay.jpg" alt="tang ziyin 支付宝"><p>支付宝</p></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a> <a href="/tags/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 联邦学习</a> <a href="/tags/%E5%8D%8F%E4%BD%9C%E5%AD%A6%E4%B9%A0/" rel="tag"># 协作学习</a> <a href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95/" rel="tag"># 决策树算法</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2020/09/19/Docker%E5%AD%A6%E4%B9%A0-%E4%BD%BF%E7%94%A8Docker%E6%9E%84%E5%BB%BAzeek%E5%88%B0kafka%E7%9A%84%E8%BF%9E%E6%8E%A5/" rel="prev" title="Docker学习&使用Docker构建zeek到kafka的连接"><i class="fa fa-chevron-left"></i> Docker学习&使用Docker构建zeek到kafka的连接</a></div><div class="post-nav-item"><a href="/2021/01/16/%E5%8D%8F%E4%BD%9C%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8%E5%A4%A7%E8%B5%9B%E5%8F%82%E8%B5%9B%E6%80%BB%E7%BB%93/" rel="next" title="协作学习与网络安全大赛参赛总结">协作学习与网络安全大赛参赛总结 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#GBDT-amp-XGBoost-amp-LightGBM"><span class="nav-number">1.</span> <span class="nav-text">GBDT &amp; XGBoost &amp; LightGBM</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#GBDT"><span class="nav-number">1.1.</span> <span class="nav-text">GBDT</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.1.</span> <span class="nav-text">目标函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%EF%BC%88XGBoost%EF%BC%89"><span class="nav-number">1.1.2.</span> <span class="nav-text">基于决策树的目标函数（XGBoost）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBoost"><span class="nav-number">1.2.</span> <span class="nav-text">XGBoost</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LightGBM"><span class="nav-number">1.3.</span> <span class="nav-text">LightGBM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DimBoost"><span class="nav-number">1.4.</span> <span class="nav-text">DimBoost</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FATE"><span class="nav-number">1.5.</span> <span class="nav-text">FATE</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%8F%E4%BD%9C%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0"><span class="nav-number">2.</span> <span class="nav-text">协作学习实现</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%A7%E8%87%B4%E6%B5%81%E7%A8%8B"><span class="nav-number">2.1.</span> <span class="nav-text">大致流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Quantile-Sketch%E8%8E%B7%E5%8F%96%E5%85%A8%E5%B1%80%E5%80%99%E9%80%89%E8%8A%82%E7%82%B9"><span class="nav-number">2.2.</span> <span class="nav-text">Quantile Sketch获取全局候选节点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E7%9B%B4%E6%96%B9%E5%9B%BE-amp-%E5%88%86%E8%A3%82%E7%82%B9%E8%AE%A1%E7%AE%97"><span class="nav-number">2.3.</span> <span class="nav-text">梯度直方图 &amp; 分裂点计算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E5%AE%9E%E7%8E%B0-amp-predict"><span class="nav-number">2.4.</span> <span class="nav-text">多分类实现 &amp; predict</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PS"><span class="nav-number">3.</span> <span class="nav-text">PS</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84"><span class="nav-number">3.1.</span> <span class="nav-text">数据结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Softmax%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">3.2.</span> <span class="nav-text">Softmax交叉熵损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Softmax"><span class="nav-number">3.3.</span> <span class="nav-text">Softmax</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="tang ziyin" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">tang ziyin</p><div class="site-description" itemprop="description">Awake, arise or be for ever fall&#39;n</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">13</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">4</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">28</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/jacoeus" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jacoeus" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:jacoeus@qq.com" title="E-Mail → mailto:jacoeus@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">tang ziyin</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">67k</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动</div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script defer src="/lib/three/three.min.js"></script><script defer src="/lib/three/three-waves.min.js"></script><script src="/js/local-search.js"></script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '74c9f8bac222854997e4',
      clientSecret: 'ea29bbd05fcfba1c58731dab48c464e83ca06b61',
      repo        : 'gitalk-comment',
      owner       : 'jacoeus',
      admin       : ['jacoeus'],
      id          : '8caa7b2de3229f4cad803e6862d8d348',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});</script></body></html>