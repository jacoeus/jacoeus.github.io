<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Blog搭建摘要</title>
    <url>/2020/09/14/Blog%E6%90%AD%E5%BB%BA%E6%91%98%E8%A6%81/</url>
    <content><![CDATA[<p>使用<a href="https://hexo.io/zh-cn/">hexo</a>进行blog搭建，在对blog搭建和next主题进行配置的过程中整理了一些有用的tips，并在下文进行简单的罗列~</p>
<a id="more"></a>

<h1 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h1><ul>
<li><p>通过在文章中使用 <code>&lt;!-- more --&gt;</code> 手动可以对文章进行截断</p>
</li>
<li><p>在文章的 <a href="https://hexo.io/docs/front-matter.html">front-matter</a> 中添加 <code>description</code>，并提供文章摘录</p>
</li>
<li><p>自动形成摘要，在 <strong>主题配置文件</strong> 中添加：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">auto_excerpt:</span><br><span class="line">  enable: true</span><br><span class="line">  length: 150</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<p>使用<a href="https://github.com/timnew/hexo-console-rename">hexo-console-rename</a>对已生成的文章进行改名</p>
<p>修改文章title，运行<code>hexo rename</code>命令即可将所有文章的title与文件名同步</p>
<hr>
<p>新建文章后自动使用typora打开新建的md文件</p>
<p>在scipts文件夹中新建example.js，并键入如下命令</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">let</span> spawn = <span class="built_in">require</span>(<span class="string">&#x27;hexo-util/lib/spawn&#x27;</span>);</span><br><span class="line"></span><br><span class="line">hexo.on(<span class="string">&#x27;new&#x27;</span>, <span class="function">(<span class="params">data</span>) =&gt;</span> &#123;</span><br><span class="line">  spawn(<span class="string">&#x27;typora&#x27;</span>, [hexo.base_dir, data.path]);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<p>参考博客 <a href="https://leaferx.online/2018/03/17/hexo-auto-open-vscode/">HEXO小技巧在 hexo new 的时候自动用 VS Code 打开新建文章</a></p>
<hr>
<p>Hexo 不支持指定多个同级分类</p>
<p>分类具有顺序性和层次性</p>
<p>标签没有顺序和层次</p>
<p>如果你需要为文章添加多个分类，可以尝试以下 list 中的方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">categories:</span><br><span class="line">- [Diary, PlayStation]</span><br><span class="line">- [Diary, Games]</span><br><span class="line">- [Life]</span><br></pre></td></tr></table></figure>

<hr>
<p>文章可以先以草稿draft形式存储，待编辑完成后发表</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new draft &lt;title&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo publish post &lt;title&gt;</span><br></pre></td></tr></table></figure>

<p>草稿默认不会显示在页面中，您可在执行时加上 <code>--draft</code> 参数，或是把 <code>render_drafts</code> 参数设为 <code>true</code> 来预览草稿</p>
<hr>
<p>在文章中插入引言，可包含作者、来源和标题。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% blockquote [author[, source]] [link] [source_link_title] %&#125;</span><br><span class="line">content</span><br><span class="line">&#123;% endblockquote %&#125;</span><br></pre></td></tr></table></figure>

<p>其他风格</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% cq %&#125;  &#123;% endcq %&#125;</span><br></pre></td></tr></table></figure>

<blockquote class="blockquote-center">
            <i class="fa fa-quote-left"></i>
            <p>content </p>

            <i class="fa fa-quote-right"></i>
          </blockquote>

<p>其他对markdown呈现效果的优化方案：<a href="https://hexo.io/zh-cn/docs/tag-plugins">标签插件（Tag Plugins）</a></p>
<hr>
<p>参考主题配置文件的custom_file_path项，可以在source/_data中建立对应文件来调整前端界面</p>
]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker学习&amp;使用Docker构建zeek到kafka的连接</title>
    <url>/2020/09/19/Docker%E5%AD%A6%E4%B9%A0-%E4%BD%BF%E7%94%A8Docker%E6%9E%84%E5%BB%BAzeek%E5%88%B0kafka%E7%9A%84%E8%BF%9E%E6%8E%A5/</url>
    <content><![CDATA[<p>记录一个又是在debug的下午，试图通过kafka将zeek解析pcap文件的结果导入Timescaledb，在开心的参照官方给出的使用docker构建zeek到kafka连接的<a href="https://github.com/blacktop/docker-zeek/blob/master/docs/kafka.md">教程</a>复制粘贴的过程中碰到了奇怪的问题<code>can&#39;t update DNS cache</code>，后来发现似乎只要删除命令中的<code>-P</code>就能解决问题</p>
<p>在尝试解决问题的过程中，对docker进行了简单的学习，也尝试理解blacktop/kafka以及blacktop/zeek:kafka两个容器以及docker容器之间的通信机制，本文就docker的学习内容进行简单的整理</p>
<a id="more"></a>

<h1 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h1><blockquote><p>Docker 是一个开放源代码软件，是一个开放平台，用于开发应用、交付应用、运行应用。 Docker允许用户将基础设施中的应用单独分割出来，形成更小的颗粒，从而提高交付软件的速度。 Docker容器与虚拟机类似，但二者在原理上不同。</p>
<footer><strong>Docker 维基百科</strong><cite><a href="https://zh.wikipedia.org/zh-cn/Docker">zh.wikipedia.org/zh-cn/Docker</a></cite></footer></blockquote>

<p><a href="https://labs.play-with-docker.com/">https://labs.play-with-docker.com/</a></p>
<p>以下整理了一些作为Docker的使用者（开发者部分有需要后续进行补充，读者也可以参考<a href="https://docs.docker.com/reference/">官方文档</a>进行学习）可能需要的基础内容并就在构建zeek到kafka连接时用到的命令进行解释</p>
<h2 id="Docker容器"><a href="#Docker容器" class="headerlink" title="Docker容器"></a>Docker容器</h2><blockquote>
<p>Docker通过容器化的技术使您能够将应用程序与基础架构分开，从而可以快速交付软件</p>
</blockquote>
<p>Docker提供了在松散隔离的环境（容器）中打包和运行应用程序的功能。隔离和安全性保证了你同一主机上同时运行多个容器。容器是轻量级的，因为它们不需要管理程序的额外负载，而是直接在主机的内核中运行。于是与虚拟机相比，docker容器的空间占用和资源消耗是小一个量级的。以下是官网对容器特性的总结</p>
<ul>
<li><strong>灵活</strong>：即使最复杂的应用程序也可以容器化</li>
<li><strong>轻量级</strong>：容器利用并共享了主机内核，在系统资源方面比虚拟机更有效</li>
<li><strong>可移植</strong>：您可以在本地构建，部署到云并在任何地方运行</li>
<li><strong>松散耦合</strong>：容器是高度自给自足并封装的容器，使您可以在不破坏其他容器的情况下更换或升级它们</li>
<li><strong>可扩展</strong>：可以在数据中心内增加并自动分发容器副本</li>
<li><strong>安全</strong>：容器将激进的约束和隔离应用于流程，而无需用户方面的任何配置</li>
</ul>
<p>我最早使用docker就是将它当做没有图形界面的虚拟机来用（因为存储空间的限制），但这只能说是docker最粗陋的应用</p>
<p>参考<a href="https://docs.docker.com/get-docker/">官网</a>获取docker，然后通过在命令行输入相关命令即可开始使用</p>
<h2 id="Docker常用子命令"><a href="#Docker常用子命令" class="headerlink" title="Docker常用子命令"></a>Docker常用子命令</h2><p>以下整理了一些常用的Docker子命令，更多子命令参考<a href="https://docs.docker.com/engine/reference/commandline/docker/">https://docs.docker.com/engine/reference/commandline/docker/</a></p>
<p><code>docker search [ 镜像名 ]</code>查找相应镜像</p>
<p>然后根据search结果选择具体镜像使用<code>docker pull [ 仓库 ] / [ 镜像名 ]:[ tag ]</code>从<a href="https://hub.docker.com/">docker hub</a>获取相应镜像到本地</p>
<p>使用<code>docker images</code>可以查看本地的镜像</p>
<p><code>docker rmi [ 镜像名 or 镜像 id ]</code>删除镜像</p>
<p>通过<code>docker run [ 参数 ] [ 镜像名 or 镜像 id ] [ 命令 ]</code>可以基于指定镜像创建容器</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>–add-host</td>
<td>[主机名]:[ip]</td>
<td>添加自定义主机到IP的映射</td>
</tr>
<tr>
<td>–detach , -d</td>
<td></td>
<td>后台运行容器, 并返回容器ID</td>
</tr>
<tr>
<td>–dns</td>
<td>[DNS服务器地址]</td>
<td>设置自定义DNS服务器</td>
</tr>
<tr>
<td>–env , -e</td>
<td></td>
<td>设置环境变量</td>
</tr>
<tr>
<td>–expose</td>
<td>[端口号]</td>
<td>开放一个端口或多个端口</td>
</tr>
<tr>
<td>–hostname , -h</td>
<td>[主机名]</td>
<td>设置容器的主机名, 默认随机生成</td>
</tr>
<tr>
<td>–interactive , -i</td>
<td></td>
<td>以交互模式运行容器, 通常与 -t 同时使用；</td>
</tr>
<tr>
<td>–ip</td>
<td>[ipv4地址]</td>
<td>为容器设置IPv4地址 ( 需要使用自定义网络 )</td>
</tr>
<tr>
<td>–link</td>
<td>[其他容器名]:[在该容器中的别名]</td>
<td>与另一个容器建立链接, 在本容器 hosts 文件中加入关联容器的记录，<strong>官方已不推荐使用</strong></td>
</tr>
<tr>
<td>–memory , -m</td>
<td>[内存上限]</td>
<td>容器内存上限</td>
</tr>
<tr>
<td>–name</td>
<td></td>
<td>为容器分配一个名称</td>
</tr>
<tr>
<td>–net</td>
<td>[bridge / host / none / container]</td>
<td>容器的网络连接类型</td>
</tr>
<tr>
<td>-p</td>
<td>[宿主机端口]:[容器内端口]</td>
<td>宿主机到容器的端口映射</td>
</tr>
<tr>
<td>-P</td>
<td></td>
<td>将所有公开的端口发布到随机端口</td>
</tr>
<tr>
<td>–tty , -t</td>
<td></td>
<td>为容器重新分配一个伪输入终端, 通常与 -i 同时使用</td>
</tr>
<tr>
<td>–volume , -v</td>
<td>[宿主机目录路径]:[容器内目录路径]</td>
<td>挂载宿主机的指定目录到容器内的指定目录</td>
</tr>
<tr>
<td>–volumes-from</td>
<td>[其他容器名]</td>
<td>将其他容器的数据卷添加到此容器</td>
</tr>
</tbody></table>
<p>更多选项参考<a href="https://docs.docker.com/engine/reference/commandline/run/">https://docs.docker.com/engine/reference/commandline/run/</a></p>
<p>通过使用<code>docker run</code>命令，可以在宿主机和容器之间建立联系以及在多个容器之间建立复杂的连接</p>
<p><code>docker ps</code>查看运行中的容器</p>
<p><code>docker ps -a</code>查看所有容器</p>
<p>使用<code>docker attach [ 容器名 or 容器 id ]</code>可以进入运行中的容器, 显示该容器的控制台界面</p>
<p><code>docker start/stop/restart/kill/rm/rename [ 容器名 or 容器 id ]</code>启动/停止/重启/强制关闭/删除/重命名容器</p>
<h2 id="–link选项说明"><a href="#–link选项说明" class="headerlink" title="–link选项说明"></a>–link选项说明</h2><p>–link选项实现容器之间连接的方法目前官方不是很推荐，因为容器间共享环境变量会导致一些不可控因素，推荐所有容器在同一个network下来通信的方法，以及用docker-compose来链接2个容器来通信的方法，具体可以参考<a href="https://yeasy.gitbook.io/docker_practice/">Docker —— 从入门到实践</a>的<a href="https://yeasy.gitbook.io/docker_practice/network/linking">容器互联</a>章节</p>
<p>docker run –link可以用来链接2个容器，使得源容器（被链接的容器）和接收容器（主动去链接的容器）之间可以互相通信，并且接收容器可以获取源容器的一些数据</p>
<p>–link选项实现源容器和接收容器之间传递数据是通过以下2种方式：</p>
<ol>
<li><p>设置环境变量</p>
<ul>
<li>当使用–link时，docker会自动在接收容器内创建基于–link参数的环境变量</li>
<li>接收容器还会获取源容器暴露的环境变量，这些变量包括：<ul>
<li>源容器Dockerfile中ENV标签设置的环境变量</li>
<li>源容器用docker run命令创建，命令中包含的 -e或–env或–env-file设置的环境变量</li>
</ul>
</li>
</ul>
</li>
<li><p>更新/etc/hosts文件</p>
<ul>
<li>docker会将源容器的host更新到目标容器的/etc/hosts中</li>
</ul>
</li>
</ol>
<p>参考文章<a href="https://www.jianshu.com/p/21d66ca6115e">https://www.jianshu.com/p/21d66ca6115e</a></p>
<h2 id="docker-run创建zeek以及kafka容器"><a href="#docker-run创建zeek以及kafka容器" class="headerlink" title="docker run创建zeek以及kafka容器"></a>docker run创建zeek以及kafka容器</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker run -d \</span></span><br><span class="line">           --name kafka \</span><br><span class="line">           -p 9092:9092 \</span><br><span class="line">           -e KAFKA_ADVERTISED_HOST_NAME=localhost \</span><br><span class="line">           -e KAFKA_CREATE_TOPICS=&quot;zeek:1:1&quot; \</span><br><span class="line">           blacktop/kafka:0.11</span><br></pre></td></tr></table></figure>

<p>上述命令表示使用blacktop/kafka:0.11镜像建立名为kafka的容器并置于后台运行，其中建立宿主机9092端口到容器9092端口的映射，其中9092是kafka默认端口，并且在之后给容器赋环境变量<code>KAFKA_ADVERTISED_HOST_NAME=localhost</code>和<code>KAFKA_CREATE_TOPICS=&quot;zeek:1:1&quot;</code></p>
<p>直接运行此命令可以docker会自动pull blacktop/kafka:0.11镜像</p>
<p>另外，通过<code>docker exec -it [容器名] /bin/bash</code>进入容器（在运行时添加-it选项则可以用attach命令直接进入）后输入env可查看环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker <span class="built_in">exec</span> -it kafka /bin/bash</span></span><br></pre></td></tr></table></figure>

<p>上面的kafka容器作为源容器（被链接的容器），下面是基于blacktop/zeek:kafka镜像构建容器作为接收容器（主动去链接的容器）的命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker run --rm \</span></span><br><span class="line">         -v `pwd`:/pcap \</span><br><span class="line">         --link kafka:localhost \</span><br><span class="line">         blacktop/zeek:kafka -r heartbleed.pcap local &quot;Site::local_nets += &#123; 192.168.11.0/24 &#125;&quot;</span><br></pre></td></tr></table></figure>

<p>其中<code>--rm</code>表示退出时自动删除容器</p>
<p><code>-v &#96;pwd&#96;:/pcap</code>表示挂载宿主机当前目录到容器内/pcap目录</p>
<p><code>--link kafka:localhost</code>可以实现与kafka容器的链接，其中localhost是当前容器在kafka中的别名</p>
<p>另外，在blacktop/zeek:kafka后的部分<code>-r heartbleed.pcap local &quot;Site::local_nets += &#123; 192.168.11.0/24 &#125;&quot;</code>是容器默认执行的命令，即在运行<code>docker run</code>命令时会在容器内运行<code>zeek -r heartbleed.pcap local &quot;Site::local_nets += &#123; 192.168.11.0/24 &#125;&quot;</code>，即在容器内通过读取pcap文件生成日志文件</p>
<h1 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h1><p><a href="https://www.docker.com/">Docker</a> 是个划时代的开源项目，它彻底释放了计算虚拟化的威力，极大提高了应用的维护效率，降低了云计算应用开发的成本。后面有机会会继续对docker的学习整理。另外，从zeek到timescaledb的入库因为使用将log文件转为csv已经实现入库，使用docker以及kafka作为中转的方案也暂时搁置，这方面之后也希望有机会能彻底分析问题来源以及尝试用其他方法构建容器间的连接</p>
]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>Zeek</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>EffectiveJava学习笔记</title>
    <url>/2021/04/06/EffectiveJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>这篇文章是对在EffectiveJava一书阅读过程中的要点和一些相关资料的整理，旨在对内容进行整理和辅助快速回忆。另外，因为EffectiveJava书中本来的结构比较松散，而且是以一条条建议的形式给出，作为一个没有太多实践经验的初学者，刚开始很难抓到重点，我在文章按照个人理解对结构进行了一些微调，个人感觉会容易理解一点</p>
<p><code>#TODO</code></p>
<a id="more"></a>

<h1 id="面向对象编程部分整理"><a href="#面向对象编程部分整理" class="headerlink" title="面向对象编程部分整理"></a>面向对象编程部分整理</h1><p>里氏替换原则：一个类型的任何重要属性也将适用于它的子类型</p>
<h2 id="创建对象"><a href="#创建对象" class="headerlink" title="创建对象"></a>创建对象</h2><p>在对类进行创建和实例化过程中的一些技巧</p>
<p><strong>静态工厂</strong></p>
<blockquote>
<p>用静态工厂方法代替构造器</p>
</blockquote>
<p>使用静态工厂<a href="https://www.jianshu.com/p/ceb5ec8f1174">https://www.jianshu.com/p/ceb5ec8f1174</a></p>
<ul>
<li>静态工厂相较于构造器更加灵活，但不适用于需要子类化的情况（鼓励使用复合） 8</li>
</ul>
<p><strong>包含多个构造器参数的类</strong></p>
<blockquote>
<p>多个构造器参数时考虑使用构建器</p>
</blockquote>
<p>构建器（生成器）<a href="https://www.liaoxuefeng.com/wiki/1252599548343744/1281319155793953">https://www.liaoxuefeng.com/wiki/1252599548343744/1281319155793953</a></p>
<ul>
<li>重叠构造器模式在参数量较多的时候，代码会较难编写且难以阅读 10</li>
<li>JavaBean模式在构造过程中可能处于不一致状态 11</li>
</ul>
<p><strong>仅可被实例化一次的类</strong></p>
<blockquote>
<p>用私有构造器或者枚举类型强化Singleton属性</p>
</blockquote>
<p>单例<a href="https://www.jianshu.com/p/eb30a388c5fc">https://www.jianshu.com/p/eb30a388c5fc</a></p>
<ul>
<li>单元素的枚举类型经常成为实现Singleton的最佳方法，但不适用于需要继承的场景 16</li>
</ul>
<p><strong>不希望被实例化的类</strong></p>
<blockquote>
<p>通过私有构造器强化不可实例化的能力</p>
</blockquote>
<ul>
<li>编写只包含静态方法和静态域的工具类时，让该类包含一个私有的构造器来防止被实例化 17</li>
<li>通过这种方法实现的工具类不可被子类化 18</li>
</ul>
<p><strong>需要引用底层资源的类</strong></p>
<blockquote>
<p>优先考虑依赖注入来引用资源</p>
</blockquote>
<ul>
<li>静态工具类和Singleton类不适合于需要引用底层资源的类 18</li>
<li>依赖注入：当创建一个新的实例时，就将该资源传到构造器中 18</li>
<li>依赖注入的对象具有不可变性 19</li>
<li>依赖注入框架：Dagger Guice Spring 19</li>
</ul>
<p><strong>建议：避免创建不必要的类</strong></p>
<ul>
<li>如果对象是不可变的，它就始终可以被重用 20</li>
<li>要优先使用基本类型，注意无意识的自动装箱 22</li>
</ul>
<p><strong>创建型模式</strong></p>
<p>创建型模式<a href="https://www.liaoxuefeng.com/wiki/1252599548343744/1281319090782242">https://www.liaoxuefeng.com/wiki/1252599548343744/1281319090782242</a></p>
<p>创建型模式关注点是如何创建对象，其核心思想是要把对象的创建和使用相分离，这样使得两者能相对独立地变换</p>
<p>创建型模式包括：</p>
<ul>
<li>工厂方法：Factory Method</li>
<li>抽象工厂：Abstract Factory</li>
<li>建造者：Builder</li>
<li>原型：Prototype</li>
<li>单例：Singleton</li>
</ul>
<h2 id="销毁对象"><a href="#销毁对象" class="headerlink" title="销毁对象"></a>销毁对象</h2><p>Java垃圾回收机制<a href="https://juejin.cn/post/6844904057602064391">https://juejin.cn/post/6844904057602064391</a></p>
<p><a href="https://segmentfault.com/a/1190000023637649">https://segmentfault.com/a/1190000023637649</a></p>
<p>垃圾回收机制保证在合适的时间清理不可到达的对象并整理相关联的存储空间，从而在一定程度降低了内存泄漏的风险</p>
<p><strong>消除过期的对象引用</strong></p>
<ul>
<li>注意无意识的对象保持（过期引用）而不会被垃圾回收机制处理导致的内存泄漏 24</li>
<li>一旦元素被释放掉，则该元素包含的任何对象引用都应该被清空 24</li>
<li>注意缓存以及监听器和其他回调导致的内存泄漏 24</li>
<li>Heap剖析工具 25</li>
</ul>
<p><strong>避免使用终结方法和清除方法</strong></p>
<ul>
<li>注重时间的任务不应该由终结方法或者清除方法来完成 26</li>
<li>终结方法和清除方法两种合法途径 27<ul>
<li>充当安全网</li>
<li>回收本地对等体</li>
</ul>
</li>
</ul>
<p><strong>try-with-resources优先于try-finally</strong></p>
<h2 id="类和接口"><a href="#类和接口" class="headerlink" title="类和接口"></a>类和接口</h2><p><strong>使类和成员的可访问性最小化</strong></p>
<ul>
<li>尽可能使每个类或者成员不被外界访问 66</li>
<li>如果一个包级私有的顶层类（或者接口）只是在某一个类内部被用到，就应该考虑将其作为唯一使用它的那个类的私有嵌套类 66</li>
<li>如果方法覆盖了超类的一个方法，子类的访问级别就不允许低于超类的访问级别（里氏替换原则） 67</li>
<li>公有类的实例域决不能是公有的（静态域除作为常量的情况同理，而且该常量不能是可变对象的引用） 67</li>
</ul>
<p><strong>要在公有类中使用访问方法而非公有域</strong></p>
<ul>
<li>如果类需要在所在包之外进行访问，应该提供访问方法，避免直接暴露数据域 69</li>
<li>如果类是包级私有的，或者是私有的嵌套类，直接暴露它的数据域没有本质的错误 70</li>
</ul>
<p><strong>使可变性最小化</strong></p>
<ul>
<li>不可变类真正唯一的缺点是，对于每个不同的值需要一个单独的对象，但可以提供一个公有的可变配套类 74</li>
<li>除非有很好的理由让类变成可变的类，否则它就应该是可变的 76</li>
<li>除非有令人信服的理由让域变成非final的，否则要使每个域都是 private final 的 76</li>
</ul>
<p><strong>复合优先于继承</strong></p>
<ul>
<li>继承实现了代码复用，但违背了封装原则。只有当两者确实存在”is-a”关系的时候，才适合用继承 81</li>
<li>复合 转发 装饰器模式 79<ul>
<li>复合：即不扩展现有类，而是在新的类中增加一个私有域，用来引用现有类的一个实例</li>
<li>转发：新类中的每个实例方法都可以调用被包含的现有类实例中的方法</li>
<li>通过复合和转发来避免继承过程中父类版本更新或者子类错误覆写方法导致的问题</li>
<li>装饰器（Decorator）模式就是使用复合和转发实现在运行期动态给某个对象的实例增加功能的方法</li>
<li>Guava为所有的集合接口提供了转发类</li>
</ul>
</li>
</ul>
<p><strong>设计继承并提供文档说明</strong></p>
<ul>
<li>待继承类必须说明所有可覆盖方法的自用性</li>
<li><code>#TODO</code></li>
</ul>
<p><strong>接口优于抽象类</strong></p>
<p>模板方法<a href="https://www.liaoxuefeng.com/wiki/1252599548343744/1281319636041762">https://www.liaoxuefeng.com/wiki/1252599548343744/1281319636041762</a></p>
<ul>
<li><code>#TODO</code></li>
</ul>
<p><strong>为后代设计接口</strong></p>
<ul>
<li>缺省方法的声明包括一个缺省实现，这是给实现了该接口但没有实现默认方法的所有类使用的 90</li>
<li>缺省方法的使用存在较大风险，谨慎设计接口 91</li>
</ul>
<p><strong>接口只用于定义类型</strong></p>
<ul>
<li>常量接口模式是对接口的不良使用 92</li>
</ul>
<p><strong>类层次优先于标签类</strong></p>
<p><strong>静态成员类优先于非静态成员类</strong></p>
<ul>
<li>静态成员类是外围类的一个静态成员 96</li>
<li>如果成员类的每个实例都需要一个指向其外围实例的引用，就把成员类做成非静态的，否则做成静态的 98</li>
<li><code>#TODO</code></li>
</ul>
<p><strong>限制源文件为单个顶级类</strong></p>
<p><strong>结构型模式</strong></p>
<p>结构型模式<a href="https://www.liaoxuefeng.com/wiki/1252599548343744/1281319233388578">https://www.liaoxuefeng.com/wiki/1252599548343744/1281319233388578</a></p>
<p>结构型模式主要涉及如何组合各种对象以便获得更好、更灵活的结构。虽然面向对象的继承机制提供了最基本的子类扩展父类的功能，但结构型模式不仅仅简单地使用继承，而更多地通过组合与运行期的动态组合来实现更灵活的功能</p>
<p>结构型模式有：</p>
<ul>
<li>适配器</li>
<li>桥接</li>
<li>组合</li>
<li>装饰器</li>
<li>外观</li>
<li>享元</li>
<li>代理</li>
</ul>
<h2 id="继承自Object的通用方法"><a href="#继承自Object的通用方法" class="headerlink" title="继承自Object的通用方法"></a>继承自Object的通用方法</h2><p><code>#TODO</code></p>
<p><strong>equals方法</strong></p>
<ul>
<li>在希望实现<code>equals</code>方法的类中，不应该增加新的值组件 38</li>
<li>首先使用<code>istanceof</code>操作符检查参数类型是否正确 41</li>
<li>对于对象引用域，可以递归调用<code>equals</code>方法，<code>float</code>域可以使用静态<code>Float.compare</code>方法 41</li>
<li>应该最先比较最有可能不一致的域，或者开销最低的域 42</li>
<li>编写完成后，检查是否满足对称性、传递性和一致性 42</li>
<li>使用AutoValue等框架自动生成<code>equals</code> <code>hashCode</code>等方法 44</li>
</ul>
<p><strong>hashCode方法</strong></p>
<ul>
<li>每个覆盖了<code>equals</code>方法的类中，都必须覆盖<code>hashCode</code>方法 44</li>
<li>相同的对象必须具有相同的散列码 44</li>
<li>一般方案是对每个域计算<code>result = 31 * result + Type.hashCode(field_name)</code> 46</li>
<li><code>Object.hash</code>一行代码，运行速度慢 47</li>
<li>对于不可变类，考虑将散列码缓存在对象内部 47</li>
</ul>
<p><strong>toString方法</strong></p>
<ul>
<li>希望始终覆盖<code>toString</code> 48</li>
<li>对象在<code>println</code> <code>printf</code> 字符串联操作符以及<code>assert</code>或者被调试器打印出来时，会自动调用<code>toString</code> 48</li>
<li><code>toString</code>方法应该返回对象中所有值得关注的内容 49</li>
</ul>
<p><strong>clone方法</strong></p>
<p><code>#TODO</code></p>
<p><strong>Comparable接口</strong></p>
<ul>
<li><p>强烈建议<code>(x.compareTo(y) == 0) == (x.equals(y))</code>（例外情况如<code>BigDecimal</code>） 59</p>
</li>
<li><p>Java 7 中，所有装箱基本类型的类中增加了静态的<code>Compare</code>方法（不建议使用<code>&lt;</code> <code>&gt;</code>） 61</p>
</li>
<li><p>Java 8 中，<code>Comparator</code>接口配置了一组比较器构造方法（简洁，额外的性能开销） 61</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Comparator&lt;PhoneNumber&gt; COMPARATOR = </span><br><span class="line">    	comparingInt((PhoneNumber pn)) -&gt; pn.areaCode)</span><br><span class="line">    	.thenComparingInt(pn -&gt; pn.prefix)</span><br><span class="line">    	.thenComparingInt(pn -&gt; pn.lineNum);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(PhoneNumber pn)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> COMPARATOR.compare(<span class="keyword">this</span>, pn);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>实现对排序敏感的类时，都应该实现<code>Comparable</code>接口，以便分类、搜索和用在基于比较的集合中 63</p>
</li>
</ul>
<h2 id="通用编程"><a href="#通用编程" class="headerlink" title="通用编程"></a>通用编程</h2><p><strong>局部变量作用域最小化</strong></p>
<ul>
<li>在变量第一次使用的地方进行声明 219</li>
<li><code>for</code>循环优先于<code>while</code>循环 220</li>
<li>使方法小而集中 221</li>
</ul>
<p><strong>for-each循环</strong></p>
<ul>
<li>当可以选择时，<code>for-each</code>循环应该优先于<code>for</code>循环 224</li>
<li><code>for-each</code>循环不会有性能损失，且隐藏了迭代器和索引变量，避免混乱和出错的可能 222</li>
<li>编写表示一组元素的类型，应该坚决考虑实现<code>Iterabel</code>接口 224</li>
</ul>
<p><strong>类库</strong></p>
<ul>
<li>需要完成一个工作时，首先考虑是否有对应的类库，优先使用类库 226</li>
</ul>
]]></content>
      <tags>
        <tag>Java</tag>
        <tag>EffectiveJava</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>ELK环境搭建&amp;流量分析</title>
    <url>/2021/01/20/ELK%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA-%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>记录使用Prometheus或ELK来搭建一个最基础的流量监控平台的过程（针对原始流量和Zeek的日志文件），对Prometheus和ELK做了一个简单的调研和整理，然后对配置过程中部分关键步骤进行一个整理</p>
 <a id="more"></a>

<p><a href="https://www.metricfire.com/blog/prometheus-vs-elk/?GAID=167038562.1611213247#span-stylefontweight-400Use-ELK-in-the-following-casesspan">https://www.metricfire.com/blog/prometheus-vs-elk/?GAID=167038562.1611213247#span-stylefontweight-400Use-ELK-in-the-following-casesspan</a></p>
<h1 id="Prometheus-amp-ELK"><a href="#Prometheus-amp-ELK" class="headerlink" title="Prometheus &amp; ELK"></a>Prometheus &amp; ELK</h1><blockquote>
<p>Prometheus和ELK都是当前流行的开源监控系统，两者各有特色</p>
<p>Prometheus专攻指标，ELK专攻日志，大多数产品会同时使用两者</p>
</blockquote>
<h2 id="Prometheus"><a href="#Prometheus" class="headerlink" title="Prometheus"></a>Prometheus</h2><p>Prometheus是一个开源的监视和警报系统，可以从指定的目标源获取指标，主要针对需要跨多种来源来收集指标并进行警报的情况</p>
<p><strong>优势</strong></p>
<ul>
<li><p>提供与Kubernetes高度集成的服务发现功能</p>
</li>
<li><p>其他部分的基础架构损坏不影响Prometheus的工作</p>
</li>
<li><p>配置简单，操作简单</p>
</li>
<li><p>对受监视服务的负载较低</p>
</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>不提供可靠的长期数据存储、异常检测、水平扩展以及用户管理</li>
</ul>
<p>Prometheus一般配合Grafana进行可视化</p>
<p>prometheus_client是一个python包，可用于很方便的创建Prometheus可以抓取的指标对象</p>
<h2 id="ELK"><a href="#ELK" class="headerlink" title="ELK"></a>ELK</h2><p>ELK是Elasticsearch、Logstash、Kibana三个开源工具的组合，从而来对日志进行可视化的分析研究，主要针对结合大量日志数据来对特定事件进行分析的情况</p>
<p>ELK一般结合filebeat来使用，filebeat将日志信息收集，发送到logstash进行处理后存入elasticsearch，然后通过kibana来进行可视化</p>
<ul>
<li><p>Filebeat：轻量级的日志采集器</p>
</li>
<li><p>Logstash：能从一个或多个来源采集、转换并将数据发送到Elasticsearch的数据处理管道</p>
</li>
<li><p>Elasticsearch：提供分布式数据存储的NoSQL数据库</p>
</li>
<li><p>Kibana：用于Elasticsearch日志的最佳可视化工具</p>
</li>
</ul>
<p><strong>优势</strong></p>
<ul>
<li>能够对大量日志数据源进行分布式的管理</li>
<li>支持纵向和水平的扩展，提供了一系列SDK</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>安装、设置和调试较为复杂</li>
<li>需要额外的工作来保证可靠性</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>Prometheus用于指标收集，并根据这些指标来设计存储模式；ELK能获取所有类型的数据，更有效的保留数据用于后续分析，同时使用倒排索引的结构，保证快速的全文搜索</li>
<li>ELK提供更长时间的数据存储，Prometheus需要结合其他存储方案来实现长期数据存储</li>
</ul>
<h1 id="zeek-Elastic-Stack"><a href="#zeek-Elastic-Stack" class="headerlink" title="zeek + Elastic Stack"></a>zeek + Elastic Stack</h1><p>使用docker来搭建ELK环境</p>
<p><a href="https://github.com/deviantony/docker-elk">https://github.com/deviantony/docker-elk</a></p>
<blockquote>
<p>docker-compose up</p>
</blockquote>
<p>Docker Compose构建ELK环境</p>
<blockquote>
<p>docker-compose down -v</p>
</blockquote>
<p>暂停容器并清除数据</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&#x27;3.2&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">elasticsearch:</span></span><br><span class="line">    <span class="attr">build:</span></span><br><span class="line">      <span class="attr">context:</span> <span class="string">elasticsearch/</span></span><br><span class="line">      <span class="attr">args:</span></span><br><span class="line">        <span class="attr">ELK_VERSION:</span> <span class="string">$ELK_VERSION</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">type:</span> <span class="string">bind</span></span><br><span class="line">        <span class="attr">source:</span> <span class="string">./elasticsearch/config/elasticsearch.yml</span></span><br><span class="line">        <span class="attr">target:</span> <span class="string">/usr/share/elasticsearch/config/elasticsearch.yml</span></span><br><span class="line">        <span class="attr">read_only:</span> <span class="literal">true</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">type:</span> <span class="string">volume</span></span><br><span class="line">        <span class="attr">source:</span> <span class="string">elasticsearch</span></span><br><span class="line">        <span class="attr">target:</span> <span class="string">/usr/share/elasticsearch/data</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;9200:9200&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;9300:9300&quot;</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="attr">ES_JAVA_OPTS:</span> <span class="string">&quot;-Xmx256m -Xms256m&quot;</span></span><br><span class="line">      <span class="attr">ELASTIC_PASSWORD:</span> <span class="string">changeme</span></span><br><span class="line">      <span class="comment"># Use single node discovery in order to disable production mode and avoid bootstrap checks.</span></span><br><span class="line">      <span class="comment"># see: https://www.elastic.co/guide/en/elasticsearch/reference/current/bootstrap-checks.html</span></span><br><span class="line">      <span class="attr">discovery.type:</span> <span class="string">single-node</span></span><br><span class="line">    <span class="attr">networks:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">elk</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">logstash:</span></span><br><span class="line">    <span class="attr">build:</span></span><br><span class="line">      <span class="attr">context:</span> <span class="string">logstash/</span></span><br><span class="line">      <span class="attr">args:</span></span><br><span class="line">        <span class="attr">ELK_VERSION:</span> <span class="string">$ELK_VERSION</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">type:</span> <span class="string">bind</span></span><br><span class="line">        <span class="attr">source:</span> <span class="string">./logstash/config/logstash.yml</span></span><br><span class="line">        <span class="attr">target:</span> <span class="string">/usr/share/logstash/config/logstash.yml</span></span><br><span class="line">        <span class="attr">read_only:</span> <span class="literal">true</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">type:</span> <span class="string">bind</span></span><br><span class="line">        <span class="attr">source:</span> <span class="string">./logstash/pipeline</span></span><br><span class="line">        <span class="attr">target:</span> <span class="string">/usr/share/logstash/pipeline</span></span><br><span class="line">        <span class="attr">read_only:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;5044:5044&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;5000:5000/tcp&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;5000:5000/udp&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;9600:9600&quot;</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="attr">LS_JAVA_OPTS:</span> <span class="string">&quot;-Xmx256m -Xms256m&quot;</span></span><br><span class="line">    <span class="attr">networks:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">elk</span></span><br><span class="line">    <span class="attr">depends_on:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">elasticsearch</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">kibana:</span></span><br><span class="line">    <span class="attr">build:</span></span><br><span class="line">      <span class="attr">context:</span> <span class="string">kibana/</span></span><br><span class="line">      <span class="attr">args:</span></span><br><span class="line">        <span class="attr">ELK_VERSION:</span> <span class="string">$ELK_VERSION</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">type:</span> <span class="string">bind</span></span><br><span class="line">        <span class="attr">source:</span> <span class="string">./kibana/config/kibana.yml</span></span><br><span class="line">        <span class="attr">target:</span> <span class="string">/usr/share/kibana/config/kibana.yml</span></span><br><span class="line">        <span class="attr">read_only:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;5601:5601&quot;</span></span><br><span class="line">    <span class="attr">networks:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">elk</span></span><br><span class="line">    <span class="attr">depends_on:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">elasticsearch</span></span><br><span class="line"></span><br><span class="line"><span class="attr">networks:</span></span><br><span class="line">  <span class="attr">elk:</span></span><br><span class="line">    <span class="attr">driver:</span> <span class="string">bridge</span></span><br><span class="line"></span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line">  <span class="attr">elasticsearch:</span></span><br></pre></td></tr></table></figure>

<p>上面是docker-compose的配置文件，ELK的配置文件分别在docker-elk的elasticsearch、logstash、kibana文件夹下的config文件夹下，三个容器通过使用docker建立的elk网络桥接起来，通过命令<code>docker network network ls</code>可以查看该网络</p>
<p>ELK提供了通过filebeat来收集zeek日志的方法</p>
<blockquote>
<p>sudo docker run –rm –cap-add=NET_RAW –net=host -v <code>pwd</code>:/pcap:rw blacktop/zeek:elastic -i af_packet::eth0 local</p>
</blockquote>
<p>使用docker运行zeek来监听eth0网口的流量</p>
<p>参考官方在<a href="http://localhost:5601/app/home#/tutorial/zeekLogs%E7%9A%84%E6%96%B9%E6%B3%95%E5%AE%89%E8%A3%85filebeat">http://localhost:5601/app/home#/tutorial/zeekLogs的方法安装filebeat</a></p>
<p>其中，可以对zeek.yml进行修改</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">module:</span> <span class="string">zeek</span></span><br><span class="line">  <span class="attr">capture_loss:</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">connection:</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">var.paths:</span> [<span class="string">&quot;[path]/conn.log&quot;</span>]</span><br></pre></td></tr></table></figure>

<p>上面是一个简单的例子，对不需要的日志置false，为需要收集的日志配置响应日志所在的路径，ubuntu系统zeek采集实时流量的日志应在<code>/usr/local/zeek/logs/current</code>文件夹下，即<code>var.paths: [&quot;/usr/local/zeek/logs/current/conn.log&quot;]</code></p>
<p>回到<a href="http://localhost:5601/app/home#/tutorial/zeekLogs%EF%BC%8C%E5%9C%A8%E6%9C%80%E5%90%8E%E4%B8%80%E6%AD%A5check">http://localhost:5601/app/home#/tutorial/zeekLogs，在最后一步check</a> data，成功则点击Zeek Overview进入kibana为zeek设计的界面</p>
<p>侧边栏Kibana的Discover可以查看存储在elasticsearch的数据</p>
<h2 id="离线分析的情况"><a href="#离线分析的情况" class="headerlink" title="离线分析的情况"></a>离线分析的情况</h2><p>使用logstash来做日志收集，logstash会根据事件传输的当前时间自动给事件加上@timestamp字段，但针对离线分析的情况，则需要手动指定时间戳</p>
<p>通过使用filebeat来处理zeek能够自动将zeek的ts时间作为索引存入elasticsearch，以此为基础可以进行离线分析</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo filebeat setup</span><br><span class="line"></span><br><span class="line">sudo service filebeat start</span><br><span class="line"></span><br><span class="line">sudo docker run --rm -v `<span class="built_in">pwd</span>`:/pcap blacktop/zeek -r dns-traffic.20140409.pcap <span class="built_in">local</span> <span class="string">&quot;Site::local_nets += &#123; 192.168.11.0/24 &#125;&quot;</span></span><br></pre></td></tr></table></figure>

<h1 id="ELK监测网络流量"><a href="#ELK监测网络流量" class="headerlink" title="ELK监测网络流量"></a>ELK监测网络流量</h1><p><a href="https://www.elastic.co/guide/en/beats/packetbeat/current/packetbeat-installation-configuration.html">https://www.elastic.co/guide/en/beats/packetbeat/current/packetbeat-installation-configuration.html</a></p>
<p>以ubuntu为例，使用命令</p>
<blockquote>
<p>curl -L -O <a href="https://artifacts.elastic.co/downloads/beats/packetbeat/packetbeat-7.10.2-amd64.deb">https://artifacts.elastic.co/downloads/beats/packetbeat/packetbeat-7.10.2-amd64.deb</a></p>
</blockquote>
<blockquote>
<p>sudo dpkg -i packetbeat-7.10.2-amd64.deb</p>
</blockquote>
<p>配置/etc/packetbeat/packetbeat.yml</p>
<p>修改kibana和elasticsearch的配置部分</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">setup.kibana:</span></span><br><span class="line">  <span class="attr">host:</span> <span class="string">&quot;localhost:5601&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">output.elasticsearch:</span></span><br><span class="line">  <span class="attr">hosts:</span> [<span class="string">&quot;localhost:9200&quot;</span>]</span><br><span class="line">  <span class="attr">username:</span> <span class="string">&quot;elastic&quot;</span></span><br><span class="line">  <span class="attr">password:</span> <span class="string">&quot;changeme&quot;</span></span><br></pre></td></tr></table></figure>

<p>在配置文件上加上嗅探类型</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">packetbeat.interfaces.type:</span> <span class="string">af_packet</span></span><br></pre></td></tr></table></figure>

<p>选择网络接口，packetbeat提供了<code>packetbeat devices</code>的命令来查询</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">packetbeat.interfaces.device:</span> <span class="string">eth0</span></span><br></pre></td></tr></table></figure>

<p>其他配置参考<a href="https://www.elastic.co/guide/en/beats/packetbeat/current/configuring-howto-packetbeat.html">https://www.elastic.co/guide/en/beats/packetbeat/current/configuring-howto-packetbeat.html</a></p>
<p>类似于filebeat，使用下述命令启动服务</p>
<blockquote>
<p>packetbeat setup</p>
</blockquote>
<blockquote>
<p>sudo service packetbeat start</p>
</blockquote>
<p>侧边栏Kibana的Discover可以查看存储在elasticsearch的数据</p>
<p>可以在Dashboard处自定义图表来可视化监控到的流量</p>
<h2 id="离线分析的情况-1"><a href="#离线分析的情况-1" class="headerlink" title="离线分析的情况"></a>离线分析的情况</h2><p><a href="https://www.elastic.co/cn/blog/analyzing-network-packets-with-wireshark-elasticsearch-and-kibana">https://www.elastic.co/cn/blog/analyzing-network-packets-with-wireshark-elasticsearch-and-kibana</a></p>
<p><a href="https://harrunisk.github.io/2018-09-12-ElasticWifiAnalysisEn/">https://harrunisk.github.io/2018-09-12-ElasticWifiAnalysisEn/</a></p>
<p>上面几种方案尝试了一下都没能成功将pcap导入elasticsearch，可能是哪里疏忽了</p>
<p>使用tshark导出pcap包中流量的三元组</p>
<blockquote>
<p>tshark -r example.pcap -T fields -e frame.time -e ip.src -e ip.dst -e ip.proto -E header=y -E separator=, -E quote=d -E occurrence=f &gt; pcap.csv</p>
</blockquote>
<p><code>#TODO</code></p>
<h1 id="kafka-amp-prometheus"><a href="#kafka-amp-prometheus" class="headerlink" title="kafka&amp;prometheus"></a>kafka&amp;prometheus</h1><p>之后尝试使用kafka和prometheus来进行流量分析，发现使用jmx_exporter和node_exporter prometheus只能采集到kafka本身的指标，官方似乎也没有提供将流量数据通过kafka导入prometheus的方案，具体原因<code>#TODO</code></p>
<p><a href="https://www.metricfire.com/blog/kafka-monitoring-using-prometheus/?GAID=167038562.1611213247&amp;GAID=167038562.1611213247">https://www.metricfire.com/blog/kafka-monitoring-using-prometheus/?GAID=167038562.1611213247&amp;GAID=167038562.1611213247</a></p>
<ul>
<li>注意缩进</li>
<li><code>sudo chown -R 472:472 grafana/</code></li>
</ul>
<p><a href="https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/">https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/</a></p>
]]></content>
      <tags>
        <tag>Prometheus</tag>
        <tag>ELK</tag>
        <tag>流量分析</tag>
        <tag>环境搭建</tag>
      </tags>
  </entry>
  <entry>
    <title>JustEnoughScalaforSpark笔记</title>
    <url>/2021/11/25/JustEnoughScalaforSpark%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>作为学习Spark的过程，感觉即便考虑使用Java进行编程，对Scala有个一定的理解还是有必要的，也能方便阅读其他人的代码，然后发现一个名字挺有诱惑力的项目<a href="https://github.com/deanwampler/JustEnoughScalaForSpark">https://github.com/deanwampler/JustEnoughScalaForSpark</a> ，遂上手看了一下，记些笔记供以后查阅</p>
<a id="more"></a>

<h1 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h1><p><strong>注释</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 单行注释</span></span><br><span class="line"><span class="comment">/*多行注释*/</span></span><br></pre></td></tr></table></figure>

<p><strong>常量&amp;变量</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> immutableValue = <span class="string">&quot;常量&quot;</span></span><br><span class="line"><span class="keyword">var</span> mutableVariable = <span class="string">&quot;变量&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>函数</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function_name</span> </span>= &#123;&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function_name</span></span>(parameter:<span class="type">Type</span>):<span class="type">Type</span> = &#123;</span><br><span class="line">    return_value <span class="comment">//最后一行默认表示返回值，return关键字可选，;可选</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>字符串</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="string">s&quot;&quot;</span><span class="string">&quot;这是一个跨行的字符串</span></span><br><span class="line"><span class="string">|  字符串前添加s表示可以内嵌变量和表达式$sc.version$  &quot;</span><span class="string">&quot;&quot;</span>.stripMargin <span class="comment">//stripMargin可以移除空格，结合|可以控制缩进</span></span><br><span class="line"><span class="string">&quot;/&quot;</span> <span class="comment">//String</span></span><br><span class="line">&#x27;/&#x27; <span class="comment">//Char</span></span><br></pre></td></tr></table></figure>

<p><strong>判断</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> success = <span class="keyword">if</span> (shakespeare.exists == <span class="literal">false</span>) &#123; <span class="comment">//if 结构在Scala是表达式</span></span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>循环</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">  play &lt;- plays</span><br><span class="line">  ...</span><br><span class="line">&#125; <span class="keyword">yield</span> &#123; block_of_final_expressions &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> failures = <span class="keyword">for</span> &#123;</span><br><span class="line">    play &lt;- plays</span><br><span class="line">    playFileName = targetDirName + pathSeparator + play</span><br><span class="line">    playFile = <span class="keyword">new</span> <span class="type">File</span>(playFileName)</span><br><span class="line">    <span class="keyword">if</span> (playFile.exists == <span class="literal">false</span>)</span><br><span class="line">&#125; <span class="keyword">yield</span> &#123;</span><br><span class="line">    <span class="string">s&quot;<span class="subst">$playFileName</span>:\tNOT FOUND!&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">plays.foreach(println) <span class="comment">//直接传入函数</span></span><br><span class="line"></span><br><span class="line">plays.foreach(str =&gt; println(str)) <span class="comment">//匿名函数</span></span><br><span class="line"></span><br><span class="line">plays.foreach((str: <span class="type">String</span>) =&gt; println(str)) <span class="comment">//明确参数类型</span></span><br><span class="line"></span><br><span class="line">plays.foreach(println(_)) <span class="comment">//省去对参数命名</span></span><br><span class="line"></span><br><span class="line">plays.foreach &#123;</span><br><span class="line">  (str: <span class="type">String</span>) =&gt; println(str)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> integers = <span class="number">0</span> to <span class="number">10</span>   <span class="comment">// Return a &quot;range&quot; from 0 to 10, inclusive</span></span><br><span class="line">integers.reduceLeft((i,j) =&gt; i+j)</span><br><span class="line">integers.reduceLeft(_+_)</span><br></pre></td></tr></table></figure>

<p><strong>tips</strong></p>
<ul>
<li>Scala 可以使用所有 Java 库</li>
<li><code>(String,String)</code> 等价于<code>Tuple2[String,String]</code></li>
</ul>
<h1 id="Scala面向对象"><a href="#Scala面向对象" class="headerlink" title="Scala面向对象"></a>Scala面向对象</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IIRecord1</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    word: <span class="type">String</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">    total_count: <span class="type">Int</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">    locations: <span class="type">Array</span>[<span class="type">String</span>], </span></span></span><br><span class="line"><span class="class"><span class="params">    counts: <span class="type">Array</span>[<span class="type">Int</span>]</span>) </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** CSV formatted string, but use [a,b,c] for the arrays */</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> locStr = locations.mkString(<span class="string">&quot;[&quot;</span>, <span class="string">&quot;,&quot;</span>, <span class="string">&quot;]&quot;</span>)  <span class="comment">// i.e., &quot;[a,b,c]&quot;</span></span><br><span class="line">        <span class="keyword">val</span> cntStr = counts.mkString(<span class="string">&quot;[&quot;</span>, <span class="string">&quot;,&quot;</span>, <span class="string">&quot;]&quot;</span>)  <span class="comment">// i.e., &quot;[1,2,3]&quot;</span></span><br><span class="line">        <span class="string">s&quot;<span class="subst">$word</span>,<span class="subst">$total_count</span>,<span class="subst">$locStr</span>,<span class="subst">$cntStr</span>&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">new</span> <span class="type">IIRecord1</span>(<span class="string">&quot;hello&quot;</span>, <span class="number">3</span>, <span class="type">Array</span>(<span class="string">&quot;one&quot;</span>, <span class="string">&quot;two&quot;</span>), <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<p><strong>Object</strong></p>
<ul>
<li><strong>Object</strong>是Scala内置的用来定义单例（只能被实例一次的类）的关键字</li>
<li>Scala没有static关键字，使用static的场景可以用object进行代替</li>
</ul>
<p><strong>Case Class</strong></p>
<ul>
<li><p>Scala使用为参数设置默认值来避免使用多个构造器的情况</p>
</li>
<li><p>case关键字为使用其定义的类提供如下特性</p>
<ul>
<li>所有构造器参数被认为是实例的私有常量</li>
<li>生成公有的与类名相同的reader方法</li>
<li>生成正确的<code>equals</code>、<code>hashCode</code>和<code>toString</code>方法实现</li>
<li>生成一个同名的object，称为<em>companion object</em></li>
<li>在<em>companion object</em>中生成一个工厂方法，接受相同的参数并进行实例化</li>
<li>在<em>companion object</em>中生成一个helper方法并支持特征匹配</li>
</ul>
<p>上述特性省去了使用Java时需要花精力去注意的很多工作，包括<a href="https://tangziyin.com/2021/04/06/EffectiveJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">EffectiveJava</a>中提到的只提供访问方法和通用方法的复写等，<em>companion object</em>提供的工厂方法和helper方法也保证了Scala中类的使用更加灵活</p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">IIRecord</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    word: <span class="type">String</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">    total_count: <span class="type">Int</span> = 0, </span></span></span><br><span class="line"><span class="class"><span class="params">    locations: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>.empty, </span></span></span><br><span class="line"><span class="class"><span class="params">    counts: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>.empty</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** </span></span><br><span class="line"><span class="comment">     * Different than our CSV output above, but see toCSV.</span></span><br><span class="line"><span class="comment">     * Array.toString is useless, so format these ourselves. </span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = </span><br><span class="line">        <span class="string">s&quot;&quot;</span><span class="string">&quot;IIRecord($word, $total_count, $locStr, $cntStr)&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** CSV-formatted string, but use [a,b,c] for the arrays */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">toCSV</span></span>: <span class="type">String</span> = </span><br><span class="line">        <span class="string">s&quot;<span class="subst">$word</span>,<span class="subst">$total_count</span>,<span class="subst">$locStr</span>,<span class="subst">$cntStr</span>&quot;</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">/** Return a JSON-formatted string for the instance. */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">toJSONString</span></span>: <span class="type">String</span> = </span><br><span class="line">        <span class="string">s&quot;&quot;</span><span class="string">&quot;&#123;</span></span><br><span class="line"><span class="string">        |  &quot;</span><span class="string">word&quot;:        &quot;</span>$<span class="string">word&quot;, </span></span><br><span class="line"><span class="string">        |  &quot;</span>total_<span class="string">count&quot;: <span class="subst">$total_count</span>, </span></span><br><span class="line"><span class="string">        |  &quot;</span><span class="string">locations&quot;:   <span class="subst">$&#123;toJSONArrayString(locations)&#125;</span>,</span></span><br><span class="line"><span class="string">        |  &quot;</span><span class="string">counts&quot;       <span class="subst">$&#123;toArrayString(counts, &quot;, &quot;)&#125;</span></span></span><br><span class="line"><span class="string">        |&#125;</span></span><br><span class="line"><span class="string">        |&quot;</span><span class="string">&quot;&quot;</span>.stripMargin</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">locStr</span> </span>= toArrayString(locations)</span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">cntStr</span> </span>= toArrayString(counts)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// &quot;[_]&quot; means we don&#x27;t care what type of elements; we&#x27;re just</span></span><br><span class="line">    <span class="comment">// calling toString on them!</span></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">toArrayString</span></span>(array: <span class="type">Array</span>[_], delim: <span class="type">String</span> = <span class="string">&quot;,&quot;</span>): <span class="type">String</span> = </span><br><span class="line">        array.mkString(<span class="string">&quot;[&quot;</span>, delim, <span class="string">&quot;]&quot;</span>)  <span class="comment">// i.e., &quot;[a,b,c]&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">toJSONArrayString</span></span>(array: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">String</span> =</span><br><span class="line">        toArrayString(array.map(quote), <span class="string">&quot;, &quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">quote</span></span>(word: <span class="type">String</span>): <span class="type">String</span> = <span class="string">&quot;\&quot;&quot;</span> + word + <span class="string">&quot;\&quot;&quot;</span>  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>下面两种表达等价</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> hello1 = <span class="keyword">new</span> <span class="type">IIRecord</span>(<span class="string">&quot;hello1&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> hello2 = <span class="type">IIRecord</span>(<span class="string">&quot;hello2&quot;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>Scala允许object和class名以小写字母开始，并向函数或方法的形式进行调用</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">stem</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(word: <span class="type">String</span>): <span class="type">String</span> = word.replaceFirst(<span class="string">&quot;s$&quot;</span>, <span class="string">&quot;&quot;</span>) <span class="comment">// insert real implementation!</span></span><br><span class="line">&#125;</span><br><span class="line">println(stem(<span class="string">&quot;dog&quot;</span>))</span><br><span class="line">println(stem(<span class="string">&quot;dogs&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//dog</span></span><br><span class="line"><span class="comment">//dog</span></span><br></pre></td></tr></table></figure>

<ul>
<li><em>companion object</em>支持模式匹配</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">Seq</span>(hello, world).map &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">IIRecord</span>(word, <span class="number">0</span>, _, _) =&gt; <span class="string">s&quot;<span class="subst">$word</span> with no occurrences.&quot;</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">IIRecord</span>(word, cnt, locs, cnts) =&gt; </span><br><span class="line">        <span class="string">s&quot;<span class="subst">$word</span> occurs <span class="subst">$cnt</span> times: <span class="subst">$&#123;locs.zip(cnts).mkString(&quot;, &quot;)&#125;</span>&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Scala-for-Spark"><a href="#Scala-for-Spark" class="headerlink" title="Scala for Spark"></a>Scala for Spark</h1><ul>
<li>demo1是一个经典的WordCount实现</li>
<li>demo2在demo1的基础上使用了case关键字来优化代码</li>
<li>demo3对demo1的输出结果的格式进行修改以进行SQL查询</li>
</ul>
<h2 id="demo1"><a href="#demo1" class="headerlink" title="demo1"></a>demo1</h2><p>这边我先列出了一些demo中用到的tips，可以和demo代码相互印证</p>
<ul>
<li><p><strong>flatMap</strong>能够遍历集合的每个元素，并对应这些元素建立集合，最终将这些集合平铺到一个大集合中</p>
</li>
<li><p>匿名函数有超过一个参数时需要使用圆括号包裹这些参数</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">(some_tuple3: (<span class="type">String</span>,<span class="type">Int</span>,<span class="type">Double</span>)) =&gt; ...</span><br><span class="line">(arg1, arg2, arg3) =&gt; ...</span><br><span class="line">(arg1: <span class="type">String</span>, arg2: <span class="type">Int</span>, arg3: <span class="type">Double</span>) =&gt; ...</span><br></pre></td></tr></table></figure>
</li>
<li><p>Scala在表示多行匿名函数时使用大括号后包裹匿名函数，不同于单行匿名函数的圆括号</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">flatMap &#123; location_contents_tuple2 =&gt; </span><br><span class="line">        <span class="keyword">val</span> words = location_contents_tuple2._2.split(<span class="string">&quot;&quot;</span><span class="string">&quot;\W+&quot;</span><span class="string">&quot;&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> fileName = location_contents_tuple2._1.split(pathSeparator).last</span><br><span class="line">        words.map(word =&gt; ((word, fileName), <span class="number">1</span>))</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>PS：Python使用圆括号包裹匿名函数的函数体</p>
</li>
<li><p>Tuple2可以使用<code>_1</code> 和<code>_2</code>方法访问第一和第二个元素（注意不是<code>_0</code>和<code>_1</code>）</p>
</li>
<li><p>对于使用”””表示的字符串，在使用正则表达式时，不需要额外进行转义<code>&quot;\\W+&quot; =&gt; &quot;&quot;&quot;\W+&quot;&quot;&quot;</code></p>
</li>
<li><p><strong>reduceByKey</strong>对所有相同Key（第一个元素）的Tuple进行groupBy操作，Value按照传入的匿名函数进行聚合</p>
<p>Spark的所有<strong>*ByKey</strong>都针对两个元素的Tuple，其中第一个元素作为Key，第二个元素作为Value</p>
</li>
<li><p><strong>groupByKey</strong>不需要传入匿名函数，按Key聚合后Value会全部放入一个CompactBuffer对象中作为一个Group</p>
</li>
<li><p><strong>mapValues</strong>相较于<strong>map</strong>只针对元素为二元组且只操作Value的情况</p>
</li>
<li><p>Vector的sortBy方法传入的匿名函数参数表示Vector的每个元素，返回用于排序的部分</p>
<p><code>file_count_tup2 =&gt; (-file_count_tup2._2, file_count_tup2._1)</code>表示首先按二元组的第二个元素降序排序，再按第一个元素升序排序</p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.<span class="type">File</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> shakespeare = <span class="keyword">new</span> <span class="type">File</span>(<span class="string">&quot;/home/jovyan/work/data/shakespeare&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> iiFirstPass1 = sc.wholeTextFiles(shakespeare.toString).</span><br><span class="line">    flatMap &#123; location_contents_tuple2 =&gt; </span><br><span class="line">        <span class="keyword">val</span> words = location_contents_tuple2._2.split(<span class="string">&quot;&quot;</span><span class="string">&quot;\W+&quot;</span><span class="string">&quot;&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> fileName = location_contents_tuple2._1.split(pathSeparator).last</span><br><span class="line">        words.map(word =&gt; ((word, fileName), <span class="number">1</span>))</span><br><span class="line">    &#125;.</span><br><span class="line">    reduceByKey((count1, count2) =&gt; count1 + count2).</span><br><span class="line">    map &#123; word_file_count_tup3 =&gt; </span><br><span class="line">        (word_file_count_tup3._1._1, (word_file_count_tup3._1._2, word_file_count_tup3._2)) </span><br><span class="line">    &#125;.</span><br><span class="line">    groupByKey.</span><br><span class="line">    sortByKey(ascending = <span class="literal">true</span>).</span><br><span class="line">    mapValues &#123; iterable =&gt; </span><br><span class="line">        <span class="keyword">val</span> vect = iterable.toVector.sortBy &#123; file_count_tup2 =&gt; </span><br><span class="line">            (-file_count_tup2._2, file_count_tup2._1)</span><br><span class="line">        &#125;</span><br><span class="line">        vect.mkString(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<ol>
<li><p>sc.wholeTextFiles获取shakespeare路径下每个文档（文档路径，文档内容）的二元组组成的集合RDD</p>
</li>
<li><p>flatMap对RDD每个二元组遍历，将文档内容按照单词分割建立一个数组，文档路径只保留文件名，最终原RDD的每个二元组被处理成一个包含多个（（单词，文件名），1）结构的数组，flatMap自动将最后得到的所有数组平铺成一个新的RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">((<span class="type">AS</span>,asyoulikeit),<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>reduceByKey按照新RDD的Key值，如上面的(AS,asyoulikeit)对所有Key相同的元素进行整合，对Value进行相加操作，于是便得到所有相同文件相同单词的数量</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">((<span class="type">All</span>,twelfthnight),<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>map通过遍历RDD的每个元素对所有元素的格式进行修改</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">(<span class="type">All</span>,(twelfthnight,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>groupByKey和sortByKey将RDD按照Key值进行聚合和排序，相同Key的Value通过CompactBuffer对象进行存储</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">(<span class="type">A</span>,<span class="type">CompactBuffer</span>((loveslabourslost,<span class="number">78</span>), (midsummersnightsdream,<span class="number">39</span>), (muchadoaboutnothing,<span class="number">31</span>), (merrywivesofwindsor,<span class="number">38</span>), (comedyoferrors,<span class="number">42</span>), (asyoulikeit,<span class="number">34</span>), (twelfthnight,<span class="number">47</span>), (tamingoftheshrew,<span class="number">59</span>)))</span><br></pre></td></tr></table></figure>
</li>
<li><p>mapValues对Value遍历，即上面的CompactBuffer对象，通过将其转化为Vector来使用Vector的sortBy方法和mkString方法进行排序和字符串化，最终得到如下结构的一系列元素</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">(<span class="type">A</span>,(loveslabourslost,<span class="number">78</span>),(tamingoftheshrew,<span class="number">59</span>),(twelfthnight,<span class="number">47</span>),(comedyoferrors,<span class="number">42</span>),(midsummersnightsdream,<span class="number">39</span>),(merrywivesofwindsor,<span class="number">38</span>),(asyoulikeit,<span class="number">34</span>),(muchadoaboutnothing,<span class="number">31</span>))</span><br><span class="line">(<span class="type">ABOUT</span>,(muchadoaboutnothing,<span class="number">18</span>))</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="demo2"><a href="#demo2" class="headerlink" title="demo2"></a>demo2</h2><ul>
<li>使用case关键词可以在传入参数时将元组分解</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> ii1 = sc.wholeTextFiles(shakespeare.toString).</span><br><span class="line">    flatMap &#123;</span><br><span class="line">        <span class="keyword">case</span> (location, contents) =&gt; </span><br><span class="line">            <span class="keyword">val</span> words = contents.split(<span class="string">&quot;&quot;</span><span class="string">&quot;\W+&quot;</span><span class="string">&quot;&quot;</span>).</span><br><span class="line">                filter(word =&gt; word.size &gt; <span class="number">0</span>)                      <span class="comment">// #1</span></span><br><span class="line">            <span class="keyword">val</span> fileName = location.split(pathSeparator).last</span><br><span class="line">            words.map(word =&gt; ((word.toLowerCase, fileName), <span class="number">1</span>))   <span class="comment">// #2</span></span><br><span class="line">    &#125;.</span><br><span class="line">    reduceByKey((count1, count2) =&gt; count1 + count2).</span><br><span class="line">    map &#123; </span><br><span class="line">        <span class="keyword">case</span> ((word, fileName), count) =&gt; (word, (fileName, count)) </span><br><span class="line">    &#125;.</span><br><span class="line">    groupByKey.</span><br><span class="line">    sortByKey(ascending = <span class="literal">true</span>).</span><br><span class="line">    mapValues &#123; iterable =&gt; </span><br><span class="line">        <span class="keyword">val</span> vect = iterable.toVector.sortBy &#123; </span><br><span class="line">            <span class="keyword">case</span> (fileName, count) =&gt; (-count, fileName) </span><br><span class="line">        &#125;</span><br><span class="line">        vect.mkString(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>使用多个case语句来进行参数匹配，这种模式的匿名函数被称为<em>partial function</em></li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="keyword">case</span> (location, <span class="string">&quot;&quot;</span>) =&gt; </span><br><span class="line">        <span class="type">Array</span>.empty[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)]  <span class="comment">// Return an empty array</span></span><br><span class="line">    <span class="keyword">case</span> (location, contents) =&gt; </span><br><span class="line">        <span class="keyword">val</span> words = contents.split(<span class="string">&quot;&quot;</span><span class="string">&quot;\W+&quot;</span><span class="string">&quot;&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> fileName = location.split(pathSep).last</span><br><span class="line">        words.map(word =&gt; ((word, fileName), <span class="number">1</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="demo3"><a href="#demo3" class="headerlink" title="demo3"></a>demo3</h2><ul>
<li><p>本例目的是将最终结果转换成便于进行SQL查询的格式并进行简单的查询，例如下面的格式</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">(a,<span class="number">3350</span>,<span class="type">Vector</span>(loveslabourslost, merrywivesofwindsor, muchadoaboutnothing, asyoulikeit, tamingoftheshrew, twelfthnight, midsummersnightsdream, comedyoferrors),<span class="type">Vector</span>(<span class="number">507</span>, <span class="number">494</span>, <span class="number">492</span>, <span class="number">461</span>, <span class="number">445</span>, <span class="number">416</span>, <span class="number">281</span>, <span class="number">254</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>zip和unzip方法正如现实中拉链拉开拉上的过程，可以将长度一样的两个集合合并成一个每个元素是一个二元组的集合，或者将一个二元组集合分开成两个集合，其中二元组集合中二元组的第一个元素与分开后第一个集合的元素能一一对应</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">[(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">3</span>,<span class="number">4</span>),(<span class="number">5</span>,<span class="number">6</span>)] =&gt; ([<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>],[<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>]) <span class="comment">//unzip</span></span><br><span class="line">([<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>],[<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>]) =&gt; [(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">3</span>,<span class="number">4</span>),(<span class="number">5</span>,<span class="number">6</span>)] <span class="comment">//zip</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> ii = sc.wholeTextFiles(shakespeare.toString).</span><br><span class="line">    flatMap &#123;</span><br><span class="line">        <span class="keyword">case</span> (location, contents) =&gt; </span><br><span class="line">            <span class="keyword">val</span> words = contents.split(<span class="string">&quot;&quot;</span><span class="string">&quot;\W+&quot;</span><span class="string">&quot;&quot;</span>).</span><br><span class="line">                filter(word =&gt; word.size &gt; <span class="number">0</span>)                      <span class="comment">// #1</span></span><br><span class="line">            <span class="keyword">val</span> fileName = location.split(pathSeparator).last</span><br><span class="line">            words.map(word =&gt; ((word.toLowerCase, fileName), <span class="number">1</span>))   <span class="comment">// #2</span></span><br><span class="line">    &#125;.</span><br><span class="line">    reduceByKey((count1, count2) =&gt; count1 + count2).</span><br><span class="line">    map &#123; </span><br><span class="line">        <span class="keyword">case</span> ((word, fileName), count) =&gt; (word, (fileName, count)) </span><br><span class="line">    &#125;.</span><br><span class="line">    groupByKey.</span><br><span class="line">    sortByKey(ascending = <span class="literal">true</span>).</span><br><span class="line">    map &#123;</span><br><span class="line">      <span class="keyword">case</span> (word, iterable) =&gt;</span><br><span class="line">        <span class="keyword">val</span> vect = iterable.toVector.sortBy &#123; </span><br><span class="line">            <span class="keyword">case</span> (fileName, count) =&gt; (-count, fileName) </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> (locations, counts) = vect.unzip  </span><br><span class="line">        <span class="keyword">val</span> totalCount = counts.reduceLeft((n1,n2) =&gt; n1+n2)</span><br><span class="line">        (word, totalCount, locations, counts)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>使用SQL语句进行查询</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SQLContext</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"><span class="keyword">val</span> iiDF = sqlContext.createDataFrame(ii).toDF(<span class="string">&quot;word&quot;</span>, <span class="string">&quot;locations_counts&quot;</span>)</span><br><span class="line">iiDF.cache</span><br><span class="line">iiDF.registerTempTable(<span class="string">&quot;inverted_index&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> topLocations = sqlContext.sql(<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    SELECT word, total_count, locations[0] AS top_location, counts[0] AS top_count</span></span><br><span class="line"><span class="string">    FROM inverted_index </span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+-----------+-----------+---------------------+---------+</span><br><span class="line">|word       |total_count|top_location         |top_count|</span><br><span class="line">+-----------+-----------+---------------------+---------+</span><br><span class="line">|a          |<span class="number">3350</span>       |loveslabourslost     |<span class="number">507</span>      |</span><br><span class="line">|abandon    |<span class="number">6</span>          |asyoulikeit          |<span class="number">4</span>        |</span><br><span class="line">|abate      |<span class="number">3</span>          |loveslabourslost     |<span class="number">1</span>        |</span><br><span class="line">|abatement  |<span class="number">1</span>          |twelfthnight         |<span class="number">1</span>        |</span><br><span class="line">|abbess     |<span class="number">8</span>          |comedyoferrors       |<span class="number">8</span>        |</span><br><span class="line">|abbey      |<span class="number">9</span>          |comedyoferrors       |<span class="number">9</span>        |</span><br><span class="line">+-----------+-----------+---------------------+---------+</span><br></pre></td></tr></table></figure>

<ul>
<li>在DataFrame之外Spark提供了Dataset，相较提供了类型保障，类似于RTT[T]，T表示记录的类型</li>
</ul>
<h1 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h1><ul>
<li><p>Scala支持将*****作为方法名，在实例之间进行乘法操作时进行调用</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Matrix</span>(<span class="params">rows: <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Double</span>]]</span>) </span>&#123;  <span class="comment">// Each row is an Array[Double]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Multiple this matrix by another. */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">*</span></span>(other: <span class="type">Matrix</span>): <span class="type">Matrix</span> = ...</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Add this matrix by another. */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">+</span></span>(other: <span class="type">Matrix</span>): <span class="type">Matrix</span> = ...</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> m1_times_m2 = m1.*(m2) <span class="comment">//两者等同</span></span><br><span class="line"><span class="keyword">val</span> m1_times_m2 = m1 * m2</span><br></pre></td></tr></table></figure>
</li>
<li><p>Scala提供<strong>Traits</strong>作为关键字提供类似Java接口的功能<a href="https://scala.cool/2018/11/java-2-scala-4/">https://scala.cool/2018/11/java-2-scala-4/</a></p>
</li>
<li><p>Scala提供<strong>until</strong>和<strong>to</strong>两种关键词生成一个Range，其中until左闭右开，to左闭右闭，另外使用<strong>by</strong>关键字控制步长</p>
</li>
<li><p><strong>Option</strong>功能类似于Try，可以用来表示一个值是可选的（有值或无值）</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">options.foreach &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span>    =&gt; println(<span class="type">None</span>)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(i) =&gt; println(i)  <span class="comment">// Note how we extract the enclosed value.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Implicits</strong><a href="https://www.jianshu.com/p/1d119c937015">https://www.jianshu.com/p/1d119c937015</a></p>
</li>
</ul>
]]></content>
      <tags>
        <tag>学习笔记</tag>
        <tag>Spark</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>RobustPeriod算法框架学习</title>
    <url>/2021/11/22/RobustPeriod%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>在AIOps了解到RobustPeriod这一算法框架来对时序数据进行处理论文</p>
<p><a href="https://developer.aliyun.com/article/782285">https://developer.aliyun.com/article/782285</a></p>
<p>囿于本人相关数学知识的积累不足，本文主要是将整体思路和公式说明进行尽可能准确的简单描述，之后也是计划与<a href="https://github.com/ariaghora/robust-period">https://github.com/ariaghora/robust-period</a> 的实现进行对应，尝试在自己的AIOps场景中进行实现</p>
<p>PS：本文只进行了要对算法进行理解复现的最低限度的学习，有兴趣相关研究的还是需要把论文的数学证明和主要的几篇相关文献也看一下了</p>
<a id="more"></a>
<p>$$<br>y_t = \tau_t + \sum_{i=1}^ms_{i,t} + r_t,  t=0,1,…,N-1<br>$$<br>文中将时序数据进行如上的数学表示，实际时序数据的函数由趋势函数$\tau_t$、m个周期分量函数$s_{i,t}$以及$r_t=a_t+n_t$分别表示噪声和离群点组成。于是，RobustPeriod为了能够发现时序数据中潜在的周期分量，并且保证结果robust，即很大程度上不受趋势函数、噪声和离群点的影响，将对时序数据进行多周期检测的过程分为三个部分</p>
<ol>
<li><p>预处理，将时序数据存在的趋势和离群点进行移除</p>
</li>
<li><p>多周期解耦，分离出不同层次的周期性，并进行排序，优先表现更显著的周期分量</p>
</li>
<li><p>单周期检测，根据周期分量生成周期候选并验证，计算周期长度</p>
</li>
</ol>
<h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><ul>
<li><p>使用Hodrick-Prescott算法计算时序数据的大致趋势并移除</p>
<blockquote>
<p>$$<br>\hat{y_t}=y_t-\hat\tau_t, \hat\tau_t=\underset{\tau_t}{argmin}\frac{1}{2}\sum_{t=0}^{N-1}+\lambda\sum_{t=1}^{N-2}(\tau_{t-1}-2\tau_t+\tau_{t+1})^2<br>$$</p>
</blockquote>
<p>Robert J Hodrick and Edward C Prescott. 1997. Postwar US business cycles: an empirical investigation. Journal of Money, Credit, and Banking (1997), 1–16.</p>
</li>
<li><p>移除显著的离群点</p>
<blockquote>
<p>$$<br>y_t^{‘}=\Psi(\frac{\hat{y_t}-\mu}{s}),\Psi(x)=sign(x)min(|x|,c)<br>\hat{y_t}=y_t-\hat\tau_t<br>$$</p>
</blockquote>
<p>Alexander Dürre, Roland Fried, and Tobias Liboschik. 2015. Robust estimation of (partial) autocorrelation. Wiley Interdisciplinary Reviews: Computational Statistics 7, 3 (may 2015), 205–222.</p>
</li>
</ul>
<h1 id="多周期解耦"><a href="#多周期解耦" class="headerlink" title="多周期解耦"></a>多周期解耦</h1><ul>
<li>这部分使用最大重复离散小波转换(Maximal overlap discrete wavelet transform, MODWT)将经过预处理的时间序列分解成不同层次的多个时间序列然后交给下一步进行周期分析</li>
</ul>
<p><strong>Daubechies MODWT</strong></p>
<p>小波变换<a href="https://zhuanlan.zhihu.com/p/22450818">https://zhuanlan.zhihu.com/p/22450818</a> ，将傅里叶变换的基函数改为小波母函数，通过两个参数控制函数位移和伸缩（时间和频率），建立能将信号在时间和频率两种层面上进行体现的时频谱</p>
<p>在连续小波变换的基础上，考虑到现实场景下数据采集是不连续的且有限的，包括计算机也不擅长进行连续问题的计算，提出了离散小波变换discrete wavelet transform<a href="https://zhuanlan.zhihu.com/p/69956002">https://zhuanlan.zhihu.com/p/69956002</a> ，这里介绍了离散小波变换的Mallet算法，通过一次半子带滤波分离出高频信号然后将低频信号进行两倍下采样后重复该过程，最后得到在时间线上频率在$(\frac{F_s}{2},F_s),(\frac{F_s}{4},\frac{F_s}{2}),(\frac{F_s}{8},\frac{F_s}{4}),…$的小波信号，这些小波信号也是对于当前频率范围兼顾时域分辨率和频域分辨率的结果</p>
<p>MODWT作为一种DWT的方案，应该是在滤波和重采样方面采用了不同的方法，带来如下优势</p>
<ul>
<li>能处理任意长度的时间序列</li>
<li>在粗粒度上有更高的分辨率</li>
<li>具有更渐进有效的小波方差估计器</li>
<li>能更有效的处理非稳态时间序列（频率随时间变化）和非高斯噪声（噪声概率密度函数不满足正态分布）</li>
</ul>
<p>文章使用的Daubechies MODWT即是基于<a href="https://zh.wikipedia.org/wiki/%E5%A4%9A%E8%B4%9D%E8%A5%BF%E5%B0%8F%E6%B3%A2">多贝西小波</a>的一种MODWT方案</p>
<p><img src="fig2.png" alt="fig2"></p>
<p>周期分解过程大致如图，对照<a href="https://zhuanlan.zhihu.com/p/69956002">Mallet算法</a>还是有很多相通之处的，通过 j=1 层小波过滤器wavelet filter $h_{1,l}$ 分离高频分量，将高频分量即周期在 [2,4] 的部分分离成 $w_{1,t}$ ；缩放过滤器scaling filter $g_{1,l}$ 分离出低频分量，用于第二层的小波分解<br>$$<br>\begin{align}<br>w_{j,t} = \sum_{l=0}^{L_j-1}h_{j,l}y_{t-l mod N}^{‘}, L_j=(2^j-1)(L_1-1)+1 \qquad (1)\\<br>v_{j,t} = \sum_{l=0}^{L_j-1}gh_{j,l}y_{t-l mod N}^{‘}, L_j=(2^j-1)(L_1-1)+1 \qquad (2)<br>\end{align}<br>$$<br>其中 $L_1$ 是多贝西小波（系数）的长度，上式(1)表示某一时刻的第 j 层小波 t 时刻的值由 $(t-L_j,t]$ 的时间序列与小波过滤器乘积累加的结果，即小波系数wavelet coefficient，从而实现 $1/2^{j+1} \leq |f| \leq 1/2^j$ 的带宽过滤，从而分离在此频率范围的周期分量</p>
<blockquote>
<p>A wavelet coefficient is the scalar product between a function (your observation) and a basis function, the wavelet. In other words, it is the “coordinate” of your function on this wavelet if you do an orthogonal projection.</p>
</blockquote>
<p><strong>Robust Unbiased Wavelet Variance</strong></p>
<p>除了多周期解耦，MODWT的另外一个优势在于其对应的小波方差估计corresponding wavelet variance estimation能够辅助在频带中定位周期分量，其实际上也类似于一个<a href="https://zh.wikipedia.org/wiki/%E8%B0%B1%E5%AF%86%E5%BA%A6">功率谱密度</a>PSD估计器。</p>
<p>根据小波方差，可以得到各个周期分量内包含单个周期的可能排序，从而优先输出最重要的周期分量。这里应该主要通过判断该周期分量是更稳态的则该周期分量更重要，因为当时间序列是稳态的时满足 $\hat{\sigma}_{y^{‘}}^2=\sum_{j=1}^{\infty}\hat{\sigma}_{w_j}^2$ ，所以这里进行排名时更期望更大的 $\hat{\sigma}_{w_j}^2$ 。</p>
<p>这里RobustPeriod进一步使用更鲁棒高效的双权中位方差biweight midvariance来计算小波方差，因为结果近似于the integral of PSD at corresponding nominal octave passband，为了加速计算过程进行了替代。</p>
<h1 id="鲁棒的单周期检测"><a href="#鲁棒的单周期检测" class="headerlink" title="鲁棒的单周期检测"></a>鲁棒的单周期检测</h1><p><strong>Robust Single Periodicity Detection</strong></p>
<p>对周期分量使用鲁棒的基于Huber-Periodogram的Fisher’s Test来生成周期候选</p>
<ul>
<li>通过补零来将小波系数的长度变成原先的两倍以适用于后面Huber-periodogram以及ACF。</li>
<li>Fisher’s test假设具有周期性的时间序列是<ol>
<li>方差为 $\sigma^2$ 的高斯白噪声</li>
<li>$P_k$ 的分布为二度卡方分布 $P_k \sim (1/2)\sigma^2\chi^2(2)$</li>
</ol>
</li>
<li>根据假设计算 g 统计量的分布，通过一个阈值来判断时间序列满足上述假设是否可信</li>
</ul>
<p>$$<br>P(g\geq g_0)=1-\sum_{k=1}^{\lfloor1/g_0\rfloor}\frac{(-1)^{k-1}N!}{k!(N-k)!}(1-kg_0)^{N-1}<br>$$</p>
<ul>
<li>其中 g 统计量被定义为 $g=max_kP_k/(\sum_{t=0}^{N^{‘}-1}||x_te^{-i2\pi kt/N^{‘}}||^2),i=\sqrt{-1}$，$P_k$ 值的计算比较复杂，参考论文，这里结合了 M-periodogram 以及 Huber loss等概念。</li>
<li>证明过程参见论文</li>
</ul>
<p><strong>Robust Huber-Periodogram based ACF for Validating Periodicity Candidates</strong></p>
<p>在这里对上一步从小波系数中获取到周期候选进行ACF峰值检测来验证以保证结果准确性（Fisher’s test的结果会受频谱泄露等的影响），ACF常规方案 $ACF(t)=\frac{1}{(N-t)\delta_w^2}\sum_{n=0}^{N-t-1}w_nw_{n+t},t=0,1,…,N-1$ ，其中$\delta_w$ 是 $w_t$ 的样本方差，但该方案对离群点不够鲁棒且复杂度是 $O(N^2)$。</p>
<p>RobustPeriod使用Huber-periodogram的输出来建立了一个 $O(NlogN)$的鲁棒的ACF方案，具体参见论文<br>$$<br>\begin{align}<br>HuberACF(t)=\frac{p_t}{(N-t)p_0},t=0,1,…,N-1\\<br>p_t=\frac{1}{\sqrt{N^{‘}}}\sum_{k=0}^{N^{‘}-1}\bar{P_k}e^{i2\pi kt/N^{‘}}<br>\end{align}<br>$$<br>对HuberACF结果的峰值进行检测，计算峰值超过阈值的峰之间距离的中位数作为周期长度，只有周期长度处于 $R_k=[\frac{1}{2}(\frac{N}{k+1}+\frac{N}{k})-1,…,\frac{1}{2}(\frac{N}{k}+\frac{N}{k-1})+1]$ 的范围才会被认为时最终的周期长度</p>
<h1 id="算法实践"><a href="#算法实践" class="headerlink" title="算法实践"></a>算法实践</h1><p><code>#TODO</code></p>
]]></content>
      <tags>
        <tag>异常检测</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark日志分析算法实现</title>
    <url>/2021/12/31/Spark%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>在学习了Spark以及一些日志分析算法后，萌生了对一些算法在spark上进行<strong>并行化实现</strong>的想法</p>
<p>实际操作时发现从spark的简单示例到真实将使用for循环实现的算法在spark使用map等操作来进行代替还是需要挺多技巧的，本文尝试从elasticsearch获取日志数据，通过对日志分析算法中的<strong>AECID-PG</strong>这一基于树的日志解释器生成算法进行spark实现来计算kubernetes环境下各个pod对应日志的树解释器</p>
<p>其间参考了一些其他算法的spark实现进行学习，<a href="https://www.cnblogs.com/brooksj/p/spark-Apriori.html">Apriori实现</a>，<a href="https://www.infoq.cn/article/use-spark-to-achieve-naive-bayesian-algorithm">朴素贝叶斯算法</a>以及之前学习GBDT时参考的<a href="https://www.infoq.cn/article/use-spark-to-achieve-naive-bayesian-algorithm">FATE secureboost</a></p>
<a id="more"></a>

<h1 id="Spark数据处理思路"><a href="#Spark数据处理思路" class="headerlink" title="Spark数据处理思路"></a>Spark数据处理思路</h1><p><strong>Apriori 频繁项集挖掘并行化实现思路</strong></p>
<p>常规Apriori算法计算频繁k项集需要从频繁1项集开始逐次递加项数，每次计算k+1项集时，需要根据频繁k项集通过扫描整个数据集来生成候选集并进行候选集的支持度计算，于是支持度最小的候选项便是频繁k+1项集</p>
<p>可以看出常规算法在数据集较大时会在<strong>支持度计算部分</strong>消耗大量时间，因此要进行并行化改造的关键就在于对该部分设计并行化算法</p>
<p><a href="https://www.cnblogs.com/brooksj/p/spark-Apriori.html">https://www.cnblogs.com/brooksj/p/spark-Apriori.html</a> 并行化设计的思路主要是考虑将对于支持度计数的过程使用wordcount来进行统计，通过<strong>flatmap</strong>操作生成候选集，然后通过<strong>map</strong>和<strong>reduceByKey</strong>进行支持度的统计</p>
<p><strong>secureboost并行化实现思路</strong></p>
<p>FATE项目的secureboost基于spark平台实现了联邦的GBDT算法，可以说是在spark上实现复杂算法的典型案例了（当然也是因为GBDT天然对并行化的支持）</p>
<p>secureboost算法实现具体也可以参考我的另外一篇文章 GBDT协作学习实现，主要实现思路便是对所有样本计算梯度，整合成<strong>直方图</strong>的形式进行分裂收益的计算和最佳分裂点的选择</p>
<p><strong>思路整理</strong></p>
<ul>
<li>因为在进行spark的transformation时只能并行化的对RDD内数据进行处理，不能对共享进行写操作，因此也无法支持复杂的逻辑，这是spark进行分布式批量计算所必然的设计，所以在进行设计时一个重要的问题就是，对<strong>算法复杂逻辑</strong>和对<strong>大量数据进行处理</strong>时必须藉由并行化来保证时间开销的平衡</li>
<li>算法在处理大量数据集时，限制主要在于从全部数据中进行相关值的计算，而进行算法的spark实现也是主要将这部分通过spark的transformer和action两种操作替代该部分的计算，而其他设计有限量数据的或者对提取到的类似梯度或统计量的数值进行处理的算法部分则没有必要额外使用spark来实现</li>
<li>所以，一般思路便是，先对全部数据的RDD集通过transformation以及action操作得到一个概括性的数据结构，例如GBDT算法的直方图，基于这个<strong>概括性的数据结构</strong>来进行复杂的算法流程，计算结果再通过transformation分配给RDD的各个元素，至此可能完成了算法的一次循环</li>
<li>另外，还需要考量在spark的工作流中合适的位置进行<strong>数据持久化</strong></li>
</ul>
<h1 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h1><p><a href="https://github.com/jacoeus/SparkLogPG">https://github.com/jacoeus/SparkLogPG</a></p>
<p><strong>AECID-PG</strong>算法介绍参考另一篇博客日志分析算法学习，主要方法时将各行日志按词元token进行划分后根据词元密度，即前缀相同的日志中该词元出现的比例来判断该词元是否可变或者静态的，据此来建立一棵树，每个节点对应一个词元，每条路径对应一种日志模板</p>
<p>于是，将该算法进行spark实现的主要问题在于：<strong>并行化计算词元密度</strong>、将词元数据与<strong>日志树关联</strong>以及对树的每一层要进行一次计算（action）导致的<strong>数据持久化问题</strong></p>
<ul>
<li>在计算词元密度时，需要考虑计算当前token在与其相同前缀的所有日志行中出现的比例（不能统计该token在当前层出现的数量），因此需要获取<strong>相同前缀</strong>的日志行数量和在该前缀下统计各token的数量。因此在进行词元计算时，我采用将<strong>前缀（路径节点id）和当前token</strong>作为RDD的key来进行countByKey的统计，同时在日志树对应的父节点保存该前缀的数量以便下层循环时取用，如此实现并行化次元密度的计算</li>
<li>为了实现词元数据与日志树的关联，我采用将日志树节点通过id进行标识的方法，每次计算得到当前token对应的节点后，将节点id保存在RDD的键值(pre,roken_list)中前缀pre的最后一位来进入下一层的循环</li>
<li>数据持久化方面因为在每一层的循环中都要做一次countByKey的action操作，如果不进行持久化的话在每次循环在countByKey会重复之前循环的transformation操作，但进行持久化的话每循环一次，仅为了一次action操作就需要进行持久化，频繁持久化可能也会影响效率，我这里是简单的采用持久化的方案，对每一层计算得到的RDD就进行持久化并释放上一层的持久化，更优的方案需要之后探索<code>#TODO</code></li>
</ul>
<p><strong>算法流程</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">RDD</span> &lt;- 日志数据</span><br><span class="line"><span class="type">RDD</span>.map(日志清洗和建立token list)</span><br><span class="line"><span class="keyword">while</span>(没有待匹配的日志 || 层数达到预设的最大值)&#123;</span><br><span class="line">    <span class="comment">// 将前缀（路径节点id）和当前token作为RDD的key来进行countByKey的统计，并行计算</span></span><br><span class="line">    next_token_count &lt;- <span class="type">RDD</span>.map((pre,token_list) =&gt; ((pre,curr_token),rest_token_list)).countByKey <span class="comment">// Spark操作</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 计算各token的PF路径频率（密度）</span></span><br><span class="line">    token_PF_map:<span class="type">Map</span>[(pre,curr_token),<span class="type">PF</span>] &lt;- 对next_token_count的value值除以父节点的count得到各token的<span class="type">PF</span></span><br><span class="line">    <span class="comment">//针对相同前缀下的token计算</span></span><br><span class="line">    pre_list_map &lt;- 统计各前缀下的token及其count、<span class="type">PF</span>等指标</span><br><span class="line">    <span class="keyword">for</span>((pre,metrics_list) &lt;- pre_list_map)&#123;</span><br><span class="line">        node_id &lt;- 自增</span><br><span class="line">        node &lt;- 参考<span class="type">AECID</span>-<span class="type">PG</span>算法建立当前层对应的节点，赋值包括(是否可变、token、count、node_id等)，并加入父节点的子节点列表</span><br><span class="line">        id2node += (node_id -&gt; node)</span><br><span class="line">        token_node_map += ((pre,token),node_id)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="type">RDD</span>.filter(过滤没有加入日志树的日志行).map(将新节点加入前缀) <span class="comment">// Spark操作</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>结果示例</strong></p>
<p>在kubernetes取了apisix一个组件pod的日志进行日志树生成，这里取了前10层的日志树进行简单的可视化</p>
<p><img src="1.png" alt="1"></p>
]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>Elasticsearch</tag>
        <tag>算法实现</tag>
        <tag>日志分析</tag>
      </tags>
  </entry>
  <entry>
    <title>TimescaleDB学习</title>
    <url>/2020/09/16/TimescaleDB%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p><a href="https://docs.timescale.com/latest/main">TimescaleDB</a>是基于关系数据库PostgreSQL的一个开源时序数据库</p>
<p>在股票价格预测、物联网传感器数据变动、网络流量异常检测等场景下，时间这个因素在整个数据分析过程中尤为重要，因此在处理大量类似数据的时候，便需要TimescaleDB或其他时序数据库来实现高效的数据存取</p>
<p>最近尝试对网络流量的时序特征进行提取分析，于是开始学习TimescaleDB，并就主要内容进行整理</p>
<a id="more"></a>

<h1 id="TimescaleDB"><a href="#TimescaleDB" class="headerlink" title="TimescaleDB"></a>TimescaleDB</h1><p>TimescaleDB是作为PostgreSQL上的一个扩展来进行实现的，在PostgreSQL的基础上定义了一些抽象概念并实现了一系列相关API，这部分主要对TimescaleDB的一些设计实现进行简单的整理，下一章节会介绍PostgreSQL的SQL语法等内容，然后就TimescaleDB的一些简单应用进行说明</p>
<h2 id="时序数据"><a href="#时序数据" class="headerlink" title="时序数据"></a>时序数据</h2><p>TimescaleDB主要针对时序数据进行设计，下面罗列了时序数据的主要特征</p>
<ul>
<li><strong>Time-centric</strong>: 数据记录始终带有时间戳</li>
<li><strong>Append-only</strong>: 只能以追加的形式插入数据</li>
<li><strong>Recent</strong>: 新数据一般是在最近一个interval时间间隔的数据</li>
</ul>
<h2 id="Data-model"><a href="#Data-model" class="headerlink" title="Data model"></a>Data model</h2><p>根据时序数据的特征，TimescaleDB为用户提供了两种<a href="https://docs.timescale.com/latest/introduction/data-model">数据模型</a>：窄表模型和宽表模型</p>
<p>在大多情况下，我们使用宽表模型来对数据进行存储，也就是一个时间戳配合当前时间点一系列数据（指标）</p>
<ul>
<li><p>Narrow-table Model，有兴趣进一步了解可以参考<a href="https://docs.timescale.com/latest/introduction/data-model">官方文档</a>，下面只进行简单的描述</p>
<ul>
<li>使用一组唯一的标签集合来定义时间序列    </li>
<li>适用于需要对每个度量分别收集的情况</li>
</ul>
</li>
<li><p>Wide-table Model</p>
<ul>
<li>与数据流匹配，一个时间戳配合当前时间点一系列数据（指标）</li>
<li>便于多指标查询</li>
<li>针对多个指标仅写入一个时间戳，因此提取速度更快</li>
</ul>
</li>
<li><p>JOINs with Relational Data</p>
<ul>
<li>类似于外键，通过JOIN命令可以进行多表查询</li>
</ul>
</li>
</ul>
<h2 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h2><p>TimescaleDB在PostgreSQL基础上定义的一些抽象概念</p>
<p>在timescaledb的简单应用上对这些概念的理解要求不高，需要对大量数据进行更高效的存储则需要结合<a href="https://docs.timescale.com/latest/introduction/architecture">官方文档</a>进一步学习理解</p>
<ul>
<li><p>Hypertables超表，为完成用户交互实现的对单个连续表的抽象，以便通过标准SQL进行查询</p>
<blockquote>
<p>Hypertables in TimescaleDB are designed to be easy to manage and to behave predictably to users familiar with standard PostgreSQL tables</p>
</blockquote>
</li>
<li><p>Chunks块，每个超表被拆分为多个块，通过对每个块对应的时间间隔和分区键的区域进行散列</p>
</li>
<li><p>Native Compression原生压缩，对旧的块进行压缩，操作对用户透明</p>
</li>
<li><p>Single Node vs. Clustering单节点和集群，使用分区实现</p>
</li>
<li><p>Time intervals时间间隔</p>
<ul>
<li>v0.11.0后默认时间间隔为7天</li>
<li>可以通过<code>chunk_time_interval</code>在创建超表时设置时间间隔</li>
<li>创建超表后可以通过<code>set_chunk_time_interval</code>来改变新chunks的时间间隔</li>
<li>需要根据数据速率进行设置</li>
</ul>
</li>
<li><p>Space partitions空间分区</p>
<ul>
<li>空间分区的主要目的是使并行I/O具有相同的时间间隔</li>
<li>针对多个并发查询或者多个磁盘并行读取的情况可以使用区间分区</li>
</ul>
</li>
</ul>
<h2 id="安装-amp-配置"><a href="#安装-amp-配置" class="headerlink" title="安装&amp;配置"></a>安装&amp;配置</h2><p><a href="https://docs.timescale.com/latest/getting-started/installation">https://docs.timescale.com/latest/getting-started/installation</a></p>
<p>在ubuntu上可以直接通过apt命令安装，然后需要在postgresSQL上配置扩展</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">psql -U postgres -h localhost</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">database</span> tutorial;</span><br><span class="line">\c tutorial</span><br><span class="line"><span class="keyword">CREATE</span> EXTENSION <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> timescaledb <span class="keyword">CASCADE</span>;</span><br><span class="line"></span><br><span class="line">psql -U postgres -h localhost -d tutorial</span><br></pre></td></tr></table></figure>

<h1 id="PostgreSQL"><a href="#PostgreSQL" class="headerlink" title="PostgreSQL"></a>PostgreSQL</h1><p>参考<a href="https://www.postgresql.org/">PostgreSQL官网</a>、<a href="http://www.postgres.cn/index.php/v2/home">PostgreSQL中文社区</a>和<a href="http://www.postgres.cn/docs/10/">手册</a>，可以通过搜索框比较方便的查找相应SQL命令</p>
<p>该部分主要介绍PostgreSQL，然后结合TimescaleDB定义的一些函数给出了一些查询操作的demo</p>
<h2 id="常见的psql命令"><a href="#常见的psql命令" class="headerlink" title="常见的psql命令"></a>常见的psql命令</h2><p>用于在终端对PostgreSQL进行操作</p>
<table>
<thead>
<tr>
<th align="left">命令</th>
<th align="left">动作</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>\l</code></td>
<td align="left">列出可用的数据库</td>
</tr>
<tr>
<td align="left"><code>\c dbname</code></td>
<td align="left">连接到新数据库</td>
</tr>
<tr>
<td align="left"><code>\dt</code></td>
<td align="left">列出可用表</td>
</tr>
<tr>
<td align="left"><code>\d tablename</code></td>
<td align="left">描述给定表的细节</td>
</tr>
<tr>
<td align="left"><code>\dn</code></td>
<td align="left">列出当前数据库中的所有架构</td>
</tr>
<tr>
<td align="left"><code>\df</code></td>
<td align="left">列出当前数据库中的函数</td>
</tr>
<tr>
<td align="left"><code>\h</code></td>
<td align="left">获取有关SQL命令语法的帮助</td>
</tr>
<tr>
<td align="left"><code>\?</code></td>
<td align="left">列出所有<code>psql</code>斜杠命令</td>
</tr>
<tr>
<td align="left"><code>\set</code></td>
<td align="left">系统变量列表</td>
</tr>
<tr>
<td align="left"><code>\timing</code></td>
<td align="left">显示查询执行所需的时间</td>
</tr>
<tr>
<td align="left"><code>\x</code></td>
<td align="left">显示扩展的查询结果</td>
</tr>
<tr>
<td align="left"><code>\q</code></td>
<td align="left">退出 <code>psql</code></td>
</tr>
</tbody></table>
<h2 id="SQL语句"><a href="#SQL语句" class="headerlink" title="SQL语句"></a>SQL语句</h2><p>对官方给出的demo选取部分有代表性的进行整理</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> create_hypertable(<span class="string">&#x27;conditions&#x27;</span>, <span class="string">&#x27;time&#x27;</span>);</span><br></pre></td></tr></table></figure>

<blockquote>
<p>The ‘time’ column used in the <code>create_hypertable</code> function supports timestamp, date, or integer types</p>
</blockquote>
<p>create_hypertable是timescaledb定义的函数，用于建立超表，以conditions表为基础，time列对应为时间戳</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 基本的建表、插入、查询、修改、删表操作</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> conditions (</span><br><span class="line"> <span class="built_in">time</span>        TIMESTAMPTZ       <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"> location    <span class="built_in">TEXT</span>              <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"> temperature <span class="keyword">DOUBLE</span> <span class="keyword">PRECISION</span>  <span class="literal">NULL</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> create_hypertable(<span class="string">&#x27;conditions&#x27;</span>, <span class="string">&#x27;time&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> conditions(<span class="built_in">time</span>, location, temperature)</span><br><span class="line">  <span class="keyword">VALUES</span> (<span class="keyword">NOW</span>(), <span class="string">&#x27;office&#x27;</span>, <span class="number">70.0</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> conditions <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="built_in">time</span> <span class="keyword">DESC</span> <span class="keyword">LIMIT</span> <span class="number">100</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> conditions</span><br><span class="line">  <span class="keyword">ADD</span> <span class="keyword">COLUMN</span> humidity <span class="keyword">DOUBLE</span> <span class="keyword">PRECISION</span> <span class="literal">NULL</span>;  </span><br><span class="line"></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> conditions;</span><br></pre></td></tr></table></figure>

<h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">TIMESTAMP WITHOUT TIME ZONE <span class="comment">--日期和时间（无时区）</span></span><br><span class="line">TIME <span class="keyword">WITH</span> <span class="built_in">TIME</span> ZONE <span class="comment">--一日内的时间（有时区）</span></span><br><span class="line"><span class="built_in">DATE</span> <span class="comment">--日期</span></span><br><span class="line"><span class="built_in">INTERVAL</span> <span class="comment">--时间间隔</span></span><br></pre></td></tr></table></figure>

<p>PostgreSQL定义的<a href="http://www.postgres.cn/docs/9.4/datatype-datetime.html">日期/时间类型</a></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">NUMERIC(precision, scale) <span class="comment">--可选精度的准确数值数据类型</span></span><br><span class="line">NUMERIC <span class="comment">--创建一个可以存储一个直到实现精度上限的任意精度和标度的数值</span></span><br></pre></td></tr></table></figure>

<p>PostgreSQL定义的<a href="http://www.postgres.cn/docs/9.4/datatype.html">数据类型</a></p>
<h3 id="时间-日期函数和操作符"><a href="#时间-日期函数和操作符" class="headerlink" title="时间/日期函数和操作符"></a>时间/日期函数和操作符</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 基于time将conditions表转换为超表，并基于location建立空间二分区</span></span><br><span class="line"><span class="keyword">SELECT</span> create_hypertable(<span class="string">&#x27;conditions&#x27;</span>, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;location&#x27;</span>, <span class="number">2</span>);</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 按照day对pickup_datetime进行截断并计算每天的记录数量</span></span><br><span class="line"><span class="keyword">SELECT</span> date_trunc(<span class="string">&#x27;day&#x27;</span>, pickup_datetime) <span class="keyword">as</span> <span class="keyword">day</span>, <span class="keyword">COUNT</span>(*) <span class="keyword">FROM</span> rides <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">day</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="keyword">day</span>;</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- EXTRACT函数从日期/时间数值里抽取子域，比如年、小时等</span></span><br><span class="line"><span class="comment">-- trunc截断（向零靠近）</span></span><br><span class="line"><span class="comment">-- GROUP BY将拥有相同值的行分为一组，进一步可以使用HAVING子句筛选分组</span></span><br><span class="line"><span class="comment">-- 不在GROUP BY中列出的字段只能在聚合表达式中被引用</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="keyword">EXTRACT</span>(<span class="keyword">hour</span> <span class="keyword">from</span> pickup_datetime) <span class="keyword">as</span> <span class="keyword">hours</span>,</span><br><span class="line">  trunc(<span class="keyword">EXTRACT</span>(<span class="keyword">minute</span> <span class="keyword">from</span> pickup_datetime) / <span class="number">5</span>)*<span class="number">5</span> <span class="keyword">AS</span> five_mins,</span><br><span class="line">  <span class="keyword">COUNT</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> rides</span><br><span class="line"><span class="keyword">WHERE</span> pickup_datetime &lt; <span class="string">&#x27;2016-01-02 00:00&#x27;</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">hours</span>, five_mins;</span><br></pre></td></tr></table></figure>

<p>PostgreSQL定义的<a href="http://postgres.cn/docs/9.4/functions-datetime.html">时间/日期函数和操作符</a></p>
<h3 id="JOIN"><a href="#JOIN" class="headerlink" title="JOIN"></a>JOIN</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- JOIN实现多表查询</span></span><br><span class="line"><span class="keyword">SELECT</span> rates.description, <span class="keyword">COUNT</span>(vendor_id) <span class="keyword">AS</span> num_trips <span class="keyword">FROM</span> rides</span><br><span class="line">  <span class="keyword">JOIN</span> rates <span class="keyword">ON</span> rides.rate_code = rates.rate_code</span><br><span class="line">  <span class="keyword">WHERE</span> pickup_datetime &lt; <span class="string">&#x27;2016-02-01&#x27;</span></span><br><span class="line">  <span class="keyword">GROUP</span> <span class="keyword">BY</span> rates.description</span><br><span class="line">  <span class="keyword">ORDER</span> <span class="keyword">BY</span> rates.description;</span><br></pre></td></tr></table></figure>

<h3 id="WITH"><a href="#WITH" class="headerlink" title="WITH"></a>WITH</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> rides_length <span class="keyword">CASCADE</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> rides_length(</span><br><span class="line">	three_hour <span class="built_in">TIMESTAMP</span> <span class="keyword">WITHOUT</span> <span class="built_in">TIME</span> ZONE <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">	trip_length <span class="built_in">INTERVAL</span></span><br><span class="line">);</span><br><span class="line"><span class="keyword">SELECT</span> create_hypertable(<span class="string">&#x27;rides_length&#x27;</span>, <span class="string">&#x27;three_hour&#x27;</span>);</span><br><span class="line"><span class="comment">-- WITH提供了一种在更大的查询中编写辅助语句的方式</span></span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">data</span> <span class="keyword">AS</span> (</span><br><span class="line">	<span class="keyword">SELECT</span></span><br><span class="line">	time_bucket(<span class="string">&#x27;3 hour&#x27;</span>, pickup_datetime) <span class="keyword">AS</span> three_hour,</span><br><span class="line">	<span class="keyword">AVG</span>(trip_length) <span class="keyword">AS</span> <span class="keyword">length</span></span><br><span class="line">	<span class="keyword">FROM</span> rides</span><br><span class="line">	<span class="keyword">WHERE</span></span><br><span class="line">	ST_Distance(pickup_geom, ST_Transform(ST_SetSRID(ST_MakePoint(<span class="number">-74.0113</span>,<span class="number">40.7075</span>),<span class="number">4326</span>),<span class="number">2163</span>)) &lt; <span class="number">400</span> <span class="keyword">AND</span></span><br><span class="line">	ST_Distance(dropoff_geom, ST_Transform(ST_SetSRID(ST_MakePoint(<span class="number">-73.9851</span>,<span class="number">40.7589</span>),<span class="number">4326</span>),<span class="number">2163</span>)) &lt; <span class="number">400</span></span><br><span class="line">	<span class="keyword">GROUP</span> <span class="keyword">BY</span> three_hour <span class="keyword">ORDER</span> <span class="keyword">BY</span> three_hour</span><br><span class="line">	),</span><br><span class="line"><span class="keyword">period</span> <span class="keyword">AS</span> (</span><br><span class="line">	<span class="keyword">SELECT</span> time_bucket(<span class="string">&#x27;3 hour&#x27;</span>,  no_gaps) three_hour</span><br><span class="line">	<span class="keyword">FROM</span>  generate_series(<span class="string">&#x27;2016-01-01 00:00:00&#x27;</span>::<span class="built_in">timestamp</span>, <span class="string">&#x27;2016-01-31 23:59:59&#x27;</span>, <span class="string">&#x27;3 hour&#x27;</span>) no_gaps</span><br><span class="line">	)</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> rides_length</span><br><span class="line"><span class="keyword">SELECT</span> period.three_hour,</span><br><span class="line">(<span class="keyword">SELECT</span> data.length <span class="keyword">FROM</span> <span class="keyword">data</span></span><br><span class="line">	<span class="keyword">WHERE</span> data.three_hour &lt;= period.three_hour</span><br><span class="line">	<span class="keyword">ORDER</span> <span class="keyword">BY</span> data.three_hour <span class="keyword">DESC</span> <span class="keyword">LIMIT</span> <span class="number">1</span>) <span class="keyword">AS</span> trip_length</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">period</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> period.three_hour;</span><br></pre></td></tr></table></figure>

<h3 id="TimescaleDB自定义函数"><a href="#TimescaleDB自定义函数" class="headerlink" title="TimescaleDB自定义函数"></a>TimescaleDB自定义函数</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 按5 minute对pickup_datetime进行划分</span></span><br><span class="line"><span class="keyword">SELECT</span> time_bucket(<span class="string">&#x27;5 minute&#x27;</span>, pickup_datetime) <span class="keyword">AS</span> five_min, <span class="keyword">count</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> rides</span><br><span class="line"><span class="keyword">WHERE</span> pickup_datetime &lt; <span class="string">&#x27;2016-01-02 00:00&#x27;</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> five_min</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> five_min;</span><br></pre></td></tr></table></figure>

<p>类似time_bucket，TimescaleDB具有许多自定义的<a href="https://docs.timescale.com/latest/api">SQL函数</a>，可以使时间序列分析变得快速而简单</p>
<h2 id="Scheme-management"><a href="#Scheme-management" class="headerlink" title="Scheme management"></a>Scheme management</h2><p>TimescaleDB支持PostgreSQL支持的所有表对象</p>
<p>设计适当的表对象是使用PostgreSQL的关键部分，为给定的工作负载创建适当的索引和表架构可以显着提高性能</p>
<h3 id="Indexing"><a href="#Indexing" class="headerlink" title="Indexing"></a>Indexing</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">INDEX</span> <span class="keyword">ON</span> conditions (location, <span class="built_in">time</span> <span class="keyword">DESC</span>);</span><br></pre></td></tr></table></figure>

<p>可以在将表转换为超表之前或之后执行此操作</p>
<ul>
<li>索引选择取决于数据</li>
<li>对于离散值的列<code>CREATE INDEX ON conditions (location, time DESC);</code></li>
<li>对于连续值的列<code>CREATE INDEX ON conditions (time DESC, temperature);</code></li>
<li>在原有索引的基础上加上<code>time DESC</code>定义复合索引对部分查询实现优化</li>
<li>默认索引<ul>
<li><code>CREATE INDEX ON conditions (time DESC);</code></li>
<li>指定了可选的空间分区，TimescaleDB将自动创建索引<code>CREATE INDEX ON conditions (location, time DESC);</code></li>
</ul>
</li>
</ul>
<h3 id="Triggers"><a href="#Triggers" class="headerlink" title="Triggers"></a>Triggers</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">OR</span> <span class="keyword">REPLACE</span> <span class="keyword">FUNCTION</span> record_error()</span><br><span class="line">  <span class="keyword">RETURNS</span> <span class="keyword">trigger</span> <span class="keyword">AS</span> $record_error$</span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line"> <span class="keyword">IF</span> NEW.temperature &gt;= <span class="number">1000</span> <span class="keyword">OR</span> NEW.humidity &gt;= <span class="number">1000</span> <span class="keyword">THEN</span></span><br><span class="line">   <span class="keyword">INSERT</span> <span class="keyword">INTO</span> error_conditions</span><br><span class="line">     <span class="keyword">VALUES</span>(NEW.time, NEW.location, NEW.temperature, NEW.humidity);</span><br><span class="line"> <span class="keyword">END</span> <span class="keyword">IF</span>;</span><br><span class="line"> RETURN NEW;</span><br><span class="line"><span class="keyword">END</span>;</span><br><span class="line">$record_error$ LANGUAGE plpgsql;</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TRIGGER</span> record_error</span><br><span class="line">  <span class="keyword">BEFORE</span> <span class="keyword">INSERT</span> <span class="keyword">ON</span> conditions</span><br><span class="line">  <span class="keyword">FOR</span> <span class="keyword">EACH</span> <span class="keyword">ROW</span></span><br><span class="line">  <span class="keyword">EXECUTE</span> <span class="keyword">PROCEDURE</span> record_error();</span><br></pre></td></tr></table></figure>

<p>当超表插入新行时，触发器调用函数</p>
<blockquote>
<p>TimescaleDB支持触发器的全域：<code>BEFORE INSERT</code>，<code>AFTER INSERT</code>，<code>BEFORE UPDATE</code>，<code>AFTER UPDATE</code>，<code>BEFORE DELETE</code>，<code>AFTER DELETE</code></p>
</blockquote>
<h3 id="Constraints"><a href="#Constraints" class="headerlink" title="Constraints"></a>Constraints</h3><p>在超级表上创建，删除或更改约束将传播到块中</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> conditions (</span><br><span class="line">    <span class="built_in">time</span>       TIMESTAMPTZ</span><br><span class="line">    temp       <span class="built_in">FLOAT</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">    device_id  <span class="built_in">INTEGER</span> <span class="keyword">CHECK</span> (device_id &gt; <span class="number">0</span>),</span><br><span class="line">    location   <span class="built_in">INTEGER</span> <span class="keyword">REFERENCES</span> locations (<span class="keyword">id</span>),</span><br><span class="line">    PRIMARY <span class="keyword">KEY</span>(<span class="built_in">time</span>, device_id)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> create_hypertable(<span class="string">&#x27;conditions&#x27;</span>, <span class="string">&#x27;time&#x27;</span>);</span><br></pre></td></tr></table></figure>

<p><a href="https://www.postgresql.org/docs/current/ddl-constraints.html">PostgreSQL Constraints</a></p>
<h3 id="JSON-amp-半结构化数据"><a href="#JSON-amp-半结构化数据" class="headerlink" title="JSON &amp; 半结构化数据"></a>JSON &amp; 半结构化数据</h3><p>TimescaleDB可以使用PostgreSQL中可用的任何数据类型，包括JSON和JSONB</p>
<p>用于存储包含用户定义的字段，即由各个用户定义且随用户而异的字段名称的这类数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> metrics (</span><br><span class="line">  <span class="built_in">time</span> TIMESTAMPTZ,</span><br><span class="line">  user_id <span class="built_in">INT</span>,</span><br><span class="line">  device_id <span class="built_in">INT</span>,</span><br><span class="line">  <span class="keyword">data</span> JSONB</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<ul>
<li>类似time，user_id和device_id之类的通用字段被拉到 JSONB 结构之外并存储为列，在表列上的字段访问比在JSONB结构内部更有效</li>
<li>JSONB数据类型（即以二进制格式存储的JSON）在存储开销和查找性能上都优于JSON数据类型</li>
</ul>
<h2 id="Writing-data"><a href="#Writing-data" class="headerlink" title="Writing data"></a>Writing data</h2><p>可以使用标准<code>INSERT</code>SQL命令（<a href="https://www.postgresql.org/docs/current/static/sql-insert.html">PostgreSQL docs</a>）将数据插入到超表中</p>
<p><code>UPDATE</code>SQL命令（<a href="https://www.postgresql.org/docs/current/static/sql-update.html">PostgreSQL docs</a>）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> conditions(<span class="built_in">time</span>, location, temperature, humidity)</span><br><span class="line">  <span class="keyword">VALUES</span> (<span class="keyword">NOW</span>(), <span class="string">&#x27;office&#x27;</span>, <span class="number">70.0</span>, <span class="number">50.0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">--多行插入</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> conditions</span><br><span class="line">  <span class="keyword">VALUES</span></span><br><span class="line">    (<span class="keyword">NOW</span>(), <span class="string">&#x27;office&#x27;</span>, <span class="number">70.0</span>, <span class="number">50.0</span>),</span><br><span class="line">    (<span class="keyword">NOW</span>(), <span class="string">&#x27;basement&#x27;</span>, <span class="number">66.5</span>, <span class="number">60.0</span>),</span><br><span class="line">    (<span class="keyword">NOW</span>(), <span class="string">&#x27;garage&#x27;</span>, <span class="number">77.0</span>, <span class="number">65.2</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">--UPDATE</span></span><br><span class="line"><span class="keyword">UPDATE</span> conditions <span class="keyword">SET</span> temperature = <span class="number">70.2</span>, humidity = <span class="number">50.0</span></span><br><span class="line">  <span class="keyword">WHERE</span> <span class="built_in">time</span> = <span class="string">&#x27;2017-07-28 11:42:42.846621+00&#x27;</span> <span class="keyword">AND</span> location = <span class="string">&#x27;office&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--更新多行</span></span><br><span class="line"><span class="keyword">UPDATE</span> conditions <span class="keyword">SET</span> temperature = temperature + <span class="number">0.1</span></span><br><span class="line">  <span class="keyword">WHERE</span> <span class="built_in">time</span> &gt;= <span class="string">&#x27;2017-07-28 11:40&#x27;</span> <span class="keyword">AND</span> <span class="built_in">time</span> &lt; <span class="string">&#x27;2017-07-28 11:50&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--UPSERT</span></span><br><span class="line"><span class="comment">#TODO</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--DELETE</span></span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> conditions <span class="keyword">WHERE</span> temperature &lt; <span class="number">35</span> <span class="keyword">OR</span> humidity &lt; <span class="number">60</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> conditions <span class="keyword">WHERE</span> <span class="built_in">time</span> &lt; <span class="keyword">NOW</span>() - <span class="built_in">INTERVAL</span> <span class="string">&#x27;1 month&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>TimescaleDB通过可选<code>ON CONFLICT</code>子句（<a href="https://www.postgresql.org/docs/current/static/sql-insert.html#SQL-ON-CONFLICT">PostgreSQL docs</a>）以与PostgreSQL相同的方式支持UPSERT </p>
<p>运行较大的<code>DELETE</code>操作后，建议用户<code>VACUUM</code>或<code>VACUUM FULL</code>通过超表来回收已删除或废弃的行所占用的存储（<a href="https://www.postgresql.org/docs/current/static/sql-vacuum.html">PostgreSQL docs</a>）</p>
<h2 id="Reading-data"><a href="#Reading-data" class="headerlink" title="Reading data"></a>Reading data</h2><p>可以使用标准<code>SELECT</code>SQL命令（<a href="https://www.postgresql.org/docs/current/static/sql-select.html">PostgreSQL docs</a>）从超表中查询数据，包括使用任意<code>WHERE</code>子句，<code>GROUP BY</code>以及<code>ORDER BY</code>命令，联接，子查询，窗口函数，用户定义函数（UDF），<code>HAVING</code>子句等</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> conditions <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="built_in">time</span> <span class="keyword">DESC</span> <span class="keyword">LIMIT</span> <span class="number">100</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(*) <span class="keyword">FROM</span> conditions</span><br><span class="line">  <span class="keyword">WHERE</span> <span class="built_in">time</span> &gt; <span class="keyword">NOW</span>() - <span class="built_in">INTERVAL</span> <span class="string">&#x27;12 hours&#x27;</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- Information about each 15-min period for each location</span></span><br><span class="line"><span class="comment">-- over the past 3 hours, ordered by time and temperature</span></span><br><span class="line"><span class="keyword">SELECT</span> time_bucket(<span class="string">&#x27;15 minutes&#x27;</span>, <span class="built_in">time</span>) <span class="keyword">AS</span> fifteen_min,</span><br><span class="line">    location, <span class="keyword">COUNT</span>(*),</span><br><span class="line">    <span class="keyword">MAX</span>(temperature) <span class="keyword">AS</span> max_temp,</span><br><span class="line">    <span class="keyword">MAX</span>(humidity) <span class="keyword">AS</span> max_hum</span><br><span class="line">  <span class="keyword">FROM</span> conditions</span><br><span class="line">  <span class="keyword">WHERE</span> <span class="built_in">time</span> &gt; <span class="keyword">NOW</span>() - <span class="built_in">INTERVAL</span> <span class="string">&#x27;3 hours&#x27;</span></span><br><span class="line">  <span class="keyword">GROUP</span> <span class="keyword">BY</span> fifteen_min, location</span><br><span class="line">  <span class="keyword">ORDER</span> <span class="keyword">BY</span> fifteen_min <span class="keyword">DESC</span>, max_temp <span class="keyword">DESC</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- How many distinct locations with air conditioning</span></span><br><span class="line"><span class="comment">-- have reported data in the past day</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> location) <span class="keyword">FROM</span> conditions</span><br><span class="line">  <span class="keyword">JOIN</span> locations</span><br><span class="line">    <span class="keyword">ON</span> conditions.location = locations.location</span><br><span class="line">  <span class="keyword">WHERE</span> locations.air_conditioning = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">AND</span> <span class="built_in">time</span> &gt; <span class="keyword">NOW</span>() - <span class="built_in">INTERVAL</span> <span class="string">&#x27;1 day&#x27;</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Database</category>
        <category>TimescaleDB</category>
      </categories>
      <tags>
        <tag>TimescaleDB</tag>
        <tag>时序数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>一些联邦学习方向的调研</title>
    <url>/2021/01/29/%E4%B8%80%E4%BA%9B%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%90%91%E7%9A%84%E8%B0%83%E7%A0%94/</url>
    <content><![CDATA[<p>试图探索联邦学习方向进行研究的方向，搜索了一些资料对联邦学习领域的一些问题或者研究方向进行了一个简单的调研和整理，主要分为联邦学习的通信加速和隐私保护问题</p>
 <a id="more"></a>

<h1 id="联邦学习通信加速问题"><a href="#联邦学习通信加速问题" class="headerlink" title="联邦学习通信加速问题"></a>联邦学习通信加速问题</h1><p>From: 胡水海-联邦学习海量加密数据高效传输的探索</p>
<h2 id="数据中心内场景"><a href="#数据中心内场景" class="headerlink" title="数据中心内场景"></a>数据中心内场景</h2><p>高速网络时代如何加速联邦学习通信</p>
<h3 id="RDMA技术优化两点间通信"><a href="#RDMA技术优化两点间通信" class="headerlink" title="RDMA技术优化两点间通信"></a>RDMA技术优化两点间通信</h3><ul>
<li>使用TCP协议吞吐量受限、延迟较大</li>
<li>RDMA内核旁路使用硬件加速传输层<ul>
<li>GPU-Direct-RDMA实现GPU与RMA之间的数据零拷贝</li>
</ul>
</li>
</ul>
<h3 id="动态参数聚合模型优化多点间通信"><a href="#动态参数聚合模型优化多点间通信" class="headerlink" title="动态参数聚合模型优化多点间通信"></a>动态参数聚合模型优化多点间通信</h3><h4 id="传统参数聚合模型"><a href="#传统参数聚合模型" class="headerlink" title="传统参数聚合模型"></a>传统参数聚合模型</h4><ul>
<li>Parameter Server<ul>
<li>PS容易出现链路瓶颈</li>
</ul>
</li>
<li>Ring Allreduce<ul>
<li>某一跳阻塞会导致整个聚合任务的停滞</li>
</ul>
</li>
</ul>
<h4 id="动态参数模型"><a href="#动态参数模型" class="headerlink" title="动态参数模型"></a>动态参数模型</h4><blockquote>
<p>根据网络状态（物理拓扑、动态状态等）生成参数汇聚方案</p>
</blockquote>
<ul>
<li>根据网络情况将两种模型结合起来</li>
</ul>
<h2 id="跨区域场景"><a href="#跨区域场景" class="headerlink" title="跨区域场景"></a>跨区域场景</h2><p>高延迟、高丢包率网络环境下如何加速联邦学习通信</p>
<h3 id="机器学习专用的网络传输协议"><a href="#机器学习专用的网络传输协议" class="headerlink" title="机器学习专用的网络传输协议"></a>机器学习专用的网络传输协议</h3><ul>
<li>物理距离增加，跨区域通信时间在联邦学习中的时间占比越来越大</li>
<li>跨区域主干网具有高延迟、高丢包率等特征,丢包侦测与丢包恢复代价很大</li>
<li>机器学习模型训练可以容忍一定的丢包率<ul>
<li>目前模型训练大多采用随机梯度下降（SGD）方式通过多轮迭代进行,丢失一部分数据不影响训练算法找到模型收敛点</li>
</ul>
</li>
<li>机器学习专用的网络传输协议 MLT</li>
</ul>
<h2 id="使用-sketch-加速联邦学习"><a href="#使用-sketch-加速联邦学习" class="headerlink" title="使用 sketch 加速联邦学习"></a>使用 sketch 加速联邦学习</h2><ul>
<li><p>使用 sketch 加速联邦学习的核心思想就是利用 sketch 能够对数据进行压缩，但对数据的某些特性在一定<br>程度上进行保留，例如 Count Sketch 可以在较小的空间内存储大量的模型更新信息，并在一定程度上不影响<br>模型的最终训练效果，通过这种方式减少需要传输的数据量来减少传输的时间开销 </p>
</li>
<li><p>Fetchsgd: Communication-efficient federated learning with sketching 和 Fedsketch: Communication-efficient and private federated learning via sketching 就是两种通过将 sketch 和联邦学习结合来加速联邦学习的实现</p>
</li>
</ul>
<h1 id="联邦学习隐私保护问题"><a href="#联邦学习隐私保护问题" class="headerlink" title="联邦学习隐私保护问题"></a>联邦学习隐私保护问题</h1><blockquote>
<p>目标：实现隐私性的同时保证模型训练的效率和准确率</p>
</blockquote>
<ul>
<li>sketches如何应用与联邦学习场景</li>
<li>当前哪些sketches有利于保证隐私性和准确度</li>
<li>如何利用当前sketches进一步提升隐私性</li>
</ul>
<h2 id="隐私性的定义"><a href="#隐私性的定义" class="headerlink" title="隐私性的定义"></a>隐私性的定义</h2><p>From: Enhancing the Privacy of Federated Learning with Sketching</p>
<ul>
<li>Level-0：无隐私性</li>
<li>Level-1：原始数据保护<ul>
<li>传统联邦学习</li>
<li>模型更新信息（梯度）共享</li>
</ul>
</li>
<li>Level-2：全局隐私<ul>
<li>模型更新信息仅对汇聚节点（参数服务器）可见</li>
</ul>
</li>
<li>Level-3：本地隐私<ul>
<li>模型更新信息对所有其他节点不可见</li>
</ul>
</li>
<li>Level-4：终极目标<ul>
<li>包括用户信息、模型更新信息以及训练得到的模型对第三方都不可见</li>
<li>对于所有参与者，整个系统是一个黑盒，仅持有对预测功能的接口</li>
</ul>
</li>
</ul>
<h2 id="保护数据隐私的多方计算方法"><a href="#保护数据隐私的多方计算方法" class="headerlink" title="保护数据隐私的多方计算方法"></a>保护数据隐私的多方计算方法</h2><h3 id="基于噪音的安全计算方法"><a href="#基于噪音的安全计算方法" class="headerlink" title="基于噪音的安全计算方法"></a>基于噪音的安全计算方法</h3><p>差分隐私，保护的是数据源中一点微小的改动导致的隐私泄露问题</p>
<p>使用基于噪声的差分隐私方法的主要思想是，对计算过程用噪音干扰，让原始数据淹没在噪音中，而无法从得到的结果反推原始数据的细节</p>
<p>比较常用的是加拉普拉斯噪音（Laplace noise）。由于拉普拉斯分布的数学性质正好与差分隐私的定义相契合</p>
<p>该方法通常也要求数据满足一定的分布，这一点在现实中通常很难满足</p>
<h3 id="非噪音的安全计算方法"><a href="#非噪音的安全计算方法" class="headerlink" title="非噪音的安全计算方法"></a>非噪音的安全计算方法</h3><p>混淆电路（Garbled Circuit）、同态加密（Homomorphic Encryption）和密钥分享（Secret Sharing）</p>
<ul>
<li>基于密码学的方法</li>
<li>能够以准确值进行传输，但计算量和通讯开销会比较大</li>
<li>联邦学习保障数据隐私的另一种一种常用方法是采用同态加密技术</li>
</ul>
<h4 id="同态加密"><a href="#同态加密" class="headerlink" title="同态加密"></a>同态加密</h4><ul>
<li>同态加密是一种依赖数学计算复杂性的的加密算法,能让密文能够进行某种代数运算得到的仍是加密的<br>结果而且结果与明文进行运算后再进行加密的结果一致。</li>
<li>同态加密包括部分同态（只针对加法或乘法有同态性质，即加法同态、乘法同态等）、层次同态（只支持有限次运算）以及全同态等,相较于差分隐私的方法，同态加密的计算开销要大得多,目前大多解决方案仍是使用效率较高的部分同态</li>
<li>一般使用的同态加密算法为 PHE（多见为加法同态加密算法)），例如在 FATE 项目中使用的 Paillier 即为加法<br>同态加密算法，在联邦学习中，大多也采用加法的梯度聚合方式，各个数据孤岛在本地计算各自的模型更新<br>信息使用同态加密的算法进行加密后发送给参数服务器,参数服务器通过对密文直接采用基于加法或乘法的<br>聚合方式计算得到全局模型更新信息的密文形式，由于同态加密的特性，保证了此时解密得到的结果是正确<br>的全局模型更新信息，对模型进行更新,至此完成一轮的联邦学习过程</li>
</ul>
<h2 id="差分隐私-Sketch"><a href="#差分隐私-Sketch" class="headerlink" title="差分隐私+Sketch"></a>差分隐私+Sketch</h2><p>$$<br>\forall{S} , Pr[M(D_1)\in S]\leq e^{\epsilon} * Pr[M(D_2)\in S]<br>$$</p>
<p>上式表示对于相似的数据集D1和D2，两者通过随机化机制M处理后得到的结果属于同一个M的输出结果集合S的概率接近于1</p>
<ul>
<li>方案1： 直接在模型更新信息中加入噪声</li>
<li>方案2： Laplace噪声+Count-Min和Count（From: Efficient private statistics with succinct sketches）</li>
<li>方案3： Count-Min or Count（From: Enhancing the privacy of federated learning with sketching）<ul>
<li>模型空间远大于sketch的大小，count sketch可以满足差分隐私的条件</li>
<li>否则需要hash算法的不可逆性来在一定程度上保证数据隐私</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>学习笔记</tag>
        <tag>联邦学习</tag>
        <tag>sketch</tag>
      </tags>
  </entry>
  <entry>
    <title>spark连接elasticsearch进行数据处理</title>
    <url>/2021/11/22/spark%E8%BF%9E%E6%8E%A5elasticsearch%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<p>在数据处理时希望结合Spark和Elasticsearch这两大流行的工具，设想了将大量运维数据存储在Elasticsearch中通过Spark访问处理后再返回Elasticsearch，结合Kibana进行展示，或者使用Spark Streaming在入库Elasticsearch前进行预处理两种场景</p>
<p>本文总结了Spark与Elasticsearch进行交互的几种模式及一些demo</p>
<p><code>#TODO</code></p>
<a id="more"></a>

<p>方案1：elasticsearch部署在kubernetes环境下，本机使用local[*]方案访问用于简单测试</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.elasticsearch<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>elasticsearch-spark-30_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>7.15.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">&quot;Elastic App&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>);</span><br><span class="line">        conf.set(<span class="string">&quot;es.nodes&quot;</span>, <span class="string">&quot;10.108.23.161&quot;</span>);  <span class="comment">// CLUSTER_IP</span></span><br><span class="line">        conf.set(<span class="string">&quot;es.port&quot;</span>, <span class="string">&quot;9200&quot;</span>);</span><br><span class="line">        conf.set(<span class="string">&quot;es.net.http.auth.user&quot;</span>, <span class="string">&quot;elastic&quot;</span>);</span><br><span class="line">        conf.set(<span class="string">&quot;es.net.http.auth.pass&quot;</span>, <span class="string">&quot;nwT9nh83KTZw1RoMjxDo&quot;</span>);</span><br><span class="line">        conf.set(<span class="string">&quot;es.nodes.wan.only&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line"></span><br><span class="line">        JavaSparkContext jsc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        jsc.setLogLevel(<span class="string">&quot;ERROR&quot;</span>);</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, Map&lt;String, Object&gt;&gt; esRDD = JavaEsSpark.esRDD(jsc, <span class="string">&quot;metricbeat-7.8.0-2021.11.22-000001&quot;</span>);  <span class="comment">// index</span></span><br><span class="line">        System.out.println(esRDD.first().toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>#TODO</code></p>
]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title>协作学习与网络安全大赛参赛总结</title>
    <url>/2021/01/16/%E5%8D%8F%E4%BD%9C%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8%E5%A4%A7%E8%B5%9B%E5%8F%82%E8%B5%9B%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>前段时间一直在参加第一届全国大学生协作学习与网络安全大赛，刚答辩结束，于是在此对比赛过程做一个总结，也对决赛其他队伍的优秀思路做个简单的整理</p>
<p>相比于其他团队，我们的论文阅读更偏向于工程实现方面，参考论文也大多是将本地的GBDT算法无损的实现到联邦环境，但似乎联邦学习方面的研究课题更倾向于以深度学习为基础，也是我们调研不够充分，导致我们比赛时的很多思路以及调研受限于传统机器学习，无法很好响应协作学习的主题</p>
 <a id="more"></a>

<h1 id="赛题的主要挑战"><a href="#赛题的主要挑战" class="headerlink" title="赛题的主要挑战"></a>赛题的主要挑战</h1><ul>
<li>联邦环境 单PS+多Worker</li>
<li>缺失值处理</li>
<li>噪声处理</li>
<li>数据倾斜 Non-IID</li>
<li>测试集存在训练集没有的类</li>
<li>隐私保护</li>
</ul>
<h1 id="相关思路整理"><a href="#相关思路整理" class="headerlink" title="相关思路整理"></a>相关思路整理</h1><p>对我们以及决赛其他队伍一些思路做一个简单的整理</p>
<ul>
<li>缺失值处理<ul>
<li>超过19维直接删除</li>
<li>随机森林</li>
</ul>
</li>
<li>特征工程<ul>
<li>特征编码（无用或高相关特征）</li>
<li>LightGBM 互斥绑定</li>
<li>归一化（基于GBDT的特征映射）</li>
</ul>
</li>
<li>联邦学习安全：拜占庭问题（相似梯度聚合）</li>
<li>一些深度学习的方案<ul>
<li><code>#TODO</code></li>
</ul>
</li>
</ul>
<h1 id="比赛经历总结"><a href="#比赛经历总结" class="headerlink" title="比赛经历总结"></a>比赛经历总结</h1><ul>
<li>有必要对多种方法进行尝试比对，在最后答辩或者展示的时候对效果进行评测</li>
<li>当时认为GBDT对噪声不敏感，把缺失值只进行简单的处理，反而引入了噪声，事实上还是需要对缺失值进行合适的处理，10%的缺失值不进行合适的处理可能会完全影响预测的准确性</li>
<li>手动实现secureBoost发现效率极不理想（比赛的时候因为这个头秃了好久），在构建直方图的时候时间复杂度达到O(#instance * #feature * #bin)，而LightGBM同样使用了直方图效率好像会快很多，一方面时使用了GOSS和特征的互斥绑定等优化算法，一方面可能也是数据结构的优化问题，这可以方面后续进行了解，另外，在多进程和多线程方面也可以加以考虑，比赛时因环境限制，两者对运行时间也没有明显的减少</li>
<li>比赛时模拟环境限制，将secureBoost迁移到比赛环境时底层数据结构还是得重新设计，代码结构也做了一些修改，也是花了蛮多时间，后来加上了一些LightGBM的优化方案，整个过程也是对GBDT系列的算法实现有了较深的理解（然而实际部署联邦环境大部分都有现成的库了）</li>
</ul>
<h1 id="比赛总结"><a href="#比赛总结" class="headerlink" title="比赛总结"></a>比赛总结</h1><p>下面是我们队答辩PPT的简单文字稿，后来看了别人队感觉就很粗糙hhh</p>
<h2 id="赛题理解"><a href="#赛题理解" class="headerlink" title="赛题理解"></a>赛题理解</h2><ul>
<li>对DDOS流量进行多分类 =&gt; 决策树算法，适用于多分类情况，可解释性强，有助于后续分析</li>
<li>模拟分布式环境，单PS &amp; 多Worker =&gt; 分布式</li>
<li>数据集划分（每个Worker的数据集包含所有特征，但只有部分标签） =&gt; 横向联邦的secureBoost</li>
</ul>
<h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><p>使用适用于横向联邦的分布式GBDT算法对数据集进行多分类，并结合数据集特征进行优化</p>
<h2 id="算法说明"><a href="#算法说明" class="headerlink" title="算法说明"></a>算法说明</h2><ul>
<li>在具体实现上，参考了FATE secureBoost，XGBoost，LightGBM等工程实现项目，使用直方图算法来收集各个Worker的梯度信息，并据此找到决策树对应各个节点的分裂点</li>
</ul>
<img src="1.png" alt="图片1" style="zoom: 40%;" />

<ul>
<li><p>如上图所示，由Worker计算当前节点的训练数据的一阶和二阶梯度直方图，汇总到Server建立全局的直方图，据此可以计算得到节点的最佳分裂点</p>
</li>
<li><p>通过这种方法来建立决策树，并结合Boosting的集成学习方法实现了secureBoost算法</p>
</li>
<li><p>考虑到在模拟环境下使用全部数据集来进行建模时间消耗过大，不利于后续模型调整，我们权衡之下，基于secureBoost算法实现使用了如下的方案</p>
</li>
</ul>
<img src="2.png" alt="图片2" style="zoom:50%;" />

<ul>
<li>仅使用部分数据集来进行建模，并使用对每个Worker的数据集进行进一步划分重组来集成投票的方式来提升性能</li>
</ul>
<h2 id="性能分析"><a href="#性能分析" class="headerlink" title="性能分析"></a>性能分析</h2><ul>
<li>隐私性：通过结合同态加密技术，可以保障数据的隐私性</li>
<li>通信开销：O(#feature * #bin)</li>
<li>训练开销：O(#instance * #feature * log(#bin) / #worker)</li>
<li>能够进行无损的联邦学习</li>
<li>GBDT算法对噪声不敏感</li>
<li>通过结合集成学习保证了模型的泛化性</li>
</ul>
<h2 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h2><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>主要工作：在官方环境中重写了secureBoost，实现了直方图算法，level-wise和leaf-wise两种树的生长方式以及特征选择等，另外也实现了</li>
<li>实现了以一轮GBDT作为基决策树的随机森林</li>
<li>没有对缺失值进行合适的处理，遗漏了secureBoost本身对缺失值处理的能力</li>
</ul>
<h3 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h3><ul>
<li>考虑使用随机森林算法的列抽样来对缺失值进行处理</li>
<li>实现在分布式环境上部署，探索学习更高效的算法实现</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>联邦学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>联邦学习</tag>
        <tag>协作学习</tag>
        <tag>比赛经历</tag>
      </tags>
  </entry>
  <entry>
    <title>日志分析算法学习</title>
    <url>/2021/12/13/%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>希望对日志分析有个大致的了解，尝试学习了《Smart Log Data Analytics: Techniques for Advanced Security Analysis》一书，并就其中一些算法和思路进行整理，学习其中提到的 AECID 轻量级日志分析方案</p>
<p>书中按复杂程度依次介绍的几种日志处理工具包括：增量聚类、模板生成、时序分析以及基于树的日志解析器等，通过组合这些工具能够实现实时的日志异常检测，并且在此基础之上也能更好的进行日志分析的学习</p>
 <a id="more"></a>

<ul>
<li>日志文件通常由一组以时序列出的单行或多行字符串（日志消息）组成，其中时序通常通过附加的时间戳来体现</li>
<li>日志消息可以是高度结构化的（逗号分隔的值列表）、部分结构化的（键值对）、非结构化的（任意长度的自然文本）或者三者的组合</li>
<li>日志消息有时包括与其生成相关任务的进程id</li>
<li>日志消息有时包含行号、消息级别、严重性指示符以及静态标识符等信息</li>
</ul>
<h1 id="增量聚类"><a href="#增量聚类" class="headerlink" title="增量聚类"></a>增量聚类</h1><p><strong>关于日志聚类</strong></p>
<p>静态聚类：聚类单个日志行（会忽略行之间的顺序和依赖关系）</p>
<p>动态聚类：对日志序列进行模式提取（书中没有过多涉及，但是似乎目前研究的主要方向，DeepLog、LogSed等方法都属于动态聚类）</p>
<p>期望通过聚类解决的异常类型：</p>
<ul>
<li>离群值：单个日志行，不匹配现有模板或者不像已知的表示正常系统行为的类（静态聚类）</li>
<li>频率异常：意外频繁或罕见的日志事件</li>
<li>相关性异常：预期成对或成组发生但未发生的日志事件</li>
<li>时间间隔异常：关联事件相继发生的时间间隔出现偏差</li>
<li>序列异常：模式中发生的日志事件序列偏离顺序</li>
</ul>
<p>其中，书中介绍的两种增量聚类方案（基于字符串度量的增量聚类和基于数值距离度量增量聚类）属于静态聚类，将类似的日志行归于同一个类簇</p>
<p><strong>基于字符串度量的增量聚类</strong></p>
<p>增量聚类能够实现在线的日志聚类，在实际场景中，日志往往是一条一条生成的，需要我们以日志流的形式进行快速处理</p>
<p>日志的增量聚类方案会选择某一条日志作为类簇的中心，并将与该日志相似度在阈值之上的日志行加入该类簇</p>
<p>那么，假设当前日志为一条日志，或者称为日志线，每一条日志便以如下流程进行聚类</p>
<ul>
<li><p>日志清洗（去除非常规符号和一些占位符）</p>
</li>
<li><p>判断是否为当前的聚类中心（正好与作为聚类中心的日志行则直接归入该类簇）</p>
</li>
<li><p>否则需要筛选合适的聚类中心并加入</p>
<ul>
<li><p>长度过滤：聚类中心日志的长度要接近当前日志长度，过滤长度过短过长的聚类中心</p>
</li>
<li><p>短词过滤：聚类中心日志和当前日志长度为k的相同子串的数量需要大于M，过滤明显不相似的聚类中，其中 $M = L - k + 1 - (1 - p)kL$ ，$k$由聚类的相似度参数确定，$L = max(|l|,|c|)$，其中$|l|$表示当前日志的长度，$|c|$表示聚类中心日志长度，每次过滤复杂度 $O(|l|-k)$</p>
</li>
<li><p>计算字符串相似度指标：Levenshtein为例，通过动态规划计算两个字符串增删改完成相互转化的最少步骤，作为字符串之间的距离$d$，$1 - d/max(|a|,|b|)$作为字符串之间的相似度，复杂度$O(|a||b|)$</p>
</li>
</ul>
</li>
<li><p>训练阶段：找到满足条件的类簇，有则选择相似度最高的加入，否则建立新的类簇，自己作为聚类中心</p>
</li>
<li><p>检测阶段（白名单）：如果未能找到足够相似的聚类中心，则认为是异常日志</p>
</li>
</ul>
<p><strong>基于数值距离度量的增量聚类</strong></p>
<p>将日志文本转换成一系列数值来实现基于欧式空间的聚类，简单的转换例子即对每个字符在日志线上出现的次数进行统计，将整个日志出现不同字符的数量作为空间的维数</p>
<p><a href="https://zhuanlan.zhihu.com/p/77151308">PCA主成分分析</a>进行降维，具体PC维度取决于数据和原始维数</p>
<p>与上一种方案不同，这里需要预先取一部分（多行）日志计算转移矩阵来保证PCA降维的效果</p>
<p><strong>时序分析</strong></p>
<p>根据在固定时间窗口内各个类簇的增长情况作为异常判断的一个方向（针对小簇问题，将离群点添加到一个新的类中进行异常检测），具体参见下面的时序异常检测部分</p>
<h1 id="日志模板生成"><a href="#日志模板生成" class="headerlink" title="日志模板生成"></a>日志模板生成</h1><p>目标：生成一个模板，能对相似日志的静态部分进行标记，其他部分使用通配符替换</p>
<p>主要挑战：序列对齐</p>
<p><strong>基于<a href="https://www.zhihu.com/question/39279003">词元</a>的模板生成器</strong></p>
<ul>
<li>词元可能是一个单词，通过对日志中的单词进行分隔，找到对所有日志行都存在且出现顺序一致的单词组合作为模板</li>
<li>当前主流的模板生成方案，更高性能，但依赖当前的日志数据，能够防止高度相似的子字符串对应的词元成为模板的一部分</li>
<li>后面介绍的基于树的日志解释器生成算法采用的就是基于词元的方案</li>
</ul>
<p><strong>基于字符的模板生成器</strong></p>
<ul>
<li>主要挑战：需要计算多行序列比对</li>
<li>主要方案：先对两行生成模板，再逐行增量的调整模板</li>
<li>基于字符的算法：merge、length、equalmerge</li>
<li>融合词元和字符的算法：token_char</li>
</ul>
<p>大致思路：类似之前的Levenshtein算法，递归计算两个字符串增删改完成相互转化的最少步骤，称为LV距离，并记录在递归矩阵中的转化路径，这些转化过程便代表两个字符串不同的位置，而通过递归计算最少步骤则最大程度保证保留两个字符串的相同部分</p>
<p>在拿到递归矩阵和转化路径后通过重新走一遍该路径来确定模板，其中</p>
<ul>
<li>矩阵走对角，对应改操作或者当前位置字符相同，仅当当前字符相同时将该字符加入模板，否则以空格（占位符）表示</li>
<li>矩阵向下或向右走，对应增删操作，当前位置以空格表示（注意如果模板前一位已经是空格，则没必要再加一个，一个空格在模板中匹配任意长度的子串）</li>
</ul>
<p>Merge算法：直接在之后每一行日志完成一次上述过程，并保证新模板没有丢失前一个模板的空格</p>
<p>Length算法：只计算模板块和对应日志子串的LV距离</p>
<p>EqualMerge算法：相较Length算法，只是在出现多个相邻的未标记字符串时会进行聚合再计算距离，一定程度上提升了模板质量</p>
<p>Token_char算法：对日志根据词元进行分隔后对词元间的子串建立字符结构提取模板</p>
<h1 id="时序异常检测"><a href="#时序异常检测" class="headerlink" title="时序异常检测"></a>时序异常检测</h1><p>这边的时序异常检测主要是考虑到日志间的动态关系和集群映射会随时间进行改变，来建立跨集群映射的动态特征，从而实现涉及集群演化和时序分析的动态日志分析，大致流程如下图所示</p>
<ol>
<li>流式读取每一行日志</li>
<li>日志清洗</li>
<li>在每个时间窗口内根据相似度进行聚类</li>
<li>通过将日志行分配到相邻时间窗口的集群，来建立跨时间窗口的连接</li>
<li>当一个时间窗口结束后，对集群进行再处理来避免集群演化过程中出现分裂和合并的情况</li>
<li>计算时间窗口下集群的演化指标</li>
<li>建立时序预测模型并进行一步预测</li>
<li>通过将预测与真实值比较来进行异常检测</li>
</ol>
<p><img src="DC&AD.png" alt="DC&amp;AD"></p>
<p>为了体现集群演化，并且对不正常的演化进行异常检测，这里提出了下面的概念</p>
<ul>
<li><p>Tracking跟踪：在将当前时间窗口的日志分配给邻近时间窗口的集群时，会出现有的日志找不到合适的集群，基于此，建立指标 $overlap(C,C^{‘})=\frac{|(R_{curr} \cap R_{prev}^{‘}) \cup (R_{next} \cap R_{curr}^{‘})|}{|R_{curr}^{‘} \cup R_{prev}^{‘} \cup R_{next} \cup R_{curr}|}$ ，表示邻近时间窗口间的集群的可转化性</p>
<p><img src="CR.png" alt="CR"></p>
</li>
<li><p>Transitions转化：包括只影响单个时间窗口单个集群的内部转化和影响其他时间窗口集群的外部转化，这里通过对集群进行再分割和再合并（直到overlap达到阈值）来解决集群演化过程中出现的分裂和合并过程对计算演化指标等产生的负面影响</p>
</li>
<li><p>演化指标：建立能够描述连续窗口间的相互依赖和演化关系的指标，例如 $s=\frac{|R_{prev}^{‘}|+|R_{curr}|-2 |R_{prev}^{‘} \cap R_{curr}|}{|R_{prev}^{‘}|+|R_{curr}|}$ 来反映集群的稳定性</p>
</li>
</ul>
<p>书中推荐了ARIMA 作为时序预测的算法</p>
<h1 id="AECID"><a href="#AECID" class="headerlink" title="AECID"></a>AECID</h1><p><a href="https://github.com/ait-aecid/logdata-anomaly-miner">AECID:A Light-Weight Log Analysis Approach for Online Anomaly Detection</a></p>
<p><img src="AECID.png" alt="AECID"></p>
<p>AECID的系统架构如上图所示，主要包括两个组件AMiner和AECID CENTRAL，其中AMiner负责进行在线的异常检测，使用基于树的解释器进行日志行的高效解析，而AECID Central负责根据正常的日志训练解析器模型并分发给AMiner实例</p>
<ul>
<li><p>AECID主要使用基于树的日志解析器生成算法，算法使用类似决策树的生成模式，将静态部分作为树的节点，可变部分作为树的分支生成与日志对应的树进行后续异常，这样大大降低解析的复杂度，在O(logn)的复杂度内处理一条日志</p>
</li>
<li><p>生成的树如下图所示，基于此，我们可以简单的将树的路径转化为正则表达式进行数据处理，也可以对日志进行快速解析，提取可变部分（重要信息）来进行进一步的分析（<strong>关联分析</strong>等）</p>
</li>
<li><p>AECID实际使用的日志解析器生成算法时AECID-PG:Tree-Based Log Parser Generator，使用了基于密度的解析器生成算法，使用词元频率（而不是距离度量）来确定当前部分时静态的还是可变的，对可变部分进行树的分叉</p>
</li>
</ul>
<p><img src="LogTree.png" alt="LogTree"></p>
<p>于是，有赖于使用的基于树的解析器生成算法，AECID支持如下功能</p>
<ul>
<li>解析器模型通过学习正常系统行为的知识，能够检测当前日志事件和正常系统行为的偏差从而进行异常检测</li>
<li>基于签名的异常检测，提供基于统计数据（频率）的异常检测</li>
<li>支持基于统计特征的时序检测</li>
<li>支持规则生成器</li>
<li>支持关联引擎，能够检测分布式网络节点事件的复杂异常</li>
</ul>
<h2 id="AECID-PG"><a href="#AECID-PG" class="headerlink" title="AECID-PG"></a>AECID-PG</h2><p>基于密度的解析器生成算法大致流程</p>
<ul>
<li><p>根据预定义的分隔符将日志分隔成一系列词元</p>
</li>
<li><p>将多条日志作为行，每条日志的一系列词元作为列建立Table，基于这个Table建立解析树</p>
</li>
<li><p>根据下面的四条规则来判断当前节点树的生长方式（生成的节点类型以及是否建立多个分支）</p>
<ul>
<li><p>使用 $PF_{ij}^k=\frac{|n_j^{k+1}|}{|n_i^k|}$ 来表示路径频率 path-frequency，反映了流入 $n_i^k$ 的日志中又流入 $n_j^{k+1}$ 的比例，其中 $n$ 的上下标分别对应Table的列号和行号</p>
</li>
<li><p>规则1：$\lbrace n_j^{k+1}:\exists e_{ij}^k \wedge PF_{ij}^k \geq \theta _1 \rbrace=\varnothing \Rightarrow VAR$ 各条边的路径频率都低于阈值，即下一个节点的值很分散，则认为树的下一个节点是个变量</p>
</li>
<li><p>规则2：如果 $| n_j^{k+1}:\exists e_{ij}^k \wedge PF_{ij}^k \geq \theta _1 |=1$ 存在一条边的路径频率较大，</p>
<p>对于该边，如果满足 $PF_{ij}^k \geq \theta _2$ 则对其建立静态节点，否则建立可变节点</p>
</li>
<li><p>规则3： 如果 $|n_j^{k+1}:\exists e_{ij}^k \wedge PF_{ij}^k \geq \theta _1| &gt; 1$ 存在多条边路径频率较大，</p>
<p>如果这些边 $\sum_{j \in J}PF_{ij}^k \geq \theta _3$ 合计路径频率大于阈值则为每条边建立静态节点，否则建立可变节点</p>
</li>
<li><p>规则4：在一些日志已经结束的情况下，如果大于 $\theta _4$ 比例的日志结束了，则将之后的节点设为可选，如果没有小于 $\theta _5$ 比例的日志结束</p>
</li>
</ul>
</li>
<li><p>阻尼机制</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>AIOps</tag>
        <tag>日志分析</tag>
        <tag>算法学习</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里云ECS开发者大赛算法赛道参赛总结</title>
    <url>/2021/11/25/%E9%98%BF%E9%87%8C%E4%BA%91ECS%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B%E7%AE%97%E6%B3%95%E8%B5%9B%E9%81%93%E5%8F%82%E8%B5%9B%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p><a href="https://tianchi.aliyun.com/competition/entrance/531906/introduction">云上开发，高效智能–阿里云ECS Cloudbuild开发者大赛算法挑战赛道</a>是在天池平台进行的以阿里云弹性计算服务为背景的故障预测算法设计比赛，主要是对多个数据文件进行处理后训练模型在线预测样本是否会在2天内宕机</p>
<p>这次比赛也是我第一次参加天池相关比赛，事实上在算法方面也缺乏积累，过程中进行了挺多的尝试和学习，最终虽然有幸以复赛第3进入决赛，但是因为算法方面相较其他组做的就很普通，决赛就答了个第六\捂脸</p>
<p>在比赛记录部分整理了比赛过程中进行的主要尝试，希望能对其他同学有所帮助</p>
<p><a href="https://tianchi.aliyun.com/forum/postDetail?spm=5176.12586969.1002.9.38cb5bd9ZBI54Q&amp;postId=294919">https://tianchi.aliyun.com/forum/postDetail?spm=5176.12586969.1002.9.38cb5bd9ZBI54Q&amp;postId=294919</a></p>
 <a id="more"></a>

<h1 id="赛题分析"><a href="#赛题分析" class="headerlink" title="赛题分析"></a>赛题分析</h1><p>赛题背景：以阿里云弹性计算服务（ECS）为背景，要求设计故障预测算法对内存系统异常引起的服务器宕机进行预测来避免服务器异常导致的业务受损，保证上层客户应用的可用性与稳定性</p>
<p>数据：服务器配置信息、服务器异常日志数据（57种异常）、服务器的宕机记录以及标签样本信息</p>
<p>要求：基于上述数据，预测服务器在未来2天内是否会发生宕机</p>
<p>评价指标：F1_score作为评价指标</p>
<h2 id="问题难点"><a href="#问题难点" class="headerlink" title="问题难点"></a>问题难点</h2><ul>
<li><p>整合服务器配置信息、服务器异常日志数据、服务器的宕机记录以及标签样本信息提取有效特征</p>
</li>
<li><p>需要对57种异常提取特征</p>
</li>
<li><p>服务器异常日志较大，需要兼顾数据处理的时间开销</p>
</li>
<li><p>标签样本极不平衡</p>
</li>
</ul>
<h2 id="数据集分析"><a href="#数据集分析" class="headerlink" title="数据集分析"></a>数据集分析</h2><ul>
<li><p>异常停止发生到宕机普遍存在一段空白期，宕机时间中54/206的空白期大于两天，89/206空白期小于一小时 =&gt; 空白期会影响回归预测宕机时间的效果</p>
</li>
<li><p>发现存在33/206的宕机事件异常只集中在宕机前5~6天，60/206的宕机时间中异常集中在宕机前1小时内=&gt;认为存在多种宕机表现，可以进行多分类（判断黑样本有限，多分类很难达到预期效果）</p>
</li>
<li><p>单个异常的出现不存在明显规律，在不同场景下，可能周期性出现或者随机出现=&gt;很难对某个异常针对其特点（周期性、周期持续时间等）设计特征</p>
</li>
<li><p>发现存在仅异常12导致宕机的情况=&gt;认为异常12对宕机存在显著的关联性</p>
</li>
</ul>
<h2 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h2><ul>
<li>基于给出的标签样本对异常统一提取特征并进行不平衡的二分类，并对存在异常12的样本额外训练模型</li>
</ul>
<h1 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h1><ul>
<li><p>分析宕机发生的原因，同时发生多种异常、异常发生的数量/频率达到一定程度、异常持续发生</p>
</li>
<li><p>服务器配置特征+不同粒度窗口下异常出现的均值 + 异常出现的峰值（对所有异常整体和单个异常分别考查）</p>
</li>
<li><p>标记类别特征，其他特征进行归一化（直接除以最大值）</p>
</li>
<li><p>特征选择尝试lightgbm feature_importance和欠采样+树模型，选择前者</p>
</li>
</ul>
<h1 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h1><ul>
<li><p>尝试了lightgbm和imbalanced-learn（选择lightgbm）</p>
</li>
<li><p>对出现异常12的样本进行建模，异常12模型在本地测试时假阴性低，使用减权的方式排除被预测为False的样本，结合lightgbm五折交叉验证训练来保证稳定性，对所有样本进行训练，预测时选出超过一定阈值的样本标记为黑样本</p>
</li>
<li><p>本地模型验证：</p>
<ul>
<li><p>train_test_split来分割训练集测试集容易过拟合（使用某个服务器样本来预测该服务器的样本成绩会虚高）</p>
</li>
<li><p>随机选择部分服务器的所有样本作为测试集（同时保证测试集中黑白样本比例），来保证模型的泛化性</p>
</li>
</ul>
</li>
</ul>
<h1 id="总结-amp-改进"><a href="#总结-amp-改进" class="headerlink" title="总结&amp;改进"></a>总结&amp;改进</h1><ul>
<li><p>复赛成绩0.45，排名第3  </p>
</li>
<li><p>根据数据集分析构建模型，包括主二分类模型和针对异常12的模型，且异常12的模型特征复用了主模型的特征</p>
</li>
<li><p>多次本地测试和线上运行成绩稳定，具有良好的泛化性</p>
</li>
<li><p>使用PostgreSQL对异常日志数据进行存储，可以使用扩展TimescaleDB来高效提取复杂的时序特征</p>
</li>
<li><p>在实际环境中，考虑根据宕机信息设计训练集，对不同宕机情况预先进行多分类（尤其针对类似异常集中在宕机时间前5天附近的情况）</p>
</li>
<li><p>尝试过程中发现在训练集利用样本时间后的部分异常信息来进行训练能达到较好的效果（样本时间前5min之后发生异常作为训练集特征，样本时间前5min内发生异常作为测试集特征） ，可解释性有待探究</p>
</li>
</ul>
<h1 id="比赛记录"><a href="#比赛记录" class="headerlink" title="比赛记录"></a>比赛记录</h1><p>因为缺少参加机器学习相关比赛经验，过程中乱七八糟也做了挺多尝试，这边进行了一些记录</p>
<ul>
<li><p>赛题要求：比赛需要我们基于服务器的异常日志数据，来预测服务器在未来是否会发生宕机。提供了包括服务器配置信息、服务器异常日志数据、服务器的宕机记录以及一些预先打好标签的样本</p>
</li>
<li><p>基本思路：在找到异常日志数据中每个样本的服务器在采样时间点之前一段时间的异常日志信息，基于这些异常日志信息建立每个样本的特征，例如短时间内是否发生某个异常、某个异常发生的起止时间或者某个异常的出现次数等</p>
</li>
<li><p>主要难点：服务器异常日志数据量过大，16.7G大小的csv</p>
</li>
<li><p>尝试1：使用chunk分块进行读取处理，遍历一遍的时间开销便已经无法承受了</p>
</li>
<li><p>尝试2：考虑到服务器异常日志数据是有序排列的，于是希望能利用这个特点来快速定位到同一服务器的所有异常信息，为此标记了所有服务器异常信息在csv中的始末位置，但pandas似乎没有提供直接读取csv指定行数的方法，读取靠后的异常信息仍需要遍历几乎整个csv</p>
</li>
<li><p>尝试3：将原先的大csv按服务器不同分成小csv，从而能直接读取需要服务器的异常信息，然而很尴尬，始一开始运行分解csv的程序不久，笔记本存储空间就不够用了</p>
</li>
<li><p>尝试4：考虑到目标是进行快速的搜索，而需要处理的文件无法存放在内存，于是开始尝试使用数据库来。选择了PostgreSQL数据库来存储异常日志信息，因为只是进行搜索，为了提取特征更灵活就把服务器ip、异常事件、异常名称和自增id一起作为主键了，建表并导入数据</p>
</li>
<li><p>尝试5：利用数据库，于是可以通过遍历所有样本，将每个样本的服务器ip和采样时间加入到sql语句中在数据库查询并返回异常信息到一个Dataframe进行处理，来提取特征。但这样的时间开销还是很大，怀疑是有的服务器异常信息较多，从数据库传输异常信息到Dataframe进行处理仍然有很大的时间开销</p>
</li>
<li><p>尝试6：于是尝试在数据库中就完成特征提取，进行一些尝试并分别考察他们的时间开销，包括COUNT和GROUP BY的方法来获取每种异常的出现次数、EXISTS获取某种异常在短时间内是否发生、LIMIT 1和LIMIT 1 ORDER BY desc方法或者MIN、MAX结合GROUP BY方法来取到采样时间两天内每种异常出现的起止时间等，但直到最后的时间开销依旧不理想，猜测是某些服务器异常信息太多，采用WHERE语句限制时间和异常名称后将其异常信息加载到内存并进行操作的时间开销还是很大</p>
</li>
<li><p>尝试7：感觉使用数据库的时间开销始终较大，开始重新考虑数据处理方式。于是考虑在仅依赖于Pandas的情况下，如何对异常信息只是读取一次来进行特征提取。结合对异常信息的理解，通过分块读取异常信息进行简化，仅保留每个服务器每种异常的起止时间。将异常信息、服务器信息和样本信息按服务器名称连接起来，于是可以利用服务器的异常起止时间和样本采样时间构建特征。相较数据库的方案，可能无法获得一定时间内异常发生的次数，但仍可以粗略的得到异常持续的时间和采样前一段时间内是否发生异常等特征，且时间开销被极大压缩，能够在一分钟左右完成初赛测试集的预测。后来意识到这种方案涉及到穿越数据，但经过尝试在复赛测试集不存在穿越数据的情况下仅利用采样前5分钟之后是否发生异常这一特征和服务器信息就达到了0.4+的成绩，认为通过利用训练集样本时间之后的一些信息能够丰富数据，遂将该方案作为保留方案</p>
</li>
<li><p>尝试9：认为判断服务器是否会宕机是比两天内是否发生宕机更好的分类方案，可以通过分类算法先找出会发生宕机的服务器，再对其发生宕机的时间使用回归算法进行预测，取发生宕机时间小于两天的作为结果。但简单的进行尝试发现效果并不理想</p>
</li>
<li><p>尝试10：之前一直忽略了对给出数据进行分析的步骤，为了加深对数据的理解，观察了宕机服务器在宕机之前各服务器发生的异常、异常发生的起止时间及异常发生的时间间隔等。发现1：存在部分宕机发生前一些异常会间隔稳定的时间进行报错，而大部分异常报错时间比较随机，怀疑一些异常周期性的出现持续时间较长会导致异常。发现2：异常不再发生到宕机会间隔不同时间，大致分为，宕机5天前集中出现异常然后无异常至宕机发生（这种情况下如果卡在宕机前两天便很难进行预测）；宕机前40分钟左右开始，前5分钟左右异常不再出现；若干天前开始，6小时左右不再发生；一天前开始，最近1小时左右结束这几种情况。另外，存在部分宕机发生前一些异常会间隔稳定的时间进行报错，而大部分异常报错时间比较随机，怀疑一些异常周期性的出现持续时间较长会导致异常。发现3：存在部分单独异常12导致宕机的情况</p>
</li>
<li><p>尝试11：分析宕机发生的原因，同时发生多种异常or异常发生的数量/频率达到一定程度or单个异常持续时间较长，据此建立特征。结合方向1可以根据异常是否有周期性选择持续时间或频率作为对于该异常的特征。直觉来讲，随机报错的异常发生频率变高更容易导致宕机，间隔报错的异常持续一段时间可能导致宕机，多种异常同时出现容易导致宕机（待验证）</p>
</li>
<li><p>尝试12：简单的尝试取5min，1h，1day三个时间窗口中各种异常出现的次数作为特征，直接利用这些特征存在严重的过拟合现象。通过对不平衡数据集进行特征选择、调参和使用KFold对5折得到五个模型预测结果取均值等操作尝试提升泛化性</p>
</li>
<li><p>尝试13：重新考察数据，根据训练样本观察样本时间前异常发生的情况，部分样本存在例如样本时间前5min内异常较少而在更早的某个5min中大量出现，于是考虑在使用某个大的时间窗口中固定时间间隔异常出现数量的峰值作为特征，考虑到特征提取的效率，使用了前1h中每分钟出现异常的峰值作为特征，结合尝试12</p>
</li>
<li><p>尝试14：在尝试6的基础上优化查询方案，将每次查询结果具体到对应特征，每收集1000条样本的查询结果进行整合后写入用于最终训练的csv</p>
</li>
<li><p>尝试15：为了保证模型泛化性，进行特征工程方面的学习优化，根据经验找出类别特征，并对其他特征进行归一化。考虑到大部分特征选择方案不适用于不平衡场景，使用多次欠拟合结合树模型的方法进行特征选择，不过效果似乎没有直接使用lightgbm的feature_importance好</p>
</li>
<li><p>模型训练方面：官方给了已经打上标签的样本信息和服务器宕机信息供模型训练</p>
</li>
<li><p>基本方案：对训练集的样本信息加入提取的特征进行训练，考虑到数据集有不平衡的特性，尝试了lightgbm的is_unbalance参数和imbalanced-ensemble的一些算法</p>
</li>
<li><p>改进思路1：预测目标是是否会在两天内发生宕机，可以考虑回归算法预测宕机时间</p>
</li>
<li><p>改进思路2：充分利用服务器宕机信息，关注不可预测的服务器宕机和会发生宕机的服务器</p>
</li>
<li><p>改进思路3：认为判断服务器是否会宕机是比两天内是否发生宕机更好的分类方案，可以通过分类算法先找出会发生宕机的服务器，再对其发生宕机的时间使用回归算法进行预测，取发生宕机时间小于两天的作为结果</p>
</li>
<li><p>实现方案1：考虑是否会宕机可能是一个更有效的分类点，并且能降低数据集的不平衡性。于是，先筛除不可预测的样本，按照宕机信息将样本分为会发生宕机和不会发生宕机的情况，进行二分类的训练，在另一边将样本按照将要发生宕机的时间进行回归训练。将预测样本投入两个模型，满足会宕机且回归预测结果小于两天的预测样本作为结果输出。在判断结果时也可以按照白样本可能的数量适当调整作为二分类预测结果判断的阈值</p>
</li>
<li><p>实现方案2：结合数据处理时的发现，根据提供的服务器宕机信息将宕机按是否会宕机、异常最后出现到宕机的空白期长短进行多分类，再结合每个分类回归预测其</p>
</li>
<li><p>实现方案3：基于给出的标签样本对异常统一提取特征并进行不平衡的二分类，并对存在异常12的样本额外训练模型</p>
</li>
<li><p>方案1在前期的测试中表现不好（可能当时也还没能构建合理的测试方案），方案2黑样本不足，进一步进行多分类认为效果不好，于是最终使用了方案3</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>AIOps</tag>
        <tag>机器学习</tag>
        <tag>比赛经历</tag>
        <tag>天池大赛</tag>
      </tags>
  </entry>
  <entry>
    <title>GBDT协作学习实现</title>
    <url>/2020/11/30/GBDT%E5%8D%8F%E4%BD%9C%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>GBDT梯度提升树因其本身是基于梯度来进行决策树构建的，使得他能够无损的支持分布式决策树构建</p>
<p>通过GBDT结合同态加密技术可以应用于横向联邦学习的情况（数据集的特征保证统一）</p>
<p>GBDT的主要工程实现包括XGBoost以及LightGBM，两者在几大主流大数据平台有分布式的实现</p>
<a id="more"></a>

<h1 id="GBDT-amp-XGBoost-amp-LightGBM"><a href="#GBDT-amp-XGBoost-amp-LightGBM" class="headerlink" title="GBDT &amp; XGBoost &amp; LightGBM"></a>GBDT &amp; XGBoost &amp; LightGBM</h1><p>GBDT，梯度提升树，使用集成学习的boosting方法，根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中。<a href="https://arxiv.org/pdf/1603.02754.pdf">XGBoost</a> 和 <a href="https://papers.nips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf">LightGBM</a>是GBDT的两个工程实现</p>
<h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><p>不同于ID3系列决策树算法使用的信息熵以及信息增益，GBDT引入了<strong>损失函数</strong>以及<strong>目标函数</strong>等概念来实现树的构建</p>
<h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><ol>
<li><p>GBDT 一般是由 $k$ 个基模型（基决策树）组成的一个加法运算式 $\hat{y_i} = \sum_{t=1}^kf_t(x_i)$ ，其中 $f_k$ 为第 $k$ 个基模型， $\hat{y_i}$ 为第 $i$ 个样本的预测值</p>
<p><img src="f1_xgboost.png" alt="f1_xgboost"></p>
<blockquote>
<p>Tree Ensemble Model. The final prediction for a given example is the sum of predictions from each tree.</p>
</blockquote>
</li>
<li><p><strong>损失函数</strong>可由预测值 $\hat{y_i}$ 与真实值  $y_i$ 进行表示，$L = \sum_{i=1}^nl(y_i,\hat{y_i})$ ，其中 n 为样本数量，最简单的，我们可以把 $| \hat{y_i} - y_i | $ 理解成损失函数</p>
</li>
<li><p>由于模型的预测精度由模型的<strong>偏差</strong>和<strong>方差</strong>共同决定，损失函数代表了模型的偏差，想要方差小则需要简单的模型，所以目标函数由模型的损失函数 $L$ 与抑制模型复杂度的正则项 $\Omega$ 组成，于是在损失函数的基础上基于模型复杂度的考量建立<strong>目标函数</strong>：<br>$$<br>Obj = \sum_{i=1}^nl(\hat{y_i},y_i) + \sum_{t=1}^k\Omega(f_t)<br>$$</p>
</li>
<li><p>boosting 模型是前向加法，以第 $t$ 步的模型为例，模型对第 $i$ 个样本 $x_i$ 的预测为 $\hat{y_i}^t = \hat{y_i}^{t-1} + f_t(x_i)$</p>
<p>其中 $\hat{y_i}^{t-1}$ 由第  $t - 1$ 步的模型给出的预测值，是已知常数， $f_t(x_i)$ 是我们这次需要加入的新模型的预测值，此时，目标函数就可以写成：<br>$$<br>Obj^{(t)} = \sum_{i=1}^nl(y_i,\hat{y_i}^t) + \sum_{t=1}^t\Omega(f_i) = \sum_{i=1}^nl(y_i,\hat{y_i}^{t-1}+f_t(x_i))+\sum_{i=1}^t\Omega(f_t)<br>$$<br>求此时最优化目标函数，就相当于求解 $f_t(x_i)$ 。以 $| \hat{y_i} - y_i | $ 作为损失函数为例的话，我们当然希望其值越接近于零越好，于是我们需要求目标函数取极小值时 $f_t(x_i)$ 的解</p>
</li>
<li><p>根据泰勒公式我们把函数 $f(x+\Delta x)$ 在点 $x$ 处进行泰勒的二阶展开，可得到等式：<br>$$<br>f(x+\Delta x) \approx f(x) + f^{‘}(x) + \frac{1}{2}f^{‘’}(x)\Delta x^2f(x+\Delta x) \approx f(x) + f^{‘}(x) + \frac{1}{2}f^{‘’}(x)\Delta x^2<br>$$<br>将把 $\hat{y_i}^{t-1}$ 视为 $x$ ， $f_t(x_i)$ 视为 $\Delta x$ ，故可以将目标函数写为 ：<br>$$<br>Obj^{(t)} = \sum_{t=1}^n l(y_i,\hat{y_i}^{t-1} + f_t(x)) + \sum_{t=1}^t\Omega(f_i) = \sum_{t=1}^n\left[l(y_i,\hat{y_i}^{t-1}) + g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)\right] + \sum_{t=1}^t\Omega(f_i)<br>$$<br>其中 $g_i$ 为损失函数对 $\hat{y_i}^{t-1}$ 的一阶导， $h_i$ 为损失函数对 $\hat{y_i}^{t-1}$ 的二阶导</p>
</li>
<li><p>由于在第 $t$ 步时 $\hat{y_i}^{t-1}$ 其实是一个已知的值，所以 $l(y_i,\hat{y_i}^{t-1})$ 是一个常数，其对函数的优化不会产生影响，因此目标函数可以写成：<br>$$<br>Obj^{(t)} \approx \sum_{t=1}^n\left[g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)\right] + \sum_{t=1}^t\Omega(f_i)<br>$$<br>于是，我们只需根据前一步的 $\hat{y_i}^{t-1}$ 求出每一步损失函数的一阶导和二阶导的值，然后最优化目标函数，就可以得到一步的  $f(x)$，最后根据加法模型得到一个整体模型</p>
</li>
</ol>
<h3 id="基于决策树的目标函数（XGBoost）"><a href="#基于决策树的目标函数（XGBoost）" class="headerlink" title="基于决策树的目标函数（XGBoost）"></a>基于决策树的目标函数（XGBoost）</h3><ol>
<li><p>将决策树定义为 $f_t(x) = w_{q(x)}$ ， $x$ 为某一样本，这里的 $q(x)$ 代表了该样本所处叶子结点上，而 $w_q$ 则代表了叶子结点取值  ，所以 $w_{q(x)}$ 就代表了某个样本$x$（属于 $q(x)$ 叶子节点）的取值  $w$（即预测值）</p>
</li>
<li><p>决策树的复杂度可由叶子数 $T$ 体现，叶子节点越少模型越简单，此外叶子节点也不应该含有过高的权重 $w$ ，因此目标函数的正则项被定义为：<br>$$<br>\Omega(f_t) = \gamma T + \frac{1}{2}\lambda\sum_{j=1}^Tw_j^2<br>$$<br>即决策树模型的复杂度由生成决策树的叶子节点数量 $T$ 和所有节点权重所组成的向量的 $L_2$ 范式共同决定</p>
</li>
<li><p>设 $I_j = {i|q(x_i) = j}$ 为第 $j$ 个叶子节点的样本集合，故我们的目标函数可以写成：<br>$$<br>\begin{aligned}<br>Obj^{(t)} &amp;\approx \sum_{i=1}^n\left[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)\right] + \Omega(f_t)<br>\\&amp;= \sum_{i=1}^n\left[g_iw_{q(x_i)}+\frac{1}{2}h_iw_{q(x_i)}^2\right] + \gamma T + \frac{1}{2}\lambda\sum_{j=1}^Tw_j^2<br>\\&amp;= \sum_{j=1}^T\left[(\sum_{i\in I_j}g_i)w_j + \frac{1}{2}(\sum_{i\in I_j}h_i + \lambda)w_j^2\right] + \gamma T<br>\end{aligned}<br>$$<br>其中，第二步是遍历所有的样本后求每个样本的损失函数，但样本最终会落在叶子节点上，所以我们也可以遍历叶子节点，然后获取叶子节点上的样本集合，最后在求损失函数。即我们之前样本的集合，现在都改写成叶子结点的集合，由于一个叶子结点有多个样本存在，因此才有了 $\sum_{i\in I_j}g_i$ 和 $\sum_{i\in I_j}h_i$ 这两项，  $w_j$ 为第 $j$ 个叶子节点取值</p>
</li>
<li><p>为简化表达式，定义 $G_j = \sum_{i\in I_j}g_i$ ， $H_j = \sum_{i\in I_j}h_i$ ，则目标函数为：<br>$$<br>Obj^{(t)} = \sum_{j=1}^T\left[G_jw_j + \frac{1}{2}(H_j + \lambda)w_j^2\right] + \gamma T<br>$$<br>这里我们要注意 $G_j$ 和 $H_j$ 是前 $t-1$ 步得到的结果，其值已知可视为常数，只有最后一棵树的叶子节点 $w_j$ 不确定，那么将目标函数对 $w_j$ 求一阶导，并令其等于 $0$ ，则可以求得目标函数取极值时叶子结点 $j$ 对应的权值：<br>$$<br>w_j^* = -\frac{G_j}{H_j + \lambda}<br>$$<br>于是目标函数可以化简为：<br>$$<br>Obj = -\frac{1}{2}\sum_{j=1}^T\frac{G_j^2}{H_j+\lambda} + \gamma T<br>$$<br><img src="f2_xgboost.png" alt="f2_xgboost"></p>
<blockquote>
<p>Structure Score Calculation. We only need to sum up the gradient and second order gradient statistics on each leaf, then apply the scoring formula to get the quality score.</p>
</blockquote>
<p>上图给出目标函数计算的例子，求每个节点每个样本的一阶导数 $g_i$ 和二阶导数 $h_i$ ，然后针对每个节点对所含样本求和得到的 $G_j$ 和  $H_j$ ，最后遍历决策树的节点即可得到目标函数</p>
<p>在实现过程中，我们不需要对目标函数值进行计算，只需根据损失函数求得的一阶导 g 和二阶导 h 对数据集进行划分，寻找最佳分裂点并建树，然后求得叶子节点对应的权值 $w$ 转换得到下一轮boost的 $\hat{y_i}^{t-1}$ ，同时也作为之后进行预测的依据</p>
</li>
<li><p>其中，我们通过计算最大的分裂收益来寻找最佳分裂点，根据上一步简化的目标函数，某一节点分裂前的目标函数可以写为：<br>$$<br>Obj_1 = -\frac{1}{2}\left[\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] + \gamma T<br>$$<br>其中 $G_L$ 表示按当前分裂点分裂得到的左节点的 $G_j$ 值，而分裂后的目标函数为：</p>
<p>$$<br>Obj_2 = -\frac{1}{2}\left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}\right] + \gamma (T+1)<br>$$<br>所以，对于目标函数来说，分裂后的收益为：<br>$$<br>Gain = \frac{1}{2}\left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma<br>$$<br>为了构建能够最小化损失函数的决策树，在每次进行分裂的时候，需要在候选分裂点中选择分裂收益最大的进行分裂，层序进行，直至完成建树的过程</p>
</li>
</ol>
<h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><p>XGBoost在GDBT的基础上进行了一些算法和工程方面的优化</p>
<ol>
<li><p><strong>引入二阶导：</strong>GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数；</p>
</li>
<li><p><strong>正则项：</strong>XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合；</p>
</li>
<li><p><strong>Shrinkage（缩减）：</strong>类似于学习率，XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间；</p>
</li>
<li><p>列抽样：XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算；</p>
</li>
<li><p>缺失值处理：XGBoost 采用的<strong>稀疏感知算法</strong>极大的加快了节点分裂的速度；</p>
</li>
<li><p>并行化：块结构可以很好的支持并行计算；</p>
</li>
</ol>
<p>但仍存在如下的缺点</p>
<ol>
<li>虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；</li>
<li>预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存；</li>
</ol>
<h2 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h2><p>LightGBM 由微软提出，主要用于解决 GDBT 在<strong>海量数据</strong>中遇到的问题，以便其可以更好更快地用于工业实践中</p>
<p>LightGBM 为了解决这些问题提出了以下几点解决方案：</p>
<ol>
<li>单边梯度抽样算法：对样本进行抽样，减少了大量梯度小的样本；</li>
<li><strong>直方图算法（直方图做差加速）</strong>：将连续的特征离散化为 k 个离散特征，同时构造一个宽度为 k 的直方图用于统计信息（包括 k 个 bin）；</li>
<li>互斥特征捆绑算法：将一些特征进行融合绑定，可以降低特征数量（高维特征往往是稀疏的，特征间可能是互斥的）；</li>
<li>基于最大深度的 Leaf-wise 的垂直生长算法：减少了计算量，配合最大深度的限制防止过拟合，由于每次都需要计算增益最大的节点，所以无法并行分裂；</li>
<li>类别特征最优分割；</li>
<li>特征并行/数据并行/投票并行；</li>
<li>缓存优化；</li>
</ol>
<h2 id="DimBoost"><a href="#DimBoost" class="headerlink" title="DimBoost"></a>DimBoost</h2><p><a href="https://dl.acm.org/doi/10.1145/3183713.3196892">DimBoost: Boosting Gradient Boosting Decision Tree to Higher Dimensions</a></p>
<p>另外补充一个针对高维情况的GDBT实现，在特征数量达到百万级别时，其性能要远优于XGBoost</p>
<h2 id="FATE"><a href="#FATE" class="headerlink" title="FATE"></a>FATE</h2><blockquote>
<p> FATE (Federated AI Technology Enabler) 是微众银行AI部门发起的开源项目，为联邦学习生态系统提供了可靠的安全计算框架。FATE项目使用多方安全计算 (MPC) 以及同态加密 (HE) 技术构建底层安全计算协议，以此支持不同种类的机器学习的安全计算，包括逻辑回归、基于树的算法、深度学习和迁移学习等</p>
</blockquote>
<p>FATE官方网站：<a href="https://fate.fedai.org/">https://fate.fedai.org/</a></p>
<p>在进行协作学习实现时，我们主要参考了FATE的homo-secureboost部分代码，并根据需要进行了一些调整</p>
<ul>
<li>采用全局的近似算法作为最优切分点划分算法，即学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割；</li>
<li>使用softmax交叉熵损失函数，并取其一阶导和二阶导建立直方图；</li>
<li>实现LightGBM的直方图算法，包括直方图作差加速；</li>
<li>采用类似于XGBoost的Level-wise增长策略，方便并行计算每一层的分裂节点，提高了训练速度，但同时也因为节点增益过小增加了很多不必要的分裂；</li>
<li>稀疏感知算法 TODO</li>
<li>特征选择 TODO</li>
</ul>
<h1 id="协作学习实现"><a href="#协作学习实现" class="headerlink" title="协作学习实现"></a>协作学习实现</h1><p>先以二分类问题为例说明协作学习实现的过程和原理，再对多分类问题进行简单说明</p>
<h2 id="大致流程"><a href="#大致流程" class="headerlink" title="大致流程"></a>大致流程</h2><ul>
<li><p>Parameter Server</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">获取全局 Quantile Sketch (merge) &lt;- Worker</span><br><span class="line">计算全局候选分裂点 -&gt; Worker</span><br><span class="line"><span class="keyword">for</span> epoch_idx <span class="keyword">in</span> range(self.boosting_round)：</span><br><span class="line">	获取所有局部 g h &lt;- Worker</span><br><span class="line">    计算全局 g h -&gt; Worker</span><br><span class="line">    <span class="keyword">for</span> dep <span class="keyword">in</span> range(self.max_depth):</span><br><span class="line">        获取全局的梯度直方图 (+) &lt;- Worker</span><br><span class="line">        计算最佳分裂点 -&gt; Worker</span><br></pre></td></tr></table></figure>
</li>
<li><p>Worker</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">计算局部 Quantile Sketch -&gt; PS</span><br><span class="line">初始化(候选分裂点，计算y_hat等) &lt;- PS</span><br><span class="line"><span class="keyword">for</span> epoch_idx <span class="keyword">in</span> range(self.boosting_round)：</span><br><span class="line">	根据 y_hat 计算局部 g h -&gt; PS</span><br><span class="line">	初始化决策树（根据全局 g h 、数据集等建立根节点，初始当前层节点为根节点等） &lt;- PS</span><br><span class="line">    <span class="keyword">for</span> dep <span class="keyword">in</span> range(self.max_depth):</span><br><span class="line">        <span class="keyword">if</span> 树达到最大深度：</span><br><span class="line">        	停止树的构建</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            计算当前层各个节点各个特征的梯度直方图 -&gt; PS</span><br><span class="line">            获取当前层各个节点最佳分裂点 &lt;- PS</span><br><span class="line">            根据最佳分裂点建立下一层节点</span><br><span class="line">    根据叶子节点 weight 预测新的 y_hat</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="Quantile-Sketch获取全局候选节点"><a href="#Quantile-Sketch获取全局候选节点" class="headerlink" title="Quantile Sketch获取全局候选节点"></a>Quantile Sketch获取全局候选节点</h2><ul>
<li>每个worker分别收集对应数据集中每个feature 的所有数值并以Quantile Sketch（GK Sketch）进行存储</li>
<li>参数服务器 PS 汇集所有worker的sketch并merge，得到全局的quantile sketch</li>
<li>根据给定的全局参数 bin_num 确定分位点，查询sketch得到对应的数值</li>
</ul>
<h2 id="梯度直方图-amp-分裂点计算"><a href="#梯度直方图-amp-分裂点计算" class="headerlink" title="梯度直方图 &amp; 分裂点计算"></a>梯度直方图 &amp; 分裂点计算</h2><blockquote>
<p>梯度直方图：对于一个树结点，扫描树结点上的所有训练样本，根据其特征值，将样本梯度累加到对应的直方图桶中（候选节点将特征的值域分割成n个桶）</p>
</blockquote>
<p><img src="f3_histogram.png" alt="f3_histogram"></p>
<blockquote>
<p>Histogram-based split finding for one feature</p>
</blockquote>
<ol>
<li><p><strong>计算梯度直方图：</strong> 从待处理树节点的队列中取出待处理的树节点，在Worker上，根据本节点的训练数据计算局部一阶和二阶梯度直方图，对于每个节点的所有特征都有 g 和 h 对应的两个直方图，直方图以bin_id（根据候选分裂点划分）为横坐标，g h 值作为纵坐标</p>
</li>
<li><p><strong>同步&amp;合并直方图：</strong> Worker将局部梯度直方图推送到参数服务器，PS节点在接收到Worker发送的局部梯度直方图后，根据梯度直方图对应的树节点，将其按照特征值和 bin_id 加到全局梯度直方图上</p>
<p><img src="f4_SGBDT.png" alt="f4_LSGBDT"></p>
<blockquote>
<p>Workers construct local histograms for all features and aggregate into global ones.</p>
</blockquote>
</li>
<li><p><strong>寻找最佳分裂点：</strong> 参数服务器根据每个特征的梯度直方图计算对于该特征的最佳分裂点及其增益，选取其中目标函数增益最大的特征以及对应的候选分裂点作为全局的最佳分裂点。如果没有合适的分裂点，则将当前节点标记为叶子节点，不再进行后续分裂</p>
</li>
<li><p><strong>分裂树节点：</strong> Worker根据计算得到的最佳分裂点，创建节点，将本节点的训练数据切分到两个节点上，并根据分裂得到的 $G_L$ 和 $H_L$ 或 $G_R$ 和 $H_R$ 计算节点的 weight 值。如果树的高度没有达到最大限制，则将两个叶子节点加入到待处理树节点的队列</p>
</li>
</ol>
<h2 id="多分类实现-amp-predict"><a href="#多分类实现-amp-predict" class="headerlink" title="多分类实现 &amp; predict"></a>多分类实现 &amp; predict</h2><ul>
<li><p>在每一轮boost中对每个class分别训练决策树，将当前类别的 y 值视为 1，其他类别视为 0 进行1 vs rest二分类进行树的构建以及 $w$ 的计算</p>
</li>
<li><p>遵循 1 vs rest 分解策略，训练好 $|\mathcal Y|$ 个二分类器后，将每个分类器的实值输出 $H(\boldsymbol{x}) = \sum_{t=1}^T \alpha _th_t(\boldsymbol{x})$ 用于识别最有可能的多分类类别，即<br>$$<br>H(\boldsymbol{x}) = \underset{y \in \mathcal Y}{\arg\max} H_y(\boldsymbol{x})<br>$$</p>
</li>
<li><p>在具体实现时，我们通过将测试样本投入每轮 boost 的每个class对应的决策树中（共 boosting_round * class_num 棵），将多轮boost中对应同一class的决策树的预测值结合学习率进行累加计算，于是每个测试样本得到对应 class_num 个类的 class_num 个值，取最大值对应的 class 作为该样本的预测值</p>
</li>
</ul>
<hr>
<h1 id="PS"><a href="#PS" class="headerlink" title="PS"></a>PS</h1><h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><table>
<thead>
<tr>
<th>变量</th>
<th>shape</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>X</td>
<td>(sample_num, feature_num)</td>
<td></td>
</tr>
<tr>
<td>y</td>
<td>(sample_num)</td>
<td></td>
</tr>
<tr>
<td>y_hat ( $\hat{y}$ )</td>
<td>(sample_num, [class_num])</td>
<td>y 的预测值，对于多分类情况第二个维度大小为类的数量</td>
</tr>
<tr>
<td>g_h</td>
<td>(sample_num, 2)</td>
<td>记录每个样本对应的 g 值和 h 值</td>
</tr>
<tr>
<td>inst2node_idx</td>
<td>(sample_num, 2)</td>
<td>将样本记录与树节点关联</td>
</tr>
<tr>
<td>bin_split_points</td>
<td>(feature_num, &lt;bin_num)</td>
<td>记录候选分裂点，每个特征的候选分裂点最大为 bin_num，不同特征分裂点数量不尽相同</td>
</tr>
<tr>
<td>histograms</td>
<td>(node_num, feature_num, &lt;bin_num, 2)</td>
<td>记录梯度直方图</td>
</tr>
<tr>
<td>sample_weights</td>
<td>(sample_num)</td>
<td>记录样本对应的weights</td>
</tr>
</tbody></table>
<p>其中，sample__num表示样本数量，feature_num表示特征数量，class_num表示类数量，bin_num表示根据候选分裂点划分得到的特征取值区间数量，node_num表示当前层节点数量</p>
<h2 id="Softmax交叉熵损失函数"><a href="#Softmax交叉熵损失函数" class="headerlink" title="Softmax交叉熵损失函数"></a>Softmax交叉熵损失函数</h2><p>$$<br>\begin{aligned}<br>&amp;l (y_i, \hat{y_i}^{(t-1)}) = y_i log(1 + e^{−\hat{y_i}^{(t-1)}} ) + (1 − y_i )log(1 + e^{\hat{y_i}^{t-1}} )<br>\\&amp;g_i = \frac{1}{1+e^{-\hat{y_i}^{(t-1)}}} - y_i<br>\\&amp;h_i = \frac{1}{1+e^{-\hat{y_i}^{(t-1)}}} * (1-\frac{1}{1+e^{-\hat{y_i}^{(t-1)}}})<br>\end{aligned}<br>$$</p>
<h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h2><p>secureBoost对softmax的实现似乎与传统的Softmax交叉熵损失函数不同，下面是它的代码实现，感兴趣的读者可以尝试分析证明</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span>(<span class="params">x, axis=<span class="number">-1</span></span>):</span></span><br><span class="line">    y = np.exp(x - np.max(x, axis, keepdims=<span class="literal">True</span>))</span><br><span class="line">    <span class="keyword">return</span> y / np.sum(y, axis, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>$$<br>\begin{aligned}<br>&amp;Softmax(\boldsymbol{x}) = \frac{e^{\boldsymbol{x}-\Vert\boldsymbol{x}\Vert_{\infty}}}{\Vert e^{\boldsymbol{x}-\Vert\boldsymbol{x}\Vert_{\infty}}\Vert_1}<br>\\&amp;g_i = Softmax(\hat{y_i}^{(t-1)}) - y_i<br>\\&amp;h_i = Softmax(\hat{y_i}^{(t-1)}) * (1-Softmax(\hat{y_i}^{(t-1)}))<br>\\&amp;\hat{y_i}^{(t)} = Softmax(w_j^*) = Softmax(-\frac{G_j}{H_j+\lambda}) = Softmax(-\frac{\sum_{i \in I_j}g_i}{\sum_{i \in I_j}h_i + \lambda})<br>\end{aligned}<br>$$</p>
]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>联邦学习</tag>
        <tag>协作学习</tag>
        <tag>决策树算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Istio &amp; AIOps环境搭建</title>
    <url>/2021/06/30/Istio-AIOps%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<p>在AIOps挑战赛学习了智能运维相关的内容，尝试在Service Mesh环境下模拟配置挑战赛环境</p>
<p>使用minikube搭建微服务网络，通过Istio进行监控，分别通过Prometheus、Fluented以及Jaeger获取服务的指标、日志和调用链信息，并汇总到Elasticsearch用作后续分析</p>
<p>后续发现使用全栈的Elastic解决方案能够不依赖istio实现轻松可靠的在Elasticseach中汇总指标、日志和性能追踪，于是又尝试了如下的解决方案<a href="https://www.qikqiak.com/post/k8s-monitor-use-elastic-stack-1/">https://www.qikqiak.com/post/k8s-monitor-use-elastic-stack-1/</a></p>
<p>除此之外，<a href="https://juejin.cn/post/6847902217979052045">KubeSphere</a>汇总了包括运维数据获取等的一系列解决方案，可以非常方便地安装与管理用户最常用的云原生工具</p>
<a id="more"></a>

<h1 id="Istio-环境配置"><a href="#Istio-环境配置" class="headerlink" title="Istio 环境配置"></a>Istio 环境配置</h1><p>参考教程配置环境<a href="https://istio.io/latest/docs/setup/getting-started/">https://istio.io/latest/docs/setup/getting-started/</a></p>
<h2 id="配置流程"><a href="#配置流程" class="headerlink" title="配置流程"></a>配置流程</h2><p>参见官网demo <a href="https://istio.io/latest/docs/setup/getting-started/">https://istio.io/latest/docs/setup/getting-started/</a></p>
<p>安装minikube <a href="https://minikube.sigs.k8s.io/docs/start/#install-a-hypervisor">https://minikube.sigs.k8s.io/docs/start/#install-a-hypervisor</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">minikube start</span><br><span class="line">kubectl get po -A</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">istioctl install --set profile=demo -y</span><br><span class="line">kubectl label namespace default istio-injection=enabled</span><br><span class="line">kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml</span><br><span class="line">kubectl get services</span><br><span class="line">kubectl get pods //等待直到所有pods处于Running状态</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl exec &quot;$(kubectl get pod -l app=ratings -o jsonpath=&#x27;&#123;.items[0].metadata.name&#125;&#x27;)&quot; -c ratings -- curl -sS productpage:9080/productpage | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot;</span><br><span class="line">//命令行返回&lt;title&gt;Simple Bookstore App&lt;/title&gt;则当前配置过程正常</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">//创建一个Istio Ingress Gateway，该网关将路径映射到网格边缘处的路线</span><br><span class="line">kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml</span><br><span class="line">istioctl analyze</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=&#x27;&#123;.spec.ports[?(@.name==&quot;http2&quot;)].nodePort&#125;&#x27;)</span><br><span class="line">export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=&#x27;&#123;.spec.ports[?(@.name==&quot;https&quot;)].nodePort&#125;&#x27;)</span><br><span class="line">export INGRESS_HOST=$(minikube ip)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">minikube tunnel &#x2F;&#x2F;在新的终端窗口中运行以下命令，以启动将流量发送到Istio Ingress网关的Minikube隧道</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT</span><br><span class="line">echo &quot;http://$GATEWAY_URL/productpage&quot;</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl apply -f samples/addons</span><br><span class="line">kubectl rollout status deployment/kiali -n istio-system</span><br><span class="line">istioctl dashboard kiali</span><br></pre></td></tr></table></figure>

<h2 id="kubernets相关命令"><a href="#kubernets相关命令" class="headerlink" title="kubernets相关命令"></a>kubernets相关命令</h2><p>minikube是本地的Kubernetes，致力于使Kubernetes易于学习和开发</p>
<p><a href="https://minikube.sigs.k8s.io/docs/start/#install-a-hypervisor">https://minikube.sigs.k8s.io/docs/start/#install-a-hypervisor</a></p>
<p><a href="https://istio.io/latest/docs/setup/platform-setup/minikube/">https://istio.io/latest/docs/setup/platform-setup/minikube/</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">minikube start --driver=docker</span><br><span class="line">minikube start --memory=16384 --cpus=4 --kubernetes-version=v1.20.2</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl get po -A</span><br><span class="line">minikube kubectl -- get po -A </span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">minikube pause</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">minikube stop</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">minikube delete --all</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl get po -A //查看所有pods，第一列为命名空間</span><br><span class="line">kubectl get pod -n istio-system //查看命名空间istio-system下的pods</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl get deployment -n istio-system</span><br><span class="line">kubectl delete deployment prometheus -n istio-system</span><br><span class="line">kubectl delete pod prometheus -n istio-system</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl delete pod prometheus -n istio-system //从命名空间istio-system删除prometheus</span><br><span class="line">kubectl rollout status deployment/prometheus -n istio-system //重新安装prometheus</span><br></pre></td></tr></table></figure>

<h2 id="Istio相关命令"><a href="#Istio相关命令" class="headerlink" title="Istio相关命令"></a>Istio相关命令</h2><p><a href="https://www.servicemesher.com/istio-handbook/practice/setup-istio.html">https://www.servicemesher.com/istio-handbook/practice/setup-istio.html</a></p>
<p><a href="https://istio.io/latest/docs/setup/install/istioctl/">https://istio.io/latest/docs/setup/install/istioctl/</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">istioctl install</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">istioctl install --set profile=demo</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">istioctl x uninstall --purge</span><br></pre></td></tr></table></figure>

<p>可选<code>--purge</code>标志将删除所有Istio资源，包括可能与其他Istio控制平面共享的群集范围的资源。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">istioctl dashboard //查看可用的仪表盘</span><br></pre></td></tr></table></figure>

<h1 id="运维环境搭建"><a href="#运维环境搭建" class="headerlink" title="运维环境搭建"></a>运维环境搭建</h1><p>将指标、日志以及调用链数据集中到elasticsearch以便后续分析</p>
<p><a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/pull-image-private-registry/">https://kubernetes.io/zh/docs/tasks/configure-pod-container/pull-image-private-registry/</a></p>
<p>环境搭建速度较慢时考虑使用本地镜像</p>
<h2 id="Prometheus配置"><a href="#Prometheus配置" class="headerlink" title="Prometheus配置"></a>Prometheus配置</h2><p><a href="https://www.servicemesher.com/istio-handbook/practice/prometheus.html">https://www.servicemesher.com/istio-handbook/practice/prometheus.html</a></p>
<p>istio的sample/addons目录提供了prometheus的yaml文件，之前通过<code>kubectl apply -f samples/addons</code>已安装，可以直接<code>istioctl dashboard prometheus</code>进行观察</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">//进入prometheus所在pod</span><br><span class="line">//pod名通过kubectl get po -A命令查找并替换</span><br><span class="line">kubectl exec -it prometheus-7cb88b5945-8jbkf -n istio-system sh</span><br></pre></td></tr></table></figure>

<p>找到配置文件prometheus.yml并打开，我的在/etc/config/prometheus.yml并修改</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">//也可以通过修改 Prometheus 的 ConfigMap 对其配置进行如下修改</span><br><span class="line">kubectl edit configmap prometheus -n istio-system </span><br></pre></td></tr></table></figure>

<p>修改完成后保存，需要给 Prometheus 加上 –web.enable-lifecycle 启动参数来支持热更新（可能默认支持）</p>
<p>这些配置参数在prometheus.yaml中已经定义</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl edit deployment prometheus -n istio-system</span><br><span class="line"> ...</span><br><span class="line"> containers:</span><br><span class="line">      - args:</span><br><span class="line">        - --storage.tsdb.retention=6h</span><br><span class="line"></span><br><span class="line">        - --config.file=/etc/prometheus/prometheus.yml</span><br><span class="line"></span><br><span class="line">        - --web.enable-lifecycle</span><br><span class="line">          image: docker.io/prom/prometheus:v2.15.1</span><br><span class="line">          imagePullPolicy: IfNotPresent</span><br><span class="line"> ...</span><br></pre></td></tr></table></figure>

<p>然后再调用 Prometheus 的 HTTP API 即可更新配置：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -X POST http://&lt;Prometheus URL&gt;:9090/-/reload</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">istioctl dashboard prometheus</span><br></pre></td></tr></table></figure>

<p><a href="https://www.elastic.co/cn/what-is/prometheus-monitoring">https://www.elastic.co/cn/what-is/prometheus-monitoring</a></p>
<p>将prometheus数据导出到elasticsearch</p>
<h2 id="EFK配置"><a href="#EFK配置" class="headerlink" title="EFK配置"></a>EFK配置</h2><p><a href="https://www.servicemesher.com/istio-handbook/practice/efk.html">https://www.servicemesher.com/istio-handbook/practice/efk.html</a></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 参考efk.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># Logging Namespace. All below are a part of this namespace.</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Namespace</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">logging</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># Elasticsearch Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"> <span class="attr">name:</span> <span class="string">elasticsearch</span></span><br><span class="line"> <span class="attr">namespace:</span> <span class="string">logging</span></span><br><span class="line"> <span class="attr">labels:</span></span><br><span class="line">   <span class="attr">app:</span> <span class="string">elasticsearch</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"> <span class="attr">ports:</span></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">9200</span></span><br><span class="line">   <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">   <span class="attr">targetPort:</span> <span class="string">db</span></span><br><span class="line"> <span class="attr">selector:</span></span><br><span class="line">   <span class="attr">app:</span> <span class="string">elasticsearch</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># Elasticsearch Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"> <span class="attr">name:</span> <span class="string">elasticsearch</span></span><br><span class="line"> <span class="attr">namespace:</span> <span class="string">logging</span></span><br><span class="line"> <span class="attr">labels:</span></span><br><span class="line">   <span class="attr">app:</span> <span class="string">elasticsearch</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"> <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line"> <span class="attr">selector:</span></span><br><span class="line">   <span class="attr">matchLabels:</span></span><br><span class="line">     <span class="attr">app:</span> <span class="string">elasticsearch</span></span><br><span class="line"> <span class="attr">template:</span></span><br><span class="line">   <span class="attr">metadata:</span></span><br><span class="line">     <span class="attr">labels:</span></span><br><span class="line">       <span class="attr">app:</span> <span class="string">elasticsearch</span></span><br><span class="line">     <span class="attr">annotations:</span></span><br><span class="line">       <span class="attr">sidecar.istio.io/inject:</span> <span class="string">&quot;false&quot;</span></span><br><span class="line">   <span class="attr">spec:</span></span><br><span class="line">     <span class="attr">containers:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">zhangguanzhang/docker.elastic.co.elasticsearch.elasticsearch-oss:6.1.1</span></span><br><span class="line">       <span class="attr">name:</span> <span class="string">elasticsearch</span></span><br><span class="line">       <span class="attr">resources:</span></span><br><span class="line">         <span class="comment"># need more cpu upon initialization, therefore burstable class</span></span><br><span class="line">         <span class="attr">limits:</span></span><br><span class="line">           <span class="attr">cpu:</span> <span class="string">1000m</span></span><br><span class="line">         <span class="attr">requests:</span></span><br><span class="line">           <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">       <span class="attr">env:</span></span><br><span class="line">         <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">discovery.type</span></span><br><span class="line">           <span class="attr">value:</span> <span class="string">single-node</span></span><br><span class="line">       <span class="attr">ports:</span></span><br><span class="line">       <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">9200</span></span><br><span class="line">         <span class="attr">name:</span> <span class="string">db</span></span><br><span class="line">         <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">       <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">9300</span></span><br><span class="line">         <span class="attr">name:</span> <span class="string">transport</span></span><br><span class="line">         <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">       <span class="attr">volumeMounts:</span></span><br><span class="line">       <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">elasticsearch</span></span><br><span class="line">         <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">     <span class="attr">imagePullSecrets:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">regcred</span></span><br><span class="line">     <span class="attr">volumes:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">elasticsearch</span></span><br><span class="line">       <span class="attr">emptyDir:</span> &#123;&#125;</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># Fluentd Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"> <span class="attr">name:</span> <span class="string">fluentd-es</span></span><br><span class="line"> <span class="attr">namespace:</span> <span class="string">logging</span></span><br><span class="line"> <span class="attr">labels:</span></span><br><span class="line">   <span class="attr">app:</span> <span class="string">fluentd-es</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"> <span class="attr">ports:</span></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">fluentd-tcp</span></span><br><span class="line">   <span class="attr">port:</span> <span class="number">24224</span></span><br><span class="line">   <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">   <span class="attr">targetPort:</span> <span class="number">24224</span></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">fluentd-udp</span></span><br><span class="line">   <span class="attr">port:</span> <span class="number">24224</span></span><br><span class="line">   <span class="attr">protocol:</span> <span class="string">UDP</span></span><br><span class="line">   <span class="attr">targetPort:</span> <span class="number">24224</span></span><br><span class="line"> <span class="attr">selector:</span></span><br><span class="line">   <span class="attr">app:</span> <span class="string">fluentd-es</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># Fluentd ConfigMap, contains config files.</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"> <span class="attr">forward.input.conf:</span> <span class="string">|-</span></span><br><span class="line">   <span class="comment"># Takes the messages sent over TCP</span></span><br><span class="line">   <span class="string">&lt;source&gt;</span></span><br><span class="line">     <span class="string">@id</span> <span class="string">fluentd-containers.log</span></span><br><span class="line">     <span class="string">@type</span> <span class="string">tail</span></span><br><span class="line">     <span class="string">path</span> <span class="string">/var/log/containers/*.log</span></span><br><span class="line">     <span class="string">pos_file</span> <span class="string">/var/log/es-containers.log.pos</span></span><br><span class="line">     <span class="string">time_format</span> <span class="string">%Y-%m-%dT%H:%M:%S.%NZ</span></span><br><span class="line">     <span class="string">tag</span> <span class="string">raw.kubernetes.*</span></span><br><span class="line">     <span class="string">format</span> <span class="string">json</span></span><br><span class="line">     <span class="string">read_from_head</span> <span class="literal">false</span></span><br><span class="line">   <span class="string">&lt;/source&gt;</span></span><br><span class="line">   <span class="string">&lt;filter</span> <span class="string">**&gt;</span></span><br><span class="line">     <span class="string">@id</span> <span class="string">filter_concat</span></span><br><span class="line">     <span class="string">@type</span> <span class="string">concat</span></span><br><span class="line">     <span class="string">key</span> <span class="string">message</span></span><br><span class="line">     <span class="string">multiline_end_regexp</span> <span class="string">/\n$/</span></span><br><span class="line">     <span class="string">separator</span> <span class="string">&quot;&quot;</span></span><br><span class="line">   <span class="string">&lt;/filter&gt;</span></span><br><span class="line">   <span class="string">&lt;filter</span> <span class="string">**&gt;</span></span><br><span class="line">     <span class="string">@type</span> <span class="string">parser</span></span><br><span class="line">     <span class="string">format</span> <span class="string">json</span> <span class="comment"># apache2, nginx, etc...</span></span><br><span class="line">     <span class="string">key_name</span> <span class="string">log</span></span><br><span class="line">     <span class="string">reserve_data</span> <span class="literal">false</span></span><br><span class="line">   <span class="string">&lt;/filter&gt;</span></span><br><span class="line"> <span class="attr">output.conf:</span> <span class="string">|-</span></span><br><span class="line">   <span class="string">&lt;match</span> <span class="string">**&gt;</span></span><br><span class="line">     <span class="string">type</span> <span class="string">elasticsearch</span></span><br><span class="line">     <span class="string">log_level</span> <span class="string">info</span></span><br><span class="line">     <span class="string">include_tag_key</span> <span class="literal">true</span></span><br><span class="line">     <span class="string">host</span> <span class="string">elasticsearch</span></span><br><span class="line">     <span class="string">port</span> <span class="number">9200</span></span><br><span class="line">     <span class="string">logstash_format</span> <span class="literal">true</span></span><br><span class="line">     <span class="comment"># Set the chunk limits.</span></span><br><span class="line">     <span class="string">buffer_chunk_limit</span> <span class="string">2M</span></span><br><span class="line">     <span class="string">buffer_queue_limit</span> <span class="number">8</span></span><br><span class="line">     <span class="string">flush_interval</span> <span class="string">5s</span></span><br><span class="line">     <span class="comment"># Never wait longer than 5 minutes between retries.</span></span><br><span class="line">     <span class="string">max_retry_wait</span> <span class="number">30</span></span><br><span class="line">     <span class="comment"># Disable the limit on the number of retries (retry forever).</span></span><br><span class="line">     <span class="string">disable_retry_limit</span></span><br><span class="line">     <span class="comment"># Use multiple threads for processing.</span></span><br><span class="line">     <span class="string">num_threads</span> <span class="number">2</span></span><br><span class="line">   <span class="string">&lt;/match&gt;</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"> <span class="attr">name:</span> <span class="string">fluentd-es-config</span></span><br><span class="line"> <span class="attr">namespace:</span> <span class="string">logging</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"> <span class="attr">name:</span> <span class="string">fluentd-es</span></span><br><span class="line"> <span class="attr">namespace:</span> <span class="string">logging</span></span><br><span class="line"> <span class="attr">labels:</span></span><br><span class="line">   <span class="attr">app:</span> <span class="string">fluentd-es</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"> <span class="attr">selector:</span></span><br><span class="line">   <span class="attr">matchLabels:</span></span><br><span class="line">     <span class="attr">app:</span> <span class="string">fluentd-es</span></span><br><span class="line"> <span class="attr">template:</span></span><br><span class="line">   <span class="attr">metadata:</span></span><br><span class="line">     <span class="attr">labels:</span></span><br><span class="line">       <span class="attr">app:</span> <span class="string">fluentd-es</span></span><br><span class="line">     <span class="attr">annotations:</span></span><br><span class="line">       <span class="attr">sidecar.istio.io/inject:</span> <span class="string">&quot;false&quot;</span></span><br><span class="line">   <span class="attr">spec:</span></span><br><span class="line">     <span class="attr">containers:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">fluentd-es</span></span><br><span class="line">       <span class="attr">image:</span> <span class="string">quay.io/fluentd_elasticsearch/fluentd:v3.0.2</span></span><br><span class="line">       <span class="attr">env:</span></span><br><span class="line">       <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">FLUENTD_ARGS</span></span><br><span class="line">         <span class="attr">value:</span> <span class="string">--no-supervisor</span> <span class="string">-q</span></span><br><span class="line">       <span class="attr">resources:</span></span><br><span class="line">         <span class="attr">limits:</span></span><br><span class="line">           <span class="attr">memory:</span> <span class="string">500Mi</span></span><br><span class="line">         <span class="attr">requests:</span></span><br><span class="line">           <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">           <span class="attr">memory:</span> <span class="string">200Mi</span></span><br><span class="line">       <span class="attr">volumeMounts:</span></span><br><span class="line">       <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlog</span></span><br><span class="line">         <span class="attr">mountPath:</span> <span class="string">/var/log</span></span><br><span class="line">       <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlibdockercontainers</span></span><br><span class="line">         <span class="attr">mountPath:</span> <span class="string">/var/lib/docker/containers</span></span><br><span class="line">         <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">       <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">         <span class="attr">mountPath:</span> <span class="string">/etc/fluent/config.d</span></span><br><span class="line">     <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">30</span></span><br><span class="line">     <span class="attr">volumes:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlog</span></span><br><span class="line">       <span class="attr">hostPath:</span></span><br><span class="line">         <span class="attr">path:</span> <span class="string">/var/log</span></span><br><span class="line">     <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlibdockercontainers</span></span><br><span class="line">       <span class="attr">hostPath:</span></span><br><span class="line">         <span class="attr">path:</span> <span class="string">/var/lib/docker/containers</span></span><br><span class="line">     <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">       <span class="attr">configMap:</span></span><br><span class="line">         <span class="attr">name:</span> <span class="string">fluentd-es-config</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># Kibana Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"> <span class="attr">name:</span> <span class="string">kibana</span></span><br><span class="line"> <span class="attr">namespace:</span> <span class="string">logging</span></span><br><span class="line"> <span class="attr">labels:</span></span><br><span class="line">   <span class="attr">app:</span> <span class="string">kibana</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"> <span class="attr">ports:</span></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">5601</span></span><br><span class="line">   <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">   <span class="attr">targetPort:</span> <span class="string">ui</span></span><br><span class="line"> <span class="attr">selector:</span></span><br><span class="line">   <span class="attr">app:</span> <span class="string">kibana</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># Kibana Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"> <span class="attr">name:</span> <span class="string">kibana</span></span><br><span class="line"> <span class="attr">namespace:</span> <span class="string">logging</span></span><br><span class="line"> <span class="attr">labels:</span></span><br><span class="line">   <span class="attr">app:</span> <span class="string">kibana</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"> <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line"> <span class="attr">selector:</span></span><br><span class="line">   <span class="attr">matchLabels:</span></span><br><span class="line">     <span class="attr">app:</span> <span class="string">kibana</span></span><br><span class="line"> <span class="attr">template:</span></span><br><span class="line">   <span class="attr">metadata:</span></span><br><span class="line">     <span class="attr">labels:</span></span><br><span class="line">       <span class="attr">app:</span> <span class="string">kibana</span></span><br><span class="line">     <span class="attr">annotations:</span></span><br><span class="line">       <span class="attr">sidecar.istio.io/inject:</span> <span class="string">&quot;false&quot;</span></span><br><span class="line">   <span class="attr">spec:</span></span><br><span class="line">     <span class="attr">containers:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kibana</span></span><br><span class="line">       <span class="attr">image:</span> <span class="string">zhangguanzhang/docker.elastic.co.kibana.kibana-oss:6.1.1</span></span><br><span class="line">       <span class="attr">resources:</span></span><br><span class="line">         <span class="comment"># need more cpu upon initialization, therefore burstable class</span></span><br><span class="line">         <span class="attr">limits:</span></span><br><span class="line">           <span class="attr">cpu:</span> <span class="string">1000m</span></span><br><span class="line">         <span class="attr">requests:</span></span><br><span class="line">           <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">       <span class="attr">env:</span></span><br><span class="line">         <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ELASTICSEARCH_URL</span></span><br><span class="line">           <span class="attr">value:</span> <span class="string">http://elasticsearch:9200</span></span><br><span class="line">       <span class="attr">ports:</span></span><br><span class="line">       <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">5601</span></span><br><span class="line">         <span class="attr">name:</span> <span class="string">ui</span></span><br><span class="line">         <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">     <span class="attr">imagePullSecrets:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">regcred</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">//具体设置参数的位置需要参考demo.yaml，1.10版本默认使用meshConfig.accessLogFile为/dev/stdout</span><br><span class="line">istioctl manifest apply --set profile=demo --set values.global.proxy.accessLogFile=&quot;/dev/stdout&quot;</span><br></pre></td></tr></table></figure>

<p>以下通过部署 <code>sleep</code> 示例应用程序来测试日志采集过程</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl apply -f samples/sleep/sleep.yaml</span><br><span class="line">kubectl apply -f &lt;(istioctl kube-inject -f samples/sleep/sleep.yaml) //手动注入sidecar</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl logs -l app=sleep -c istio-proxy //查看应用日志</span><br></pre></td></tr></table></figure>

<p><a href="https://www.servicemesher.com/istio-handbook/practice/efk.html">https://www.servicemesher.com/istio-handbook/practice/efk.html</a></p>
<p>参考上文构建EFK的YAML文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl apply -f samples/efk.yaml</span><br></pre></td></tr></table></figure>

<ol>
<li><p>执行命令产生访问日志：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl exec -it $(kubectl get pod -l app=sleep -o jsonpath=&#x27;&#123;.items[0].metadata.name&#125;&#x27;) -c sleep -- curl -v httpbin:8000/status/418</span><br></pre></td></tr></table></figure>

<blockquote>
<p>如果你已经按照本书部署了 <code>Bookinfo</code> 示例，你也可以直接通过浏览器访问 /productpage 页面也可以产生访问日志。</p>
</blockquote>
</li>
<li><p>设置 Kibana 的端口转发：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl -n logging port-forward $(kubectl -n logging get pod -l app=kibana -o jsonpath=&#x27;&#123;.items[0].metadata.name&#125;&#x27;) 5601:5601 &amp;</span><br></pre></td></tr></table></figure>

<ul>
<li>此命令将 Kibaba <a href="https://www.servicemesher.com/istio-handbook/GLOSSARY.html#pod">Pod</a> 的 <code>5601</code> 端口转发到 <code>localhost:5601</code> ，<code>&amp;</code> 代表后台运行</li>
</ul>
</li>
</ol>
<p>后续需要对 <code>Fluentd</code> 进行细致的配置。如果用于生产环境，可以前往 Kubernetes 官方 github 仓库找到完整的 <code>EFK</code> 配置来进行部署：</p>
<p><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch">https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch</a></p>
<h2 id="Jaeger配置"><a href="#Jaeger配置" class="headerlink" title="Jaeger配置"></a>Jaeger配置</h2><p><a href="https://www.servicemesher.com/istio-handbook/practice/jaeger.html">https://www.servicemesher.com/istio-handbook/practice/jaeger.html</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl apply -f samples/addons/jaeger.yaml</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">istioctl dashboard jaeger</span><br></pre></td></tr></table></figure>

<p>可以在jaeger.yaml配置文件中修改参数</p>
<p>将调用链数据持久化存储 <a href="https://www.jaegertracing.io/docs/1.23/operator/#production-strategy">https://www.jaegertracing.io/docs/1.23/operator/#production-strategy</a></p>
<p><code>#TODO</code> </p>
<ul>
<li><code>Jaeger Collector</code> 默认采用直接写入存储服务的方式，大规模的使用场景下建议使用 Kafka 作为中间缓存区。</li>
</ul>
<h1 id="Istio构架"><a href="#Istio构架" class="headerlink" title="Istio构架"></a>Istio构架</h1><p><a href="https://www.servicemesher.com/istio-handbook/concepts/basic.html">https://www.servicemesher.com/istio-handbook/concepts/basic.html</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/145544378">https://zhuanlan.zhihu.com/p/145544378</a></p>
<blockquote>
<p>服务网格是一个<strong>基础设施层</strong>，用于处理服务间通信。云原生应用有着复杂的服务拓扑，服务网格保证<strong>请求在这些拓扑中可靠地穿梭</strong>。在实际应用当中，服务网格通常是由一系列轻量级的<strong>网络代理</strong>组成的，它们与应用程序部署在一起，但<strong>对应用程序透明</strong>。</p>
</blockquote>
<p>Istio 服务网格在逻辑上分为控制平面和数据平面两部分。其中，控制平面 <strong>Pilot</strong> 负责管理和配置代理来路由流量，并配置 <strong>Mixer</strong> 以实施策略和收集遥测数据；数据平面由一组以 Sidecar 方式部署的智能代理（<strong>Envoy</strong>）组成，这些代理可以调节和控制微服务及 <strong>Mixer</strong> 之间所有的网络通信。</p>
<p>控制平面的特点：</p>
<ul>
<li>不直接解析数据包</li>
<li>与控制平面中的代理通信，下发策略和配置</li>
<li>负责网络行为的可视化</li>
<li>通常提供 API 或者命令行工具可用于配置版本化管理，便于持续集成和部署</li>
</ul>
<p>数据平面的特点：</p>
<ul>
<li>通常是按照无状态目标设计的，但实际上为了提高流量转发性能，需要缓存一些数据</li>
<li>直接处理入站和出站数据包，转发、路由、健康检查、负载均衡、认证、鉴权、产生监控数据等</li>
<li>对应用来说透明，即可以做到无感知部署</li>
</ul>
<p><strong>优点</strong></p>
<ul>
<li>屏蔽分布式系统通信的复杂性(负载均衡、服务发现、认证授权、监控追踪、流量控制等等)，服务只用关注业务逻辑；</li>
<li>真正的语言无关，服务可以用任何语言编写，只需和Service Mesh通信即可；</li>
<li>对应用透明，Service Mesh组件可以单独升级；</li>
</ul>
<p><strong>挑战</strong></p>
<ul>
<li>Service Mesh组件以代理模式计算并转发请求，一定程度上会降低通信系统性能，并增加系统资源开销；</li>
<li>Service Mesh组件接管了网络流量，因此服务的整体稳定性依赖于Service Mesh，同时额外引入的大量Service Mesh服务实例的运维和管理也是一个挑战；</li>
</ul>
<h2 id="Kubernetes"><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h2><p><a href="https://jimmysong.io/kubernetes-handbook/concepts/">https://jimmysong.io/kubernetes-handbook/concepts/</a></p>
<h1 id="微服务"><a href="#微服务" class="headerlink" title="微服务"></a>微服务</h1><p><a href="https://zhuanlan.zhihu.com/p/61901608">https://zhuanlan.zhihu.com/p/61901608</a></p>
<ul>
<li>不同服务间直接进行通信</li>
<li>加入对<strong>网络传输问题</strong>的处理逻辑</li>
<li>使用<strong>TCP协议</strong>处理网络传输问题</li>
<li>第一代微服务：<strong>分布式系统</strong>发展带来的新需求（分布式系统的通用语义），如熔断策略、负载均衡、服务发现、认证和授权、quota限制、trace和监控等</li>
<li>第二代微服务：<strong>面向微服务架构的开发框架</strong>出现</li>
<li>第一代 Service Mesh：Linkerd，Envoy，NginxMesh为代表的<strong>代理模式（边车模式）</strong>将分布式服务的通信抽象为单独一层，在这一层中实现负载均衡、服务发现、认证授权、监控追踪、流量控制等分布式系统所需要的功能，作为一个和服务对等的代理服务，和服务部署在一起，接管服务的流量，通过代理之间的通信间接完成服务之间的通信请求，从而保证微服务语言无关的特性，解决版本兼容问题，同时避免开发者掌握管理复杂框架本身花费的时间精力</li>
<li>第二代 Service Mesh：以 Istio 为代表，提供统一的上层运维入口，演化出了集中式的控制面板，所有的单机代理组件通过和控制面板交互进行<strong>网络拓扑策略的更新</strong>和<strong>单机数据的汇报</strong></li>
</ul>
<h1 id="使用-Elastic-技术栈构建-K8S-全栈监控"><a href="#使用-Elastic-技术栈构建-K8S-全栈监控" class="headerlink" title="使用 Elastic 技术栈构建 K8S 全栈监控"></a>使用 Elastic 技术栈构建 K8S 全栈监控</h1><p><a href="https://www.qikqiak.com/post/k8s-monitor-use-elastic-stack-1/">https://www.qikqiak.com/post/k8s-monitor-use-elastic-stack-1/</a></p>
<p><a href="https://www.qikqiak.com/post/k8s-monitor-use-elastic-stack-2/">https://www.qikqiak.com/post/k8s-monitor-use-elastic-stack-2/</a></p>
<p><a href="https://www.qikqiak.com/post/k8s-monitor-use-elastic-stack-3/">https://www.qikqiak.com/post/k8s-monitor-use-elastic-stack-3/</a></p>
<p><a href="https://www.qikqiak.com/post/k8s-monitor-use-elastic-stack-4/">https://www.qikqiak.com/post/k8s-monitor-use-elastic-stack-4/</a></p>
<p>在安装 ElasticSearch数据节点时，storageClass需要绑定现有的storageclass name</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl get sc # 获取storageclass</span><br></pre></td></tr></table></figure>

<p><img src="2021-11-17_17-46.png" alt="2021-11-17_17-46"></p>
<p>修改yaml文件，将storageClassName改为查询到的storageclass name</p>
<p><img src="2021-11-17_17-48.png" alt="2021-11-17_17-48"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Changed password for user apm_system</span><br><span class="line">PASSWORD apm_system = KDDHU4TJi9EuD87x1DBT</span><br><span class="line"></span><br><span class="line">Changed password for user kibana_system</span><br><span class="line">PASSWORD kibana_system = LsyKNl7OqhBCc9ChCXMm</span><br><span class="line"></span><br><span class="line">Changed password for user kibana</span><br><span class="line">PASSWORD kibana = LsyKNl7OqhBCc9ChCXMm</span><br><span class="line"></span><br><span class="line">Changed password for user logstash_system</span><br><span class="line">PASSWORD logstash_system = MTUGYEFhbtSN4wU2OvjR</span><br><span class="line"></span><br><span class="line">Changed password for user beats_system</span><br><span class="line">PASSWORD beats_system = wob24NOj3J0apT8Rc62T</span><br><span class="line"></span><br><span class="line">Changed password for user remote_monitoring_user</span><br><span class="line">PASSWORD remote_monitoring_user = 0geQaxGmygn90wjVMEfe</span><br><span class="line"></span><br><span class="line">Changed password for user elastic</span><br><span class="line">PASSWORD elastic = zjA9fYQfUgwgmJn4shv5</span><br></pre></td></tr></table></figure>

<p>注意将elastic用户名和密码添加到Kubernetes的Secret 对象中一步需要将password改为之前生成的最后一个密码，即上面的zjA9fYQfUgwgmJn4shv5</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl logs -f -n elastic $(kubectl get pods -n elastic | grep kibana | sed -n 1p | awk &#x27;&#123;print $1&#125;&#x27;) | grep &quot;Status changed from yellow to green&quot;</span><br><span class="line"></span><br><span class="line">kubectl get svc kibana -n elastic</span><br></pre></td></tr></table></figure>

<p>获取elastic service的CLUSTER-IP，通过CLUSTER-IP:5601访问Kibana可视化界面，登录使用elastic作为用户名和之前获取的zjA9fYQfUgwgmJn4shv5作为密码</p>
<ul>
<li>tips：pod创建出现ImagePullBackOff时，很可能时镜像拉不下来，可以考虑在dockerhub搜索对于的镜像，拉下来后使用<code>docker tag</code>命令修改标签，或者直接在yaml里直接改image对于的地址</li>
</ul>
]]></content>
      <tags>
        <tag>AIOps</tag>
        <tag>ELK</tag>
        <tag>环境搭建</tag>
        <tag>istio</tag>
        <tag>云原生</tag>
      </tags>
  </entry>
</search>
