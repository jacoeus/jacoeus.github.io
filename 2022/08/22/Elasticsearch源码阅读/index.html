<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><meta name="google-site-verification" content="1he7C7Wt3l_3CNQSu4T8-ZTZacFv5BWFyxQZs38IW7I"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"jacoeus.github.io",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="Elasticsearch作为一个分布式的免费开源搜索和分析引擎，目前受到广泛支持，并且也具有丰富的生态 本文尝试基于对Elasticsearch以及其下面的Lucene引擎在写入数据时源码的跟踪对Elasticsearch相关的概念进行整理，并对压测方案以及优化方案进行简单的调研，同时提供了一些进一步学习的链接 #TODO对比学习其他分布式OLAP数据库与Elasticsearch的异同"><meta property="og:type" content="article"><meta property="og:title" content="Elasticsearch源码阅读"><meta property="og:url" content="https://jacoeus.github.io/2022/08/22/Elasticsearch%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/index.html"><meta property="og:site_name" content="JacoeusのBlog"><meta property="og:description" content="Elasticsearch作为一个分布式的免费开源搜索和分析引擎，目前受到广泛支持，并且也具有丰富的生态 本文尝试基于对Elasticsearch以及其下面的Lucene引擎在写入数据时源码的跟踪对Elasticsearch相关的概念进行整理，并对压测方案以及优化方案进行简单的调研，同时提供了一些进一步学习的链接 #TODO对比学习其他分布式OLAP数据库与Elasticsearch的异同"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://jacoeus.github.io/2022/08/22/Elasticsearch%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/Step4.1.png"><meta property="og:image" content="https://jacoeus.github.io/2022/08/22/Elasticsearch%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/Step3.1.png"><meta property="og:image" content="https://jacoeus.github.io/2022/08/22/Elasticsearch%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/Step7.2.png"><meta property="og:image" content="https://jacoeus.github.io/2022/08/22/Elasticsearch%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg"><meta property="og:image" content="https://jacoeus.github.io/2022/08/22/Elasticsearch%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/4.png"><meta property="og:image" content="https://jacoeus.github.io/2022/08/22/Elasticsearch%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/%E6%AE%B5%E5%90%88%E5%B9%B6.png"><meta property="og:image" content="https://jacoeus.github.io/2022/08/22/Elasticsearch%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/%E5%90%88%E5%B9%B6%E7%AD%96%E7%95%A50.png"><meta property="og:image" content="https://jacoeus.github.io/2022/08/22/Elasticsearch%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/%E5%90%88%E5%B9%B6%E7%AD%96%E7%95%A51.png"><meta property="og:image" content="https://jacoeus.github.io/2022/08/22/Elasticsearch%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/%E5%90%88%E5%B9%B6%E6%96%B9%E6%A1%882.png"><meta property="article:published_time" content="2022-08-22T08:28:21.000Z"><meta property="article:modified_time" content="2022-08-23T17:59:33.686Z"><meta property="article:author" content="tang ziyin"><meta property="article:tag" content="Elasticsearch"><meta property="article:tag" content="分布式数据库"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://jacoeus.github.io/2022/08/22/Elasticsearch%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/Step4.1.png"><link rel="canonical" href="https://jacoeus.github.io/2022/08/22/Elasticsearch%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>Elasticsearch源码阅读 | JacoeusのBlog</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">JacoeusのBlog</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Hello World</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-友链"><a href="/links/" rel="section"><i class="fa fa-link fa-fw"></i>友链</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://jacoeus.github.io/2022/08/22/Elasticsearch%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="tang ziyin"><meta itemprop="description" content="Awake, arise or be for ever fall&#39;n"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="JacoeusのBlog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Elasticsearch源码阅读</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-08-22 16:28:21" itemprop="dateCreated datePublished" datetime="2022-08-22T16:28:21+08:00">2022-08-22</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2022-08-24 01:59:33" itemprop="dateModified" datetime="2022-08-24T01:59:33+08:00">2022-08-24</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Bigdata/" itemprop="url" rel="index"><span itemprop="name">Bigdata</span></a> </span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/2022/08/22/Elasticsearch%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2022/08/22/Elasticsearch%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>16k</span></span></div></header><div class="post-body" itemprop="articleBody"><p>Elasticsearch作为一个分布式的免费开源搜索和分析引擎，目前受到广泛支持，并且也具有丰富的生态</p><p>本文尝试基于对Elasticsearch以及其下面的Lucene引擎在写入数据时源码的跟踪对Elasticsearch相关的概念进行整理，并对压测方案以及优化方案进行简单的调研，同时提供了一些进一步学习的链接</p><p><code>#TODO</code>对比学习其他分布式OLAP数据库与Elasticsearch的异同</p><a id="more"></a><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>主要基于写入流程对Elasticsearch涉及的概念进行整理</p><h2 id="Elasticsearch"><a href="#Elasticsearch" class="headerlink" title="Elasticsearch"></a>Elasticsearch</h2><p><strong>基本架构</strong></p><ul><li><p>Elasticsearch集群一般包括一个主节点和多个数据节点，其中可以存在多个主节点候选节点</p><p>采用类似raft的一致性算法来选择主节点，但选择过程会结合version</p></li><li><p><strong>Replica</strong>：集群以shard为单位组织数据，为了防止数据丢失，每个shard会在多个节点创建备份</p></li></ul><p><strong>基本写入流程</strong></p><ul><li>es收到写入请求后对其进行解析根据index和_id将请求发给index对应的多个shard中由路由规则指定的shard</li><li>主shard对请求进行分析，分流不同请求类型，计算version/seqNo/primaryTerm等字段后交由Lucene对数据进行写入，写入完成后更新TransLog和LocalCheckpoint</li><li>由主shard处理完写入请求并在完成后再发送给其他replica</li></ul><p><strong>一致性解决方案</strong></p><p><strong>TransLog</strong></p><ul><li>为了进行数据同步和避免机器宕机导致内存数据丢失，类似数据库的CommitLog，es使用TransLog记录操作</li><li>es采用在将数据写入内存后，再写TransLog并落盘，从而在宕机后能从TransLog恢复之前正在处理的操作</li></ul><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34680841"><strong>系统字段</strong></a></p><ul><li>es在原始文档基础上增加了多个系统字段来保证分布式场景下的一致性和可用性<ul><li><code>_version</code>: es使用version来保证文档变更能以正确的顺序执行</li><li><code>_seq_no</code>: Shard级别严格递增，主shard向副本shard发送请求时加入该字段，保证备份信息的有序性</li><li><code>_primary_term</code>：主分片重新分配时+1，防止恢复数据时出现多个文档_seq_no一样导致的冲突</li></ul></li></ul><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/wudingmei1023/article/details/103957629"><strong>checkPoint</strong></a></p><ul><li><p>结合<code>_seq_no</code>和<code>_primary_term</code>保证主分片失效后能在新主分片上继续之前的操作，并且不会丢失数据（<code>_primary_term</code>保证不会因为重复收到的文档而导致写入被覆盖，不会多写）</p></li><li><p>主分片记录一个GlobalCheckpoint，保证所有活跃分片都已经处理完检查点之前的操作，副本分片各自记录LocalCheckpoint，保证该分片已经处理完检查点之前的操作</p></li></ul><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/35294658"><strong>Meta</strong></a></p><ul><li><p>Meta数据由master节点进行更新，记录了整个集群的信息，包括ClusterState/MetaData/IndexMetaData，三者逐级包含</p><ul><li><p>ClusterState：记录当前集群的各种状态，Master节点是通过发布ClusterState来通知其他节点，集群中的每个节点都会在内存中维护一个当前的ClusterState，表示当前集群的各种状态，包括version当前版本号、routingTable所有index的路由表、nodes当前集群节点信息等</p><p><img src="Step4.1.png" alt="Step4.1"></p><p>MetaData：包括集群唯一id，indices包括所有index的indexMetaData等集群配置信息</p><p><img src="Step3.1.png" alt="Step3.1"></p></li><li><p>IndexMetaData：每个索引的元数据，包括Index的shard数，replica数，mappings等</p><p><img src="Step7.2.png" alt="Step7.2"></p></li></ul></li><li><p>Meta数据会在data文件夹下的_state目录中以.st文件的形式持久化，从而进行故障恢复</p></li></ul><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wupeixuan/p/12514843.html"><strong>Mapping</strong></a></p><ul><li><p>类似于数据库中的表结构定义 schema，es支持根据文档信息来判断字段合适的类型</p><p>完成写入操作时发现有新增字段的文档写入，Mapping也会被更新</p></li></ul><h2 id="Lucene"><a href="#Lucene" class="headerlink" title="Lucene"></a>Lucene</h2><p><strong>基本写入流程</strong></p><ul><li>Lucene通过IndexWriter接受Add/Update/Delete/Commit四种请求调用</li><li>IndexWriter =&gt; DocumentWriter，建立DocumentsWriterPerThread(DWPT)多线程写入</li><li>DWPT写入过程除了写入<a target="_blank" rel="noopener" href="https://juejin.cn/post/6978438790411091975#heading-12">文档数据</a>，还需要记录<a target="_blank" rel="noopener" href="https://kkewwei.github.io/elasticsearch_learning/2020/02/28/Lucene8-2-0%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-tim-tip%E8%AF%8D%E5%85%B8%E7%BB%93%E6%9E%84%E5%8E%9F%E7%90%86%E7%A0%94%E7%A9%B6/">倒排表</a>（termId=&gt;docId&amp;freq，在flush的时候更新FST和磁盘中的倒排表）、<a target="_blank" rel="noopener" href="https://juejin.cn/post/6978653916439248903">DocValue</a>（用于在查询过程中对排序聚合效率进行优化，空间换时间）和<a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1366832">Point</a>（用于加速范围查询）</li></ul><p><strong>Delete实现方案</strong></p><ul><li>segment不可更改，通过.del记录删除操作，查询和Merge时进行过滤</li><li>另外更新操作也依赖于删除操作，先删除后加入 ps：方案不适用于分布式场景，es的更新操作依赖版本控制</li></ul><p><strong>flush策略</strong>：</p><ul><li>将位于内存中的数据刷写到磁盘上，以Segment为单位进行组织</li><li>但文档数量或者占用内存达到一定阈值，或者用户提交Commit请求时触发</li></ul><p><strong>Merge策略</strong></p><ul><li>segment数量会随着数据增长快速增长，而查询请求需要检查每个segment，大量segment不利于查询效率</li><li>通过Merge过程不仅可以把小文件合并成大文件，而且更新segment中已经被删除的条目，减小存储开销，但该过程需要将segment加载到内存，会消耗大量资源</li></ul><p><strong>倒排索引</strong></p><ul><li>Lucene通过indexSearch支持对以落盘数据的查询，查询过程依赖在写入时建立的倒排索引结构</li><li>通过从文档中提取term(关键字)，建立term到文档的映射，从而在进行关键字搜索时能够快速获取关联文档</li><li>term相关信息在数据写入时进行缓存，但倒排索引的数据结构在flush的时候进行创建和更新</li></ul><p><strong>termVector</strong></p><ul><li>termVector用于保存一个文档内所有term的相关信息，包括出现次数以及位置等，可用于关键词高亮以及文档间的相似度匹配等，该部分会在写入过程进行记录，结合倒排索引作为查询结果返回</li></ul><p>于是，我们可以通过Lucene提供Add/Update/Delete/Commit四种接口，将数据先写入内存，当满足flush策略后，将内存中的数据作为一个segment进行flush写入磁盘，同时依赖Merge策略在提供正常写入查询操作之外对segment进行合并，最终磁盘中的数据通过indexSearch提供查询服务。</p><p><strong>PS</strong></p><ul><li><code>#TODO</code> indexSearch部分</li><li>底层数据结构，如何压缩和支持高效Merge，从内存到磁盘数据结构变化？</li><li>FST&amp;SkipList&amp;BKDTree等数据结构的组织形式？</li></ul><h1 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h1><h2 id="写入部分源码"><a href="#写入部分源码" class="headerlink" title="写入部分源码"></a>写入部分源码</h2><p><img src="%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg" alt="流程图"></p><ol><li><p>ES的写入请求一般会进过两层处理，首先Rest层进行<strong>请求参数解析</strong>，之后在Transport层进行实际的请求处理，其中由TransportService 提供传输服务</p></li><li><p>TransportAction.RequestFilterChain#proceed TransportAction通过调用一个过滤链来对请求进行<strong>预处理</strong></p></li><li><p>以bulk请求为例，TransportBulkAction#doExecute在对请求进行处理时，首先判断请求中或者请求元数据是否指定了pipeline，则使用相应的<strong>pipeline</strong>进行处理。然后判断本节点是否具备预处理（Ingest）的资格<code>clusterService.localNode().isIngestNode()</code>，否则需要将请求转发到有资格的节点。</p><blockquote><p>pipeline可以对原始文档做一些处理，比如HTML解析，自定义的处理</p></blockquote><p>如果需要<strong>自动创建索引</strong><code>needToCheck()</code>，则收集所有的索引信息，对于不存在且能成功创建的index，加入<code>autoCreateIndices</code>中，然后会调用createIndex方法创建index，待创建成功执行下一步</p></li><li><p>TransportBulkAction.BulkOperation#doRun 遍历BulkRequest的所有子请求 (CREATE INDEX UPDATE DELETE)，然后根据请求的操作类型执行相应代码，对于写入请求，会首先根据<strong>indexMetaData信息</strong>，resolveRouting方法为每条IndexRequest生成<strong>路由信息</strong>，并通过process方法按需生成uid</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> INDEX:</span><br><span class="line">    IndexRequest indexRequest = (IndexRequest) docWriteRequest;</span><br><span class="line">    <span class="keyword">final</span> IndexMetaData indexMetaData = metaData.index(concreteIndex);</span><br><span class="line">    MappingMetaData mappingMd = indexMetaData.mappingOrDefault();</span><br><span class="line">    Version indexCreated = indexMetaData.getCreationVersion();</span><br><span class="line">    indexRequest.resolveRouting(metaData);</span><br><span class="line">    indexRequest.process(indexCreated, mappingMd, concreteIndex.getName());</span><br><span class="line">    <span class="keyword">break</span>;</span><br></pre></td></tr></table></figure><p>然后根据每个IndexRequest请求的路由信息得到所要写入的目标shardId，之后建立ShardId到请求的映射</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Map&lt;ShardId, List&lt;BulkItemRequest&gt;&gt; requestsByShard = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; bulkRequest.requests.size(); i++) &#123;</span><br><span class="line">    DocWriteRequest&lt;?&gt; request = bulkRequest.requests.get(i);</span><br><span class="line">    <span class="keyword">if</span> (request == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    String concreteIndex = concreteIndices.getConcreteIndex(request.index()).getName();</span><br><span class="line">    ShardId shardId = clusterService.operationRouting().indexShards(clusterState, concreteIndex, request.id(),</span><br><span class="line">        request.routing()).shardId();</span><br><span class="line">    List&lt;BulkItemRequest&gt; shardRequests = requestsByShard.computeIfAbsent(shardId, shard -&gt; <span class="keyword">new</span> ArrayList&lt;&gt;());</span><br><span class="line">    shardRequests.add(<span class="keyword">new</span> BulkItemRequest(i, request));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>于是以BulkShardRequest<strong>封装每个shard对应的所有请求</strong>，并交由TransportShardBulkAction来处理，来将请求分发到不同shard</p></li><li><p>转发TransportShardBulkAction请求，TransportReplicationAction#doExecute进入TransportReplicationAction.ReroutePhase#doRun 进行路由转发<code>setPhase(task, &quot;routing&quot;)</code>标记<strong>路由阶段</strong>，此处判断主shard是否位于当前节点，从而决定将请求发送到哪</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (primary.currentNodeId().equals(state.nodes().getLocalNodeId())) &#123;</span><br><span class="line">    performLocalAction(state, primary, node, indexMetaData);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    performRemoteAction(state, primary, node); <span class="comment">//需要重路由，函数同时实现循环路由的避免</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>TransportReplicationAction.ReroutePhase#performAction向节点发送请求。至此将各个shard的BulkShard发送到shard的Primary Node</p></li><li><p>TransportReplicationAction#handlePrimaryRequest 主分片接收到请求信息调用TransportReplicationAction.AsyncPrimaryAction#doRun 检查当前是否为主分片、allocationId是否是预期值、PrimaryTerm是否是预期值等</p><p>然后TransportReplicationAction.AsyncPrimaryAction#runWithPrimaryShardReference检测主分片是否已经被迁移<code>primaryShardReference.isRelocated()</code></p></li><li><p><code>setPhase(replicationTask, &quot;primary&quot;)</code>至此开始主shard对请求进行处理的阶段，这里定义了一个globalCheckpointSyncingListener来处理副本分片的<strong>全局检查点</strong>查询请求，处理后执行ReplicationOperation#execute =&gt; TransportReplicationAction.PrimaryShardReference#perform =&gt; TransportShardBulkAction#shardOperationOnPrimary =&gt; TransportShardBulkAction#performOnPrimary，在这里检查上下文并调用executeBulkItemRequest进行所有请求进行处理</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (context.hasMoreOperationsToExecute()) &#123;</span><br><span class="line">    <span class="keyword">if</span> (executeBulkItemRequest(context, updateHelper, nowInMillisSupplier, mappingUpdater, waitForMappingUpdate,</span><br><span class="line">        ActionListener.wrap(v -&gt; executor.execute(<span class="keyword">this</span>), <span class="keyword">this</span>::onRejection)) == <span class="keyword">false</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">assert</span> context.isInitial();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>TransportShardBulkAction#executeBulkItemRequest 会<code>final long version = context.getRequestToExecute().version();</code>获取<strong>version</strong>，然后分流删除请求和其他请求，以index请求为例执行IndexShard#applyIndexOperationOnPrimary，这里会检查<strong>primaryTerm</strong>，然后进行index操作需要的准备</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> String resolvedType = mapperService.resolveDocumentType(sourceToParse.type());</span><br><span class="line"><span class="keyword">final</span> SourceToParse sourceWithResolvedType;</span><br><span class="line"><span class="keyword">if</span> (resolvedType.equals(sourceToParse.type())) &#123;</span><br><span class="line">     sourceWithResolvedType = sourceToParse;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    sourceWithResolvedType = <span class="keyword">new</span> SourceToParse(sourceToParse.index(), </span><br><span class="line">        resolvedType, sourceToParse.id(),sourceToParse.source(), </span><br><span class="line">        sourceToParse.getXContentType(), sourceToParse.routing());</span><br><span class="line">&#125;</span><br><span class="line">operation = prepareIndex(docMapper(resolvedType),          </span><br><span class="line">    indexSettings.getIndexVersionCreated(), sourceWithResolvedType,seqNo,      </span><br><span class="line">    opPrimaryTerm, version, versionType, origin, autoGeneratedTimeStamp, isRetry, </span><br><span class="line">    ifSeqNo, ifPrimaryTerm); <span class="comment">//准备index操作的参数</span></span><br><span class="line">Mapping update = operation.parsedDoc().dynamicMappingsUpdate();</span><br><span class="line"><span class="keyword">if</span> (update != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Engine.IndexResult(update);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>进入InternalEngine#index打开一个ReadLock，首先制定执行策略<code>indexingStrategyForOperation(index)</code>，然后生成或注册一个sequence number，根据执行策略<code>plan.indexIntoLucene || plan.addStaleOpToLucene</code>执行InternalEngine#indexIntoLucene</p><p>这里如果当前是副本分片，则需要通过解析Doc来更新SeqNo和version</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">index.parsedDoc().updateSeqID(index.seqNo(), index.primaryTerm());</span><br><span class="line">index.parsedDoc().version().setLongValue(plan.versionForIndexing);</span><br></pre></td></tr></table></figure><p>之后以增操作为例则进入InternalEngine#addDocs调用到<code>indexWriter.addDocuments(docs);</code>，正式进入Lucene引擎</p></li><li><p>IndexWriter#updateDocuments调用DocumentsWriter#updateDocuments并处理返回的<strong>SeqNo</strong>，updateDocuments通过**DocumentsWriterPerThread(DWPT)**实现多线程写入优化，每个DWPT对应独立的内存空间</p><ul><li><p>DocumentsWriterPerThread#updateDocuments</p><p>DefaultIndexingChain#processDocument遍历Field执行DefaultIndexingChain#processField</p><blockquote><p>Lucene内部索引构建最关键的概念是IndexingChain，即链式的索引构建。Lucene提供了各种不同类型的索引类型，例如倒排、正排（列存）、StoreField、DocValues等。每个不同的索引类型对应不同的索引算法、数据结构以及文件存储，有些是列级别的，有些是文档级别的。所以一个文档写入后，需要被这么多种不同索引处理，有些索引会共享memory-buffer，有些则是完全独立的。</p></blockquote><p>DefaultIndexingChain#getOrAddField计算fp，如果在当前segment第一次碰到这个filed则新建<code>fp = new PerField(docWriter.getIndexCreatedVersionMajor(), fi, invert);</code>，然后存储在fieldHash中</p></li><li><p>DefaultIndexingChain.PerField#invert将field值通过分词器转换为tokenStream词元流TermsHashPerField#add()处理tokenStream将<strong>term</strong>加入TermsHash，这里对addTerm有两种不同实现，以FreqProxTermsWriterPerField#addTerm为例，能够记录term在文档中的词频和位置，分成新document、当前doc存在新出现term进行处理，然后将写入缓冲池</p><p>在之后由<strong>flushPolicy</strong>更新，包括flushOnDocCount()和flushOnRAM()两种方案，对应文档数量和内存占用达到阈值</p></li><li><p>DefaultIndexingChain#processField中完成invert处理后调用CompressingStoredFieldsWriter#writeField，根据Field类型对值进行压缩存储到bufferedDocs，是GrowableByteArrayDataOutput类的实例 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/384486147">Lucene源码解析——StoredField存储方式 - 知乎 (zhihu.com)</a></p></li><li><p>DefaultIndexingChain#indexDocValue根据DocValue的类型(NUMERIC BINARY SORTED SORTED_NUMERIC SORTED_SET)</p><blockquote><p>DocValue记录DocId到Value的映射并序列化存储在磁盘，便于<strong>聚合排序</strong>等操作，避免这些操作时需要将所有文档加入内存</p></blockquote></li><li><p>DefaultIndexingChain#indexPoint为了加速<strong>RangeQuery</strong>提出的一种数据结构，底层用BKDTree</p></li></ul></li><li><p><code>dwpt.updateDocuments</code>执行结束后，<code>flushingDWPT = flushControl.doAfterDocument(perThread, isUpdate);</code>查看是否满足flush条件，满足则进行flush</p><p>DocumentsWriterFlushControl#doAfterDocument，flushPolicy对更新操作进行先删除后插入，对插入操作<code>flushPolicy.onInsert(this, perThread)</code></p><p>每个DWPT的<a target="_blank" rel="noopener" href="https://kkewwei.github.io/elasticsearch_learning/2020/02/28/Lucene8-2-0%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-tim-tip%E8%AF%8D%E5%85%B8%E7%BB%93%E6%9E%84%E5%8E%9F%E7%90%86%E7%A0%94%E7%A9%B6/#flush%E5%88%B0%E6%96%87%E4%BB%B6%E4%B8%AD">flush操作</a>在DocumentsWriterPerThread#flush中执行，其中FreqProxTermsWriter#flush 对FreqProxTerms进行flush的同时执行BlockTreeTermsWriter#write，即<strong>FST</strong>的构建</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Fields fields, NormsProducer norms)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    String lastField = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">for</span>(String field : fields) &#123; <span class="comment">//遍历segment每个field</span></span><br><span class="line">        <span class="keyword">assert</span> lastField == <span class="keyword">null</span> || lastField.compareTo(field) &lt; <span class="number">0</span>;</span><br><span class="line">        lastField = field;</span><br><span class="line">        Terms terms = fields.terms(field);</span><br><span class="line">        <span class="keyword">if</span> (terms == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        TermsEnum termsEnum = terms.iterator();</span><br><span class="line">        TermsWriter termsWriter = <span class="keyword">new</span> TermsWriter(fieldInfos.fieldInfo(field));</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            BytesRef term = termsEnum.next();</span><br><span class="line">            <span class="keyword">if</span> (term == <span class="keyword">null</span>) &#123;</span><br><span class="line">                  <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            termsWriter.write(term, termsEnum, norms); <span class="comment">//将term加入词典并建立索引结构</span></span><br><span class="line">        &#125;</span><br><span class="line">        termsWriter.finish(); <span class="comment">//将词典索引结构FST放入tip文件</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在BlockTreeTermsWriter.TermsWriter#finish中调用FST#save(DataOutput)对FST进行存储</p><p>fst.OnHeapFSTStore作为FST在堆内进行存储时进行读写的类，以BytesStore和byte[]两种形式组织数据，但FST大于1GB时采用BytesStore，否则使用一个byte数组存储</p><p>fst.OffHeapFSTStore 发现Lucene8.1.0有将FST存储在堆外的类，但没有写入方法实现</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">writeTo</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">&quot;writeToOutput operation is not supported for OffHeapFSTStore&quot;</span>); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>InternalEngine#index收到写入result后，如果写入成功，则将该写入操作写入<strong>TransLog</strong>并更新<strong>本地检查点</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (index.origin().isFromTranslog() == <span class="keyword">false</span>) &#123;</span><br><span class="line">    <span class="keyword">final</span> Translog.Location location;</span><br><span class="line">    <span class="keyword">if</span> (indexResult.getResultType() == Result.Type.SUCCESS) &#123;</span><br><span class="line">        location = translog.add(<span class="keyword">new</span> Translog.Index(index, indexResult));</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (indexResult.getSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO) &#123;</span><br><span class="line">        <span class="comment">// if we have document failure, record it as a no-op in the translog and Lucene with the generated seq_no</span></span><br><span class="line">        <span class="keyword">final</span> NoOp noOp = <span class="keyword">new</span> NoOp(indexResult.getSeqNo(), index.primaryTerm(), index.origin(),</span><br><span class="line">        index.startTime(), indexResult.getFailure().toString());</span><br><span class="line">        location = innerNoOp(noOp).getTranslogLocation();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        location = <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    indexResult.setTranslogLocation(location);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (plan.indexIntoLucene &amp;&amp; indexResult.getResultType() == Result.Type.SUCCESS) &#123;</span><br><span class="line">    <span class="keyword">final</span> Translog.Location translogLocation = trackTranslogLocation.get() ? indexResult.getTranslogLocation() : <span class="keyword">null</span>;</span><br><span class="line">    versionMap.maybePutIndexUnderLock(index.uid().bytes(), <span class="keyword">new</span> IndexVersionValue(translogLocation, plan.versionForIndexing, index.seqNo(), index.primaryTerm()));</span><br><span class="line">&#125;</span><br><span class="line">localCheckpointTracker.markSeqNoAsProcessed(indexResult.getSeqNo());</span><br><span class="line"><span class="keyword">if</span> (indexResult.getTranslogLocation() == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="comment">// the op is coming from the translog (and is hence persisted already) or it does not have a sequence number</span></span><br><span class="line">    <span class="keyword">assert</span> index.origin().isFromTranslog() || indexResult.getSeqNo() == SequenceNumbers.UNASSIGNED_SEQ_NO;</span><br><span class="line">    localCheckpointTracker.markSeqNoAsPersisted(indexResult.getSeqNo());</span><br><span class="line">&#125;</span><br><span class="line">indexResult.setTook(System.nanoTime() - index.startTime());</span><br><span class="line">indexResult.freeze();</span><br><span class="line"><span class="keyword">return</span> indexResult;</span><br></pre></td></tr></table></figure></li><li><p>对写入操作返回结果进行检查，根据返回结果执行相应操作，如果需要更新<strong>mapping</strong>，则执行MappingUpdatedAction#updateMappingOnMaster</p></li></ol><h2 id="flush"><a href="#flush" class="headerlink" title="flush"></a>flush</h2><p>Lucene在内存占用或者文档数量达到阈值后会触发flush， <code>flushOnDocCount()</code>和<code>flushOnRAM()</code></p><p>或者调用IndexWriter#flush(boolean, boolean) 将当前内存中的数据全部落盘</p><h2 id="Merge"><a href="#Merge" class="headerlink" title="Merge"></a>Merge</h2><p><a target="_blank" rel="noopener" href="https://elasticsearch.cn/question/12808">如何深入理解ES的merge策略 - Elastic 中文社区 (elasticsearch.cn)</a></p><p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1846903">Elasticsearch merge 你懂了吗？ - 腾讯云开发者社区-腾讯云 (tencent.com)</a></p><p>index.EsTieredMergePolicy es提供的Merge策略</p><p>Lucene由ConcurrentMergeScheduler和SerialMergeScheduler控制merge操作，IndexWriter#merge进行段合并，Segment合并原理<a target="_blank" rel="noopener" href="https://kkewwei.github.io/elasticsearch_learning/2021/06/13/Lucene8-6-2%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-Segment-StoredFields%E5%90%88%E5%B9%B6%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/">Lucene8.6.2底层架构-Segment StoredFields合并原理详解 | Hexo (kkewwei.github.io)</a></p><p>Luence</p><p>TieredMergePolicy#getSortedBySegmentSize 提供了根据段提交信息来进行进行排序的方，可供采用的段信息在SegmentCommitInfo记录</p><h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><p><strong>fst</strong>：有限状态转换器，用来代替哈希表，牺牲一部分查询效率但极大减小空间开销，且相较前缀树还能共享term单词的后缀，用于建立term到docID的映射关系。</p><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/354203255">Lucene-FST-1 - 知乎 (zhihu.com)</a></p><p><a target="_blank" rel="noopener" href="https://kkewwei.github.io/elasticsearch_learning/2020/02/25/Lucene8-2-0%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-%E8%AF%8D%E5%85%B8fst%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/">Lucene8.2.0底层架构-词典fst原理解析 | Hexo (kkewwei.github.io)</a></p><p><strong>skipList</strong>：<a target="_blank" rel="noopener" href="https://kkewwei.github.io/elasticsearch_learning/2020/02/28/Lucene8-2-0%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-tim-tip%E8%AF%8D%E5%85%B8%E7%BB%93%E6%9E%84%E5%8E%9F%E7%90%86%E7%A0%94%E7%A9%B6/#%E5%8D%95%E4%B8%AA%E8%AF%8D%E7%9A%84%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%E7%BB%93%E6%9E%84%E8%90%BD%E7%9B%98">单个词的倒排索引结构落盘</a></p><p><strong>BKDTree</strong>：<a target="_blank" rel="noopener" href="https://kkewwei.github.io/elasticsearch_learning/2020/11/01/Lucene8-6-2%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-BKW%E6%A0%91%E6%9E%84%E5%BB%BA%E8%BF%87%E7%A8%8B/">Lucene8.6.2底层架构-BKW树构建过程 | Hexo (kkewwei.github.io)</a></p><p><strong>IntBlockPool/ByteBlockPool</strong>：<a target="_blank" rel="noopener" href="https://kkewwei.github.io/elasticsearch_learning/2019/10/06/Lucene8-2-0%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-ByteBlockPool%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/">Lucene8.2.0底层架构-ByteBlockPool结构分析 | Hexo (kkewwei.github.io)</a></p><h2 id="data文件夹"><a href="#data文件夹" class="headerlink" title="data文件夹"></a>data文件夹</h2><p><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/791498">Elasticsearch 内存占用分析及 page cache 监控-阿里云开发者社区 (aliyun.com)</a></p><p><a target="_blank" rel="noopener" href="https://kkewwei.github.io/elasticsearch_learning/2020/02/28/Lucene8-2-0%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-tim-tip%E8%AF%8D%E5%85%B8%E7%BB%93%E6%9E%84%E5%8E%9F%E7%90%86%E7%A0%94%E7%A9%B6/#flush%E5%88%B0%E6%96%87%E4%BB%B6%E4%B8%AD">Lucene8.2.0底层架构-tim/tip词典结构原理研究 | Hexo (kkewwei.github.io)</a></p><p><a target="_blank" rel="noopener" href="https://kkewwei.github.io/elasticsearch_learning/2019/10/29/Lucenec%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-fdt-fdx%E6%9E%84%E5%BB%BA%E8%BF%87%E7%A8%8B/">Lucene8.2.0底层架构-fdt/fdx构建过程 | Hexo (kkewwei.github.io)</a>、</p><p><a target="_blank" rel="noopener" href="https://kkewwei.github.io/elasticsearch_learning/2019/11/15/Lucene%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-dvm-dvm%E6%9E%84%E5%BB%BA%E8%BF%87%E7%A8%8B/">Lucene8.2.0底层架构-dvd/dvm构建过程 | Hexo (kkewwei.github.io)</a></p><h1 id="压测分析"><a href="#压测分析" class="headerlink" title="压测分析"></a>压测分析</h1><h2 id="esrally离线安装"><a href="#esrally离线安装" class="headerlink" title="esrally离线安装"></a>esrally离线安装</h2><p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1891569?from=10680">Elasticsearch压测工具esrally部署之踩坑实录（上） - 腾讯云开发者社区-腾讯云 (tencent.com)</a></p><p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1892065">Elasticsearch压测工具Esrally部署之踩坑实录（下） - 腾讯云开发者社区-腾讯云 (tencent.com)</a></p><p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1595367">ElasticSearch压测工具：esrally离线使用详解 - 腾讯云开发者社区-腾讯云 (tencent.com)</a></p><p><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/851848">【离线】esrally实践总结-阿里云开发者社区 (aliyun.com)</a></p><ul><li><p>其中数据集默认需要联网下载，可以通过在其他可联网linux机器上参考<a target="_blank" rel="noopener" href="https://esrally.readthedocs.io/en/stable/offline.html#using-tracks">Offline Usage - Rally 2.6.0 documentation (esrally.readthedocs.io)</a>安装，然后打包对应.rally文件夹置入离线机器相应目录</p></li><li><p>离线向 x.x.x.x:9200 es服务器运行http_logs压测任务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">esrally race --pipeline=benchmark-only --target-hosts=x.x.x.x:9200 --challenge=append-no-conflicts --offline --track-path=~/.rally/benchmarks/tracks/default/http_logs/  </span><br></pre></td></tr></table></figure></li></ul><h2 id="堆内存分析"><a href="#堆内存分析" class="headerlink" title="堆内存分析"></a>堆内存分析</h2><p><img src="4.png" alt="mat"></p><p>根据java堆内存占用发现，在esrally index-append过程中，es消耗的堆内存主要来自IndexService类、PainlessScriptEngine类和IndexWriter类的对象，其中，IndexWriter对象中包含多个dwpt分别占用一部分内存，每个dwpt的主要内存占用来自多个filed的pointValuesWriter和docValuesWriter</p><h1 id="优化方案"><a href="#优化方案" class="headerlink" title="优化方案"></a>优化方案</h1><p><strong>存储优化</strong></p><ul><li><p>FST转移到堆外内存</p><p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1770296">腾讯万亿级 Elasticsearch 内存效率提升解密 - 腾讯云开发者社区-腾讯云 (tencent.com)</a></p></li><li><p>Segment Merge策略优化</p></li></ul><p><strong>性能优化</strong></p><p><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/745572">内核调优 | 如何提升Elasticsearch master调度性能40倍-阿里云开发者社区 (aliyun.com)</a></p><p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1626392">腾讯Elasticsearch海量规模背后的内核优化剖析 - 腾讯云开发者社区-腾讯云 (tencent.com)</a></p><h1 id="Lucene段合并策略优化"><a href="#Lucene段合并策略优化" class="headerlink" title="Lucene段合并策略优化"></a>Lucene段合并策略优化</h1><h2 id="Lucene段合并策略"><a href="#Lucene段合并策略" class="headerlink" title="Lucene段合并策略"></a>Lucene段合并策略</h2><p><img src="%E6%AE%B5%E5%90%88%E5%B9%B6.png" alt="段合并"></p><p>Elasticsearch底层采用Lucene引擎，在磁盘上以Segment段的形式组织数据。根据flush策略，每次完成文档写入操作后会检查当前DWPT的文档数量或内存占用是否达到阈值，则将内存中存储的文档flush到磁盘文件，或者由于自动刷新流程会根据refresh_interval间隔时间自动创建新段。</p><p>我们需要及时将在内存中缓存的数据写入磁盘来提供查询功能，如果需要在数据写入1s后即可进行查询，则每1s需要创建若干个段（取决于线程数），这样很容易在较短时间产生大量的段。这样会导致：</p><ul><li>消耗资源：每一个段都会消耗文件句柄、内存和cpu运行周期。</li><li>搜索变慢：每个查询请求会轮流检查每个段，因此段越多，查询效率也就越低。</li><li>数据冗余：写放大问题，需要段合并来移除被标记为已删除的文档。</li></ul><p>当然另一方面，段合并需要额外的磁盘IO开销，在进行段合并时，写入查询效率也会收到影响。因此需要采取合适的段合并策略来解决上述问题。</p><p>Lucene在下述场景下回触发段合并</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">enum</span> <span class="title">MergeTrigger</span> </span>&#123;</span><br><span class="line">  SEGMENT_FLUSH, <span class="comment">//段被flush到磁盘时触发</span></span><br><span class="line">  FULL_FLUSH, <span class="comment">//commit导致full flush时触发</span></span><br><span class="line">  EXPLICIT, <span class="comment">//被用户显式调用</span></span><br><span class="line">  MERGE_FINISHED, <span class="comment">//被已完成的段合并操作触发</span></span><br><span class="line">  CLOSING <span class="comment">//IndexWriter即将被关闭时触发</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Luence默认采用的段合并策略为TieredMergePolicy，同时支持LogMergePolicy、LogByteSizeMergePolicy、LogDocMergePolicy（这些方案通过对段大小进行log平滑运算，据此分层对相邻段文件进行合并）。</p><p>这些段合并策略在触发后计算当前段应该以什么样的顺序相互进行合并，并将生成的方案交给MergeScheduler在后台选择合适的时机执行真正的段合并操作。</p><p>其中，TieredMergePolicy会根据段大小进行排序，在排序结果中可以选择大小接近的段进行合并</p><ol><li>首先根据已知的段信息(sizeInBytes)对其timsort进行排序，采用的排序方案如下，其中SegmentCommitInfo记录了段中被标记删除的数量、文档存活代数、包含的文档数等信息。</li><li>对排序结果筛除正在合并的段、删除比率低的大段。根据保留段的信息和设定的每一层合并的个数，计算当前层应该使用的levelSize大小。</li><li>对保留的各段按照levelSize分组合并，通过score计算更优的组合进行放入MergeSpecification，重复计算直到不存在满足的merge组合，将多次merge操作及其对应的<code>List&lt;SegmentCommitInfo&gt;</code>交给MergeScheduler进行段合并。</li></ol><h2 id="日志场景下存在的问题"><a href="#日志场景下存在的问题" class="headerlink" title="日志场景下存在的问题"></a>日志场景下存在的问题</h2><p><img src="%E5%90%88%E5%B9%B6%E7%AD%96%E7%95%A50.png" alt="合并方案"></p><p>TieredMergePolicy合并方式的优势在于<strong>合并高效</strong>，可以<strong>快速降低文件数</strong>。主要问题是<strong>数据不连续</strong>，这样会导致我们在查询的时候文件裁剪的能力很弱。</p><blockquote><p>比如查询最近一小时的数据，很有可能一小时的文件被分别合并到了几天前的文件中去了，导致需要遍历的文件增加了。</p></blockquote><ul><li>在日志场景中，我们更关注<strong>不久前的日志</strong>，在针对日志的查询中，也经常会关注<strong>一段时间内的日志</strong>，因此，通过设计段合并策略来将时间因素纳入考虑来保证段内数据的<strong>连续性</strong>能够一定程度提升相关查询的效率。另外，日志场景中，<strong>删除和更新操作很少</strong>，因此在段合并过程可以基本不考虑写放大的相关问题。</li></ul><h2 id="解决方案调研"><a href="#解决方案调研" class="headerlink" title="解决方案调研"></a>解决方案调研</h2><p><img src="%E5%90%88%E5%B9%B6%E7%AD%96%E7%95%A51.png" alt="合并方案"></p><p>典型的解决数据连续性的合并策略，包括以 Cassandra、HBase 为代表的基于时间窗口的合并策略，优点是数据按时间序合并，查询高效，且可以支持表内 TTL。但限制是只支持时序场景，而且文件大小可能不一致，从而影响合并效率。还有以 LevelDB、RocksDB 为代表的分层合并，一层一组有序，每次抽取部分数据向下层合并，优点是查询高效，但是写放大比较严重，相同的数据可能会被多次合并，影响写入吞吐。</p><p><img src="%E5%90%88%E5%B9%B6%E6%96%B9%E6%A1%882.png" alt="合并方案2"></p><p>腾讯提出的段合并策略：为了提升数据连续性、收敛文件数量，提升文件的裁剪能力来提高查询性能。通过主要是按时间序分层合并，<strong>每层文件之间按创建时间排序</strong>，除了第一层外，都按照时间序和目标大小进行合并，<strong>不固定每次合并文件数量</strong>，这样保证了合并的高效性。对于少量的未合并的文件以及冷分片文件，采用<strong>持续合并</strong>的策略，将超过默认五分钟不再写入的分片进行持续合并，并控制合并并发和范围，以降低合并开销。腾讯通过上述方案将搜索场景的查询性能提升了 40%。</p><h2 id="一些优化思路"><a href="#一些优化思路" class="headerlink" title="一些优化思路"></a>一些优化思路</h2><ul><li>考虑到Lucene采用DWPT来多线程的进行数据写入，文件的创建时间不能很好反映日志的生成时间，可以考虑结合每个段中日志的时间戳字段，取段内日志的时间范围作为段信息。</li><li>在排序时控制不允许时间相差过大的段排在临近位置，时间重叠率高优先合并。</li><li>通过查询命中情况来代表段的冷热程度，在查询时记录单位时间内查询命中次数作为段信息。</li><li>在计算score时将各个段的冷热差异或时间重叠率作为依据。</li></ul><h1 id="PS"><a href="#PS" class="headerlink" title="PS"></a>PS</h1><p>To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream when we fail. We expect some caller to eventually deal with the real exception, so we don’t want any ‘catch’ clauses, but rather a finally that takes note of the problem.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;** A &#123;@link BytesStore&#125;, used during building, or during reading when *  the FST is very large (more than 1 GB).  If the FST is less than 1 *  GB then bytesArray is set instead. *&#x2F;final BytesStore bytes;</span><br></pre></td></tr></table></figure><p>IndexSearcher#search()</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; TODO: should we make this&#x2F;&#x2F; threaded...?  the Collector could be sync&#39;d?&#x2F;&#x2F; always use single thread:</span><br></pre></td></tr></table></figure></div><div class="reward-container"><div></div><button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'>打赏</button><div id="qr" style="display:none"><div style="display:inline-block"><img src="/images/alipay.jpg" alt="tang ziyin 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>tang ziyin</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://jacoeus.github.io/2022/08/22/Elasticsearch%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/" title="Elasticsearch源码阅读">https://jacoeus.github.io/2022/08/22/Elasticsearch源码阅读/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Elasticsearch/" rel="tag"># Elasticsearch</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/" rel="tag"># 分布式数据库</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2022/08/22/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8A%80%E6%9C%AF%E6%95%B4%E7%90%86/" rel="prev" title="分布式数据库技术整理"><i class="fa fa-chevron-left"></i> 分布式数据库技术整理</a></div><div class="post-nav-item"></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Elasticsearch"><span class="nav-number">1.1.</span> <span class="nav-text">Elasticsearch</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lucene"><span class="nav-number">1.2.</span> <span class="nav-text">Lucene</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%BA%90%E7%A0%81"><span class="nav-number">2.</span> <span class="nav-text">源码</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%99%E5%85%A5%E9%83%A8%E5%88%86%E6%BA%90%E7%A0%81"><span class="nav-number">2.1.</span> <span class="nav-text">写入部分源码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#flush"><span class="nav-number">2.2.</span> <span class="nav-text">flush</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Merge"><span class="nav-number">2.3.</span> <span class="nav-text">Merge</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84"><span class="nav-number">3.</span> <span class="nav-text">数据结构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#data%E6%96%87%E4%BB%B6%E5%A4%B9"><span class="nav-number">3.1.</span> <span class="nav-text">data文件夹</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8E%8B%E6%B5%8B%E5%88%86%E6%9E%90"><span class="nav-number">4.</span> <span class="nav-text">压测分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#esrally%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85"><span class="nav-number">4.1.</span> <span class="nav-text">esrally离线安装</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A0%86%E5%86%85%E5%AD%98%E5%88%86%E6%9E%90"><span class="nav-number">4.2.</span> <span class="nav-text">堆内存分析</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%88"><span class="nav-number">5.</span> <span class="nav-text">优化方案</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lucene%E6%AE%B5%E5%90%88%E5%B9%B6%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96"><span class="nav-number">6.</span> <span class="nav-text">Lucene段合并策略优化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Lucene%E6%AE%B5%E5%90%88%E5%B9%B6%E7%AD%96%E7%95%A5"><span class="nav-number">6.1.</span> <span class="nav-text">Lucene段合并策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%97%A5%E5%BF%97%E5%9C%BA%E6%99%AF%E4%B8%8B%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">6.2.</span> <span class="nav-text">日志场景下存在的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94"><span class="nav-number">6.3.</span> <span class="nav-text">解决方案调研</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF"><span class="nav-number">6.4.</span> <span class="nav-text">一些优化思路</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PS"><span class="nav-number">7.</span> <span class="nav-text">PS</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><a href="/"><img class="site-author-image" itemprop="image" alt="tang ziyin" src="/images/avatar.jpg"></a><p class="site-author-name" itemprop="name">tang ziyin</p><div class="site-description" itemprop="description">Awake, arise or be for ever fall&#39;n</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">17</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">10</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">32</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/jacoeus" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jacoeus" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:jacoeus@qq.com" title="E-Mail → mailto:jacoeus@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2022</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">tang ziyin</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">93k</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动</div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdnjs.cloudflare.com/ajax/libs/valine/1.3.10/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'iogdw17Nj7hJHEGAgwRDUDrW-gzGzoHsz',
      appKey     : 'dVrERgoAN8k3ElI9ISHu1itV',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});</script></body></html>